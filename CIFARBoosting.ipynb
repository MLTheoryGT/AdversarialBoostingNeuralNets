{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "from WongBasedTraining import WongBasedTrainingCIFAR10\n",
    "from Architectures import PreActResNet18, WideResNet\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/NVIDIA/apex\n",
    "# !cd apex\n",
    "# !pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch_memlab in /usr/local/lib/python3.6/dist-packages (0.2.2)\n",
      "Requirement already satisfied: calmsize in /usr/local/lib/python3.6/dist-packages (from pytorch_memlab) (0.1.3)\n",
      "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.6/dist-packages (from pytorch_memlab) (1.7.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytorch_memlab) (41.0.1)\n",
      "Requirement already satisfied: pandas>=0.18 in /usr/local/lib/python3.6/dist-packages (from pytorch_memlab) (0.25.0)\n",
      "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.4->pytorch_memlab) (0.7)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=1.4->pytorch_memlab) (1.17.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.4->pytorch_memlab) (3.7.4.3)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.4->pytorch_memlab) (0.17.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.18->pytorch_memlab) (2019.2)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.18->pytorch_memlab) (2.8.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas>=0.18->pytorch_memlab) (1.12.0)\n",
      "\u001b[33mWARNING: You are using pip version 19.2.1, however version 21.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch_memlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Boosting import Ensemble, runBoosting\n",
    "from AdversarialAttacks import attack_fgsm, attack_pgd\n",
    "from pytorch_memlab import LineProfiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR10 Boosting (Adversarial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Ensemble (We put this first because it's used very often)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maxSamples_vals = [100000, 200000, 500000]\n",
    "maxSamples_vals = [30004]\n",
    "# maxSamples_vals = [30003]\n",
    "# maxSamples_vals = [500000, 1000000]\n",
    "# done for 50K, 100K\n",
    "# maxSamples_vals = [1000000, 2000000]\n",
    "# maxSamples_vals = [123456]\n",
    "batch_size=100\n",
    "# maxSamples_vals = [batch_size + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_wl = 15 # maybe around 100? #later: maybe change this to an array?\n",
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensembles = []\n",
    "# epsilons = [0.0, 0.01, 0.02, 0.03, 0.05, 0.1]\n",
    "epsilons = [0.127]\n",
    "train_eps_nn = 8\n",
    "# epsilons = []\n",
    "# epsilons = [0.0, 0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Testing import testEnsemble\n",
    "path = f'./models/{maxSamples_vals[0]}Eps{train_eps_nn}/'\n",
    "attack=attack_pgd\n",
    "attackStr=\"attack_pgd\"\n",
    "ensemble = testEnsemble(path, [attack], num_wl, numsamples_train=200, numsamples_val=1500, attack_eps_ensemble=epsilons, gradOptWeights=False, attack_iters=20, restarts=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxSamples = maxSamples_vals[0]\n",
    "resultsPath = f'results/plots/cifar10/train_eps_{train_eps_nn}/{attackStr}/'\n",
    "acc_file = resultsPath + f'acc_maxSamples_{maxSamples}.png'\n",
    "adv_acc_file = resultsPath + f'adv_acc_maxSamples_{maxSamples}.png'\n",
    "loss_file = resultsPath + f'loss_maxSamples_{maxSamples}.png'\n",
    "# wl_train_acc_file = resultsPath + f'wl_train_acc_maxSamples_{maxSamples}.png'\n",
    "ensemble.plot_accuracies(acc_file)\n",
    "ensemble.plot_loss(loss_file)\n",
    "ensemble.plot_adversarial_accuracies(adv_acc_file)\n",
    "# ensemble.plot_wl_acc(wl_train_acc_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(maxSamples_vals): 1\n",
      "maxSamples: 30004\n",
      "attack_eps_nn:  [0.127]\n",
      "attack_eps_ensemble:  [0.127]\n",
      "train_eps_nn:  8\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "attack eps ens [0.127]\n",
      "path_head: ./models/30004Eps8/\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training weak learner 0 of 15\n",
      "C_t:  [[ 1.  1.  1.  1.  1.  1. -9.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1. -9.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1. -9.]\n",
      " [ 1.  1.  1.  1. -9.  1.  1.  1.  1.  1.]\n",
      " [ 1. -9.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      " [ 1. -9.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      " [ 1.  1. -9.  1.  1.  1.  1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1. -9.  1.  1.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1. -9.  1.]\n",
      " [ 1.  1.  1. -9.  1.  1.  1.  1.  1.  1.]]\n",
      "targets:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3])\n",
      "f:  [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "fexp:  [[1. 1. 1. 1. 1. 1. 0. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
      " [1. 1. 1. 1. 0. 1. 1. 1. 1. 1.]\n",
      " [1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 0. 1.]\n",
      " [1. 1. 1. 0. 1. 1. 1. 1. 1. 1.]]\n",
      "Epoch \t Seconds \t LR \t \t Train Loss \t Train Acc\n",
      "train_loader: <torch.utils.data.dataloader.DataLoader object at 0x7f06a9f06400>\n",
      "Progress: 10000,  val accuracy: 0.2500\n",
      "PGD accuracy: []\n",
      "Progress: 20000,  val accuracy: 0.3500\n",
      "PGD accuracy: []\n",
      "Progress: 30000,  val accuracy: 0.3000\n",
      "PGD accuracy: []\n",
      "Progress: 40000,  val accuracy: 0.4000\n",
      "PGD accuracy: []\n",
      "Progress: 50000,  val accuracy: 0.4900\n",
      "PGD accuracy: []\n",
      "After fit function:  0:01:17.742402\n",
      "Test accuracy of weak learner:  0.4445\n",
      "Training accuracy of weak learner:  0.44088\n",
      "After train/test acc:  0:01:36.186248\n",
      "After allindices:  0:13:41.899376\n",
      "Predictions:  [4. 1. 9. 6. 8. 1. 6. 7. 0. 9.]\n",
      "Alpha:  0.18779931950031642\n",
      "before pessimistic update:  0:13:41.901731\n",
      "correct Indices Mask:  [False False  True ... False False False]\n",
      "incorrect Indices Mask:  [ True  True False ...  True  True  True]\n",
      "after pessimistic update:  0:13:41.908934\n",
      "t:  0 memory allocated: 629714944\n",
      "After WL  0  time elapsed(s):  824\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training weak learner 1 of 15\n",
      "C_t:  [[  1.20659135   1.20659135   1.20659135   1.20659135   1.20659135\n",
      "    1.20659135 -10.85932217   1.20659135   1.20659135   1.20659135]\n",
      " [  1.20659135   1.20659135   1.20659135   1.20659135   1.20659135\n",
      "    1.20659135   1.20659135   1.20659135   1.20659135 -10.85932217]\n",
      " [  0.82878101   0.82878101   0.82878101   0.82878101   0.82878101\n",
      "    0.82878101   0.82878101   0.82878101   0.82878101  -7.4590291 ]\n",
      " [  1.20659135   1.20659135   1.20659135   1.20659135 -10.85932217\n",
      "    1.20659135   1.20659135   1.20659135   1.20659135   1.20659135]\n",
      " [  1.20659135 -10.85932217   1.20659135   1.20659135   1.20659135\n",
      "    1.20659135   1.20659135   1.20659135   1.20659135   1.20659135]\n",
      " [  0.82878101  -7.4590291    0.82878101   0.82878101   0.82878101\n",
      "    0.82878101   0.82878101   0.82878101   0.82878101   0.82878101]\n",
      " [  1.20659135   1.20659135 -10.85932217   1.20659135   1.20659135\n",
      "    1.20659135   1.20659135   1.20659135   1.20659135   1.20659135]\n",
      " [  0.82878101   0.82878101   0.82878101   0.82878101   0.82878101\n",
      "    0.82878101   0.82878101  -7.4590291    0.82878101   0.82878101]\n",
      " [  1.20659135   1.20659135   1.20659135   1.20659135   1.20659135\n",
      "    1.20659135   1.20659135   1.20659135 -10.85932217   1.20659135]\n",
      " [  1.20659135   1.20659135   1.20659135 -10.85932217   1.20659135\n",
      "    1.20659135   1.20659135   1.20659135   1.20659135   1.20659135]]\n",
      "targets:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3])\n",
      "f:  [[0.18779932 0.18779932 0.18779932 0.18779932 0.18779932 0.18779932\n",
      "  0.         0.18779932 0.18779932 0.18779932]\n",
      " [0.18779932 0.18779932 0.18779932 0.18779932 0.18779932 0.18779932\n",
      "  0.18779932 0.18779932 0.18779932 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.18779932]\n",
      " [0.18779932 0.18779932 0.18779932 0.18779932 0.         0.18779932\n",
      "  0.18779932 0.18779932 0.18779932 0.18779932]\n",
      " [0.18779932 0.         0.18779932 0.18779932 0.18779932 0.18779932\n",
      "  0.18779932 0.18779932 0.18779932 0.18779932]\n",
      " [0.         0.18779932 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.18779932 0.18779932 0.         0.18779932 0.18779932 0.18779932\n",
      "  0.18779932 0.18779932 0.18779932 0.18779932]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.18779932 0.         0.        ]\n",
      " [0.18779932 0.18779932 0.18779932 0.18779932 0.18779932 0.18779932\n",
      "  0.18779932 0.18779932 0.         0.18779932]\n",
      " [0.18779932 0.18779932 0.18779932 0.         0.18779932 0.18779932\n",
      "  0.18779932 0.18779932 0.18779932 0.18779932]]\n",
      "fexp:  [[1.20659135 1.20659135 1.20659135 1.20659135 1.20659135 1.20659135\n",
      "  0.         1.20659135 1.20659135 1.20659135]\n",
      " [1.20659135 1.20659135 1.20659135 1.20659135 1.20659135 1.20659135\n",
      "  1.20659135 1.20659135 1.20659135 0.        ]\n",
      " [0.82878101 0.82878101 0.82878101 0.82878101 0.82878101 0.82878101\n",
      "  0.82878101 0.82878101 0.82878101 0.        ]\n",
      " [1.20659135 1.20659135 1.20659135 1.20659135 0.         1.20659135\n",
      "  1.20659135 1.20659135 1.20659135 1.20659135]\n",
      " [1.20659135 0.         1.20659135 1.20659135 1.20659135 1.20659135\n",
      "  1.20659135 1.20659135 1.20659135 1.20659135]\n",
      " [0.82878101 0.         0.82878101 0.82878101 0.82878101 0.82878101\n",
      "  0.82878101 0.82878101 0.82878101 0.82878101]\n",
      " [1.20659135 1.20659135 0.         1.20659135 1.20659135 1.20659135\n",
      "  1.20659135 1.20659135 1.20659135 1.20659135]\n",
      " [0.82878101 0.82878101 0.82878101 0.82878101 0.82878101 0.82878101\n",
      "  0.82878101 0.         0.82878101 0.82878101]\n",
      " [1.20659135 1.20659135 1.20659135 1.20659135 1.20659135 1.20659135\n",
      "  1.20659135 1.20659135 0.         1.20659135]\n",
      " [1.20659135 1.20659135 1.20659135 0.         1.20659135 1.20659135\n",
      "  1.20659135 1.20659135 1.20659135 1.20659135]]\n",
      "Epoch \t Seconds \t LR \t \t Train Loss \t Train Acc\n",
      "train_loader: <torch.utils.data.dataloader.DataLoader object at 0x7f06a9f320f0>\n",
      "Progress: 10000,  val accuracy: 0.2000\n",
      "PGD accuracy: []\n",
      "Progress: 20000,  val accuracy: 0.3200\n",
      "PGD accuracy: []\n",
      "Progress: 30000,  val accuracy: 0.3500\n",
      "PGD accuracy: []\n",
      "Progress: 40000,  val accuracy: 0.3800\n",
      "PGD accuracy: []\n",
      "Progress: 50000,  val accuracy: 0.4200\n",
      "PGD accuracy: []\n",
      "After fit function:  0:15:02.477668\n",
      "Test accuracy of weak learner:  0.4358\n",
      "Training accuracy of weak learner:  0.4328\n",
      "After train/test acc:  0:15:21.343973\n"
     ]
    }
   ],
   "source": [
    "# from Boosting import SchapireWongMulticlassBoosting\n",
    "from AdversarialAttacks import attack_fgsm, attack_pgd\n",
    "import gc\n",
    "# Ensemble.record_accuracies, Ensemble.calc_accuracies, attack_fgsm, Ensemble.schapireContinuousPredict\n",
    "for i in range(len(maxSamples_vals)):\n",
    "    print(\"len(maxSamples_vals):\", len(maxSamples_vals))\n",
    "    maxSamples = maxSamples_vals[i]\n",
    "    print(\"maxSamples:\", maxSamples)\n",
    "    ensemble = runBoosting(num_wl, maxSamples, dataset=datasets.CIFAR10, weakLearnerType = WongBasedTrainingCIFAR10, val_attacks = [], \n",
    "                               attack_eps_nn=epsilons, attack_eps_ensemble=epsilons, train_eps_nn=train_eps_nn, adv_train_prefix=num_wl,\n",
    "                               batch_size=batch_size, val_flag=False, model_base=PreActResNet18, attack_iters=20, restarts=1)\n",
    "    print(\"number of wl in ensemble:\", len(ensemble.weakLearners))\n",
    "    ensembles.append(ensemble)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Plotting if Needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"PGD accuracy over weak learners\")\n",
    "plt.xlabel(\"Weak Learners\")\n",
    "plt.ylabel(\"PGD accuracy\")\n",
    "plt.plot([0.44428571428571423, 0.4492857142857143, 0.4685714285714286, 0.45714285714285713, 0.4814285714285714, 0.48571428571428577, 0.48214285714285704, 0.48214285714285715], label=\"Eps=0.127\")\n",
    "plt.legend()\n",
    "plt.savefig(dpi=150, fname=\"./results/plots/cifar10/train_eps_8/attack_pgd/adv_acc_maxSamples_750009.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Checking PGD Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestNN = WongBasedTrainingCIFAR10(attack_eps=[0.127])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TestNN.model.load_state_dict(torch.load(\"./models/750005Eps8/wl_0.pth\"))\n",
    "TestNN.model.load_state_dict(torch.load(\"./models/wongBaseline15E.pth\"))\n",
    "TestNN.model.cuda()\n",
    "TestNN.model.eval()\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2471, 0.2435, 0.2616)),\n",
    "])\n",
    "\n",
    "# Normalize the test set same as training set without augmentation\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2471, 0.2435, 0.2616)),\n",
    "])\n",
    "\n",
    "trainset = datasets.CIFAR10(\n",
    "    root=\"./data\", train=True, download=True, transform=transform_train)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=100, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = datasets.CIFAR10(\n",
    "    root=\"./data\", train=False, download=True, transform=transform_test)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=1000, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check a single WL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.455\n",
      "0.44\n",
      "0.443\n",
      "0.44\n",
      "0.447\n",
      "0.428\n",
      "0.418\n",
      "0.453\n",
      "0.459\n",
      "0.471\n",
      "Overall acc:  0.44539999999999996\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "accs = []\n",
    "for data in test_loader:\n",
    "    X = data[0]\n",
    "    y = data[1]\n",
    "    losses, acc = TestNN.calc_accuracies(X.cuda(), y.cuda(), val_attacks=[attack_pgd], attack_iters=20)\n",
    "    print(acc[attack_pgd][0])\n",
    "    accs.append(acc[attack_pgd][0])\n",
    "    \n",
    "    if i>20:\n",
    "        break\n",
    "    i+=1\n",
    "print(\"Overall acc: \", sum(accs)/len(accs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check for PGD (single weak learner and ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgd_steps = [20, 40, 60, 80, 100]\n",
    "# pgd_steps = [20]\n",
    "attack_eps = [0.03]\n",
    "num_wl = 1\n",
    "from Testing import testEnsemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2471, 0.2435, 0.2616)),\n",
    "])\n",
    "\n",
    "# Normalize the test set same as training set without augmentation\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2471, 0.2435, 0.2616)),\n",
    "])\n",
    "\n",
    "trainset = datasets.CIFAR10(\n",
    "    root=\"./data\", train=True, download=True, transform=transform_train)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=100, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = datasets.CIFAR10(\n",
    "    root=\"./data\", train=False, download=True, transform=transform_test)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=1000, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test pgd on weak learners\n",
    "wl_accuracies = []\n",
    "for attack_iters in pgd_steps:\n",
    "    TestNN = WongBasedTrainingCIFAR10(attack_eps=attack_eps)\n",
    "    TestNN.model.load_state_dict(torch.load(\"./models/750000Eps8/wl_0.pth\"))\n",
    "    TestNN.model.cuda()\n",
    "    TestNN.model.eval()\n",
    "    print(\"\")\n",
    "    \n",
    "    i = 0\n",
    "#     accs = []\n",
    "    for data in test_loader:\n",
    "        X = data[0]\n",
    "        y = data[1]\n",
    "        losses, acc = TestNN.calc_accuracies(X.cuda(), y.cuda(), val_attacks=[attack_pgd], attack_iters=attack_iters)\n",
    "        print(acc)\n",
    "        wl_accuracies.append(acc[attack_pgd])\n",
    "\n",
    "    #     if i>5:\n",
    "        break\n",
    "        i+=1\n",
    "print(\"pgd_steps:\", pgd_steps)\n",
    "print(\"wl_accuracies:\", wl_accuracies)\n",
    "plt.subplots()\n",
    "plt.plot(pgd_steps, wl_accuracies)\n",
    "plt.xlabel('PGD iterations')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Weak learner accuracy vs PGD iterations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'val'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-7c3dce3759fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#         print(\"before ens acc\", ensemble.accuracies)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mensemble\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_accuracies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_attacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mattack_pgd\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattack_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattack_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ensemble accuracies:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensemble\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/storage/AdversarialBoostingNeuralNets/Ensemble.py\u001b[0m in \u001b[0;36mrecord_accuracies\u001b[0;34m(self, progress, train_loader, test_loader, numsamples_train, numsamples_val, val_attacks, attack_iters)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0mval_loss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss_dicts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mval_acc_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_acc_dicts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_acc_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_loss_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'val'"
     ]
    }
   ],
   "source": [
    "# Test pgd on ensemble (1 wl)\n",
    "for attack_iters in pgd_steps:\n",
    "    ensemble = Ensemble(weakLearners=[], weakLearnerWeights=[], weakLearnerType=WongNeuralNetCIFAR10, attack_eps=[0.03])\n",
    "    ensemble.losses[\"val\"]\n",
    "    for i in range(1):\n",
    "        ensemble.addWeakLearner(\"./models/750000Eps8/wl_0.pth\", 1)\n",
    "#         print(\"before ens acc\", ensemble.accuracies)\n",
    "\n",
    "        ensemble.record_accuracies(i, train_loader, test_loader, 1000, 1000, val_attacks=[attack_pgd], attack_iters=attack_iters)\n",
    "        print(\"ensemble accuracies:\", ensemble.accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Weak Learner  0 .  Time Elapsed (s):  0\n",
      "Weak Learner  1 .  Time Elapsed (s):  0\n",
      "Weak Learner  2 .  Time Elapsed (s):  0\n",
      "Weak Learner  3 .  Time Elapsed (s):  0\n",
      "pgd called with 0.127 0.01 20 1\n",
      "{'val': 0.71, <function attack_pgd at 0x7f20b4390400>: [0.39]}\n",
      "pgd called with 0.127 0.01 20 1\n",
      "{'val': 0.73, <function attack_pgd at 0x7f20b4390400>: [0.41]}\n",
      "pgd called with 0.127 0.01 20 1\n",
      "{'val': 0.74, <function attack_pgd at 0x7f20b4390400>: [0.47]}\n",
      "ensemble accuracies: {'train': [0.7466666666666667], 'val': [0.7266666666666666], 'attack_fgsm': [], 'attack_pgd': [[0.42333333333333334]], 'wl_train': [], 'wl_val': []}\n",
      "Weak Learner  4 .  Time Elapsed (s):  60\n",
      "Weak Learner  5 .  Time Elapsed (s):  60\n",
      "Weak Learner  6 .  Time Elapsed (s):  60\n",
      "Weak Learner  7 .  Time Elapsed (s):  60\n",
      "Weak Learner  8 .  Time Elapsed (s):  60\n",
      "Weak Learner  9 .  Time Elapsed (s):  60\n",
      "Weak Learner  10 .  Time Elapsed (s):  60\n",
      "Weak Learner  11 .  Time Elapsed (s):  60\n",
      "Weak Learner  12 .  Time Elapsed (s):  60\n",
      "Weak Learner  13 .  Time Elapsed (s):  60\n",
      "Weak Learner  14 .  Time Elapsed (s):  60\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'accuracies' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-e6563a780e5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mattackStr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"attack_pgd\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mensemble\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestEnsemble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mattack\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_wl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumsamples_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumsamples_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattack_eps_ensemble\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepsilons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattack_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattack_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0maccuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensemble\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mattackStr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cur acc:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'accuracies' is not defined"
     ]
    }
   ],
   "source": [
    "# test pgd on ensemble (more wl)\n",
    "num_wl = 15\n",
    "train_eps_nn = 8\n",
    "for attack_iters in pgd_steps:\n",
    "    path = f'./models/{maxSamples_vals[0]}Eps{train_eps_nn}/'\n",
    "    attack=attack_pgd\n",
    "    attackStr=\"attack_pgd\"\n",
    "    ensemble = testEnsemble(path, [attack], num_wl, numsamples_train=400, numsamples_val=400, attack_eps_ensemble=epsilons, attack_iters=attack_iters)\n",
    "    accuracies.append(ensemble.accuracies[attackStr])\n",
    "    print(\"cur acc:\", accuracies[-1])\n",
    "    \n",
    "    \n",
    "#     maxSamples = maxSamples_vals[0]\n",
    "#     resultsPath = f'results/plots/cifar10/train_eps_{train_eps_nn}_iter_{attack_iters}/{attackStr}/'\n",
    "#     acc_file = resultsPath + f'acc_maxSamples_{maxSamples}.png'\n",
    "#     adv_acc_file = resultsPath + f'adv_acc_maxSamples_{maxSamples}.png'\n",
    "#     loss_file = resultsPath + f'loss_maxSamples_{maxSamples}.png'\n",
    "#     # wl_train_acc_file = resultsPath + f'wl_train_acc_maxSamples_{maxSamples}.png'\n",
    "#     ensemble.plot_accuracies(acc_file)\n",
    "#     ensemble.plot_loss(loss_file)\n",
    "#     ensemble.plot_adversarial_accuracies(adv_acc_file)\n",
    "    # ensemble.plot_wl_acc(wl_train_acc_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
