{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "from WeakLearners import WongNeuralNetCIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch_memlab in /usr/local/lib/python3.6/dist-packages (0.2.2)\n",
      "Requirement already satisfied: calmsize in /usr/local/lib/python3.6/dist-packages (from pytorch_memlab) (0.1.3)\n",
      "Requirement already satisfied: pandas>=0.18 in /usr/local/lib/python3.6/dist-packages (from pytorch_memlab) (0.25.0)\n",
      "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.6/dist-packages (from pytorch_memlab) (1.7.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytorch_memlab) (41.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.18->pytorch_memlab) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.18->pytorch_memlab) (2019.2)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.18->pytorch_memlab) (1.17.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.4->pytorch_memlab) (3.7.4.3)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.4->pytorch_memlab) (0.17.1)\n",
      "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.4->pytorch_memlab) (0.7)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas>=0.18->pytorch_memlab) (1.12.0)\n",
      "\u001b[33mWARNING: You are using pip version 19.2.1, however version 21.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch_memlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Boosting import Ensemble, runBoosting\n",
    "from AdversarialAttacks import attack_fgsm, attack_pgd\n",
    "from pytorch_memlab import LineProfiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR10 Boosting (Adversarial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maxSamples_vals = [100000, 200000, 500000]\n",
    "maxSamples_vals = [750006]\n",
    "# maxSamples_vals = [30003]\n",
    "# maxSamples_vals = [500000, 1000000]\n",
    "# done for 50K, 100K\n",
    "# maxSamples_vals = [1000000, 2000000]\n",
    "# maxSamples_vals = [123456]\n",
    "batch_size=100\n",
    "# maxSamples_vals = [batch_size + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_wl = 15 # maybe around 100? #later: maybe change this to an array?\n",
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensembles = []\n",
    "# epsilons = [0.0, 0.01, 0.02, 0.03, 0.05, 0.1]\n",
    "epsilons = [0.127]\n",
    "train_eps_nn = 8\n",
    "# epsilons = []\n",
    "# epsilons = [0.0, 0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(maxSamples_vals): 1\n",
      "maxSamples: 750006\n",
      "attack_eps_nn:  [0.127]\n",
      "attack_eps_ensemble:  [0.127]\n",
      "train_eps_nn:  8\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "attack eps ens [0.127]\n",
      "path_head: ./models/750006Eps8/\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training weak learner 0\n",
      "C_t:  [[ 1.  1.  1.  1.  1.  1. -9.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1. -9.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1. -9.]\n",
      " [ 1.  1.  1.  1. -9.  1.  1.  1.  1.  1.]\n",
      " [ 1. -9.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      " [ 1. -9.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      " [ 1.  1. -9.  1.  1.  1.  1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1. -9.  1.  1.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1. -9.  1.]\n",
      " [ 1.  1.  1. -9.  1.  1.  1.  1.  1.  1.]]\n",
      "targets:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3])\n",
      "f:  [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "fexp:  [[1. 1. 1. 1. 1. 1. 0. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
      " [1. 1. 1. 1. 0. 1. 1. 1. 1. 1.]\n",
      " [1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 0. 1.]\n",
      " [1. 1. 1. 0. 1. 1. 1. 1. 1. 1.]]\n",
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/cifar.py:118: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  img, target = self.data[index], self.targets[index]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 10000,  val accuracy: 0.2200\n",
      "Progress: 20000,  val accuracy: 0.2800\n",
      "Progress: 30000,  val accuracy: 0.2700\n",
      "Progress: 40000,  val accuracy: 0.2800\n",
      "Progress: 50000,  val accuracy: 0.2800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:893: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  \"please use `get_last_lr()`.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Progress: 60000,  val accuracy: 0.3300\n",
      "Progress: 70000,  val accuracy: 0.3200\n",
      "Progress: 80000,  val accuracy: 0.3200\n",
      "Progress: 90000,  val accuracy: 0.3800\n",
      "Progress: 100000,  val accuracy: 0.3700\n",
      "Epoch 2\n",
      "Progress: 110000,  val accuracy: 0.4200\n",
      "Progress: 120000,  val accuracy: 0.3800\n",
      "Progress: 130000,  val accuracy: 0.4500\n",
      "Progress: 140000,  val accuracy: 0.4000\n",
      "Progress: 150000,  val accuracy: 0.3800\n",
      "Epoch 3\n",
      "Progress: 160000,  val accuracy: 0.4300\n",
      "Progress: 170000,  val accuracy: 0.4300\n",
      "Progress: 180000,  val accuracy: 0.4600\n",
      "Progress: 190000,  val accuracy: 0.4800\n",
      "Progress: 200000,  val accuracy: 0.5200\n",
      "Epoch 4\n",
      "Progress: 210000,  val accuracy: 0.5200\n",
      "Progress: 220000,  val accuracy: 0.4700\n",
      "Progress: 230000,  val accuracy: 0.4600\n",
      "Progress: 240000,  val accuracy: 0.5200\n",
      "Progress: 250000,  val accuracy: 0.4700\n",
      "Epoch 5\n",
      "Progress: 260000,  val accuracy: 0.5200\n",
      "Progress: 270000,  val accuracy: 0.5100\n",
      "Progress: 280000,  val accuracy: 0.6100\n",
      "Progress: 290000,  val accuracy: 0.5200\n",
      "Progress: 300000,  val accuracy: 0.6600\n",
      "Epoch 6\n",
      "Progress: 310000,  val accuracy: 0.5300\n",
      "Progress: 320000,  val accuracy: 0.6700\n",
      "Progress: 330000,  val accuracy: 0.6100\n",
      "Progress: 340000,  val accuracy: 0.6100\n",
      "Progress: 350000,  val accuracy: 0.6100\n",
      "Epoch 7\n",
      "Progress: 360000,  val accuracy: 0.6000\n",
      "Progress: 370000,  val accuracy: 0.6700\n",
      "Progress: 380000,  val accuracy: 0.6200\n",
      "Progress: 390000,  val accuracy: 0.6000\n",
      "Progress: 400000,  val accuracy: 0.5800\n",
      "Epoch 8\n",
      "Progress: 410000,  val accuracy: 0.6300\n",
      "Progress: 420000,  val accuracy: 0.6700\n",
      "Progress: 430000,  val accuracy: 0.6200\n",
      "Progress: 440000,  val accuracy: 0.6400\n",
      "Progress: 450000,  val accuracy: 0.6600\n",
      "Epoch 9\n",
      "Progress: 460000,  val accuracy: 0.6900\n",
      "Progress: 470000,  val accuracy: 0.6000\n",
      "Progress: 480000,  val accuracy: 0.6600\n",
      "Progress: 490000,  val accuracy: 0.6900\n",
      "Progress: 500000,  val accuracy: 0.6900\n",
      "Epoch 10\n",
      "Progress: 510000,  val accuracy: 0.6400\n",
      "Progress: 520000,  val accuracy: 0.7100\n",
      "Progress: 530000,  val accuracy: 0.7200\n",
      "Progress: 540000,  val accuracy: 0.7200\n",
      "Progress: 550000,  val accuracy: 0.6800\n",
      "Epoch 11\n",
      "Progress: 560000,  val accuracy: 0.6900\n",
      "Progress: 570000,  val accuracy: 0.7000\n",
      "Progress: 580000,  val accuracy: 0.6800\n",
      "Progress: 590000,  val accuracy: 0.7200\n",
      "Progress: 600000,  val accuracy: 0.6900\n",
      "Epoch 12\n",
      "Progress: 610000,  val accuracy: 0.7200\n",
      "Progress: 620000,  val accuracy: 0.7600\n",
      "Progress: 630000,  val accuracy: 0.7000\n",
      "Progress: 640000,  val accuracy: 0.7300\n",
      "Progress: 650000,  val accuracy: 0.7200\n",
      "Epoch 13\n",
      "Progress: 660000,  val accuracy: 0.6900\n",
      "Progress: 670000,  val accuracy: 0.8200\n",
      "Progress: 680000,  val accuracy: 0.7400\n",
      "Progress: 690000,  val accuracy: 0.8000\n",
      "Progress: 700000,  val accuracy: 0.7900\n",
      "Epoch 14\n",
      "Progress: 710000,  val accuracy: 0.7200\n",
      "Progress: 720000,  val accuracy: 0.7100\n",
      "Progress: 730000,  val accuracy: 0.7500\n",
      "Progress: 740000,  val accuracy: 0.7400\n",
      "Progress: 750000,  val accuracy: 0.7200\n",
      "Epoch 15\n",
      "Total training time1426.5041456222534\n",
      "After fit function:  0:23:46.670614\n",
      "Test accuracy of weak learner:  0.6814\n",
      "Training accuracy of weak learner:  0.70766\n",
      "After train/test acc:  0:24:05.770619\n",
      "After allindices:  0:34:50.172367\n",
      "Predictions:  [5. 9. 9. 6. 1. 1. 2. 7. 8. 7.]\n",
      "Alpha:  0.3465735902799726\n",
      "before pessimistic update:  0:34:50.175048\n",
      "correct Indices Mask:  [False  True  True ...  True  True False]\n",
      "incorrect Indices Mask:  [ True False False ... False False  True]\n",
      "after pessimistic update:  0:34:50.181778\n",
      "t:  0 memory allocated: 142938624\n",
      "After WL  0  time elapsed(s):  2093\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training weak learner 1\n",
      "C_t:  [[  1.41421356   1.41421356   1.41421356   1.41421356   1.41421356\n",
      "    1.41421356 -12.72792206   1.41421356   1.41421356   1.41421356]\n",
      " [  0.70710678   0.70710678   0.70710678   0.70710678   0.70710678\n",
      "    0.70710678   0.70710678   0.70710678   0.70710678  -6.36396103]\n",
      " [  0.70710678   0.70710678   0.70710678   0.70710678   0.70710678\n",
      "    0.70710678   0.70710678   0.70710678   0.70710678  -6.36396103]\n",
      " [  1.41421356   1.41421356   1.41421356   1.41421356 -12.72792206\n",
      "    1.41421356   1.41421356   1.41421356   1.41421356   1.41421356]\n",
      " [  0.70710678  -6.36396103   0.70710678   0.70710678   0.70710678\n",
      "    0.70710678   0.70710678   0.70710678   0.70710678   0.70710678]\n",
      " [  0.70710678  -6.36396103   0.70710678   0.70710678   0.70710678\n",
      "    0.70710678   0.70710678   0.70710678   0.70710678   0.70710678]\n",
      " [  0.70710678   0.70710678  -6.36396103   0.70710678   0.70710678\n",
      "    0.70710678   0.70710678   0.70710678   0.70710678   0.70710678]\n",
      " [  0.70710678   0.70710678   0.70710678   0.70710678   0.70710678\n",
      "    0.70710678   0.70710678  -6.36396103   0.70710678   0.70710678]\n",
      " [  0.70710678   0.70710678   0.70710678   0.70710678   0.70710678\n",
      "    0.70710678   0.70710678   0.70710678  -6.36396103   0.70710678]\n",
      " [  1.41421356   1.41421356   1.41421356 -12.72792206   1.41421356\n",
      "    1.41421356   1.41421356   1.41421356   1.41421356   1.41421356]]\n",
      "targets:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3])\n",
      "f:  [[0.34657359 0.34657359 0.34657359 0.34657359 0.34657359 0.34657359\n",
      "  0.         0.34657359 0.34657359 0.34657359]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.34657359]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.34657359]\n",
      " [0.34657359 0.34657359 0.34657359 0.34657359 0.         0.34657359\n",
      "  0.34657359 0.34657359 0.34657359 0.34657359]\n",
      " [0.         0.34657359 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.34657359 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.34657359 0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.34657359 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.34657359 0.        ]\n",
      " [0.34657359 0.34657359 0.34657359 0.         0.34657359 0.34657359\n",
      "  0.34657359 0.34657359 0.34657359 0.34657359]]\n",
      "fexp:  [[1.41421356 1.41421356 1.41421356 1.41421356 1.41421356 1.41421356\n",
      "  0.         1.41421356 1.41421356 1.41421356]\n",
      " [0.70710678 0.70710678 0.70710678 0.70710678 0.70710678 0.70710678\n",
      "  0.70710678 0.70710678 0.70710678 0.        ]\n",
      " [0.70710678 0.70710678 0.70710678 0.70710678 0.70710678 0.70710678\n",
      "  0.70710678 0.70710678 0.70710678 0.        ]\n",
      " [1.41421356 1.41421356 1.41421356 1.41421356 0.         1.41421356\n",
      "  1.41421356 1.41421356 1.41421356 1.41421356]\n",
      " [0.70710678 0.         0.70710678 0.70710678 0.70710678 0.70710678\n",
      "  0.70710678 0.70710678 0.70710678 0.70710678]\n",
      " [0.70710678 0.         0.70710678 0.70710678 0.70710678 0.70710678\n",
      "  0.70710678 0.70710678 0.70710678 0.70710678]\n",
      " [0.70710678 0.70710678 0.         0.70710678 0.70710678 0.70710678\n",
      "  0.70710678 0.70710678 0.70710678 0.70710678]\n",
      " [0.70710678 0.70710678 0.70710678 0.70710678 0.70710678 0.70710678\n",
      "  0.70710678 0.         0.70710678 0.70710678]\n",
      " [0.70710678 0.70710678 0.70710678 0.70710678 0.70710678 0.70710678\n",
      "  0.70710678 0.70710678 0.         0.70710678]\n",
      " [1.41421356 1.41421356 1.41421356 0.         1.41421356 1.41421356\n",
      "  1.41421356 1.41421356 1.41421356 1.41421356]]\n",
      "Epoch 0\n",
      "Progress: 10000,  val accuracy: 0.2500\n",
      "Progress: 20000,  val accuracy: 0.3200\n",
      "Progress: 30000,  val accuracy: 0.2200\n",
      "Progress: 40000,  val accuracy: 0.3300\n",
      "Progress: 50000,  val accuracy: 0.3100\n",
      "Epoch 1\n",
      "Progress: 60000,  val accuracy: 0.3300\n",
      "Progress: 70000,  val accuracy: 0.2800\n",
      "Progress: 80000,  val accuracy: 0.3500\n",
      "Progress: 90000,  val accuracy: 0.3300\n",
      "Progress: 100000,  val accuracy: 0.2800\n",
      "Epoch 2\n",
      "Progress: 110000,  val accuracy: 0.4000\n",
      "Progress: 120000,  val accuracy: 0.3600\n",
      "Progress: 130000,  val accuracy: 0.3700\n",
      "Progress: 140000,  val accuracy: 0.3200\n",
      "Progress: 150000,  val accuracy: 0.3900\n",
      "Epoch 3\n",
      "Progress: 160000,  val accuracy: 0.4000\n",
      "Progress: 170000,  val accuracy: 0.4100\n",
      "Progress: 180000,  val accuracy: 0.4300\n",
      "Progress: 190000,  val accuracy: 0.4200\n",
      "Progress: 200000,  val accuracy: 0.4500\n",
      "Epoch 4\n",
      "Progress: 210000,  val accuracy: 0.4100\n",
      "Progress: 220000,  val accuracy: 0.4500\n",
      "Progress: 230000,  val accuracy: 0.4000\n",
      "Progress: 240000,  val accuracy: 0.4800\n",
      "Progress: 250000,  val accuracy: 0.4200\n",
      "Epoch 5\n",
      "Progress: 260000,  val accuracy: 0.5000\n",
      "Progress: 270000,  val accuracy: 0.4700\n",
      "Progress: 280000,  val accuracy: 0.4700\n",
      "Progress: 290000,  val accuracy: 0.5700\n",
      "Progress: 300000,  val accuracy: 0.5000\n",
      "Epoch 6\n",
      "Progress: 310000,  val accuracy: 0.5300\n",
      "Progress: 320000,  val accuracy: 0.4800\n",
      "Progress: 330000,  val accuracy: 0.4300\n",
      "Progress: 340000,  val accuracy: 0.4800\n",
      "Progress: 350000,  val accuracy: 0.5200\n",
      "Epoch 7\n",
      "Progress: 360000,  val accuracy: 0.5300\n",
      "Progress: 370000,  val accuracy: 0.4700\n",
      "Progress: 380000,  val accuracy: 0.6000\n",
      "Progress: 390000,  val accuracy: 0.4900\n",
      "Progress: 400000,  val accuracy: 0.5600\n",
      "Epoch 8\n",
      "Progress: 410000,  val accuracy: 0.5400\n",
      "Progress: 420000,  val accuracy: 0.5600\n",
      "Progress: 430000,  val accuracy: 0.5500\n",
      "Progress: 440000,  val accuracy: 0.6000\n",
      "Progress: 450000,  val accuracy: 0.5900\n",
      "Epoch 9\n",
      "Progress: 460000,  val accuracy: 0.5700\n",
      "Progress: 470000,  val accuracy: 0.5000\n",
      "Progress: 480000,  val accuracy: 0.5600\n",
      "Progress: 490000,  val accuracy: 0.5500\n",
      "Progress: 500000,  val accuracy: 0.5500\n",
      "Epoch 10\n",
      "Progress: 510000,  val accuracy: 0.5700\n",
      "Progress: 590000,  val accuracy: 0.6500\n",
      "Progress: 600000,  val accuracy: 0.6300\n",
      "Epoch 12\n",
      "Progress: 610000,  val accuracy: 0.6400\n",
      "Progress: 620000,  val accuracy: 0.5800\n",
      "Progress: 630000,  val accuracy: 0.6400\n",
      "Progress: 640000,  val accuracy: 0.6300\n",
      "Progress: 650000,  val accuracy: 0.6000\n",
      "Epoch 13\n",
      "Progress: 660000,  val accuracy: 0.6200\n",
      "Progress: 670000,  val accuracy: 0.6300\n",
      "Progress: 680000,  val accuracy: 0.5900\n",
      "Progress: 690000,  val accuracy: 0.6100\n",
      "Progress: 700000,  val accuracy: 0.6800\n",
      "Epoch 14\n",
      "Progress: 710000,  val accuracy: 0.6100\n",
      "Progress: 720000,  val accuracy: 0.5900\n",
      "Progress: 730000,  val accuracy: 0.5700\n",
      "Progress: 740000,  val accuracy: 0.6400\n",
      "Progress: 750000,  val accuracy: 0.7000\n",
      "Epoch 15\n",
      "Total training time1438.2260446548462\n",
      "After fit function:  0:58:51.681191\n",
      "Test accuracy of weak learner:  0.6977\n",
      "Training accuracy of weak learner:  0.7204\n",
      "After train/test acc:  0:59:10.564421\n",
      "After allindices:  1:09:54.655768\n",
      "Predictions:  [5. 9. 7. 6. 1. 1. 4. 7. 0. 7.]\n",
      "Alpha:  0.2141886394353441\n",
      "before pessimistic update:  1:09:54.658855\n",
      "correct Indices Mask:  [False  True False ...  True  True False]\n",
      "incorrect Indices Mask:  [ True False  True ... False False  True]\n",
      "after pessimistic update:  1:09:54.665741\n",
      "t:  1 memory allocated: 141070848\n",
      "After WL  1  time elapsed(s):  4198\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training weak learner 2\n",
      "C_t:  [[  1.75200742   1.75200742   1.75200742   1.75200742   1.75200742\n",
      "    1.75200742 -15.76806681   1.75200742   1.75200742   1.75200742]\n",
      " [  0.57077384   0.57077384   0.57077384   0.57077384   0.57077384\n",
      "    0.57077384   0.57077384   0.57077384   0.57077384  -5.13696453]\n",
      " [  0.87600371   0.87600371   0.87600371   0.87600371   0.87600371\n",
      "    0.87600371   0.87600371   0.87600371   0.87600371  -7.88403341]\n",
      " [  1.75200742   1.75200742   1.75200742   1.75200742 -15.76806681\n",
      "    1.75200742   1.75200742   1.75200742   1.75200742   1.75200742]\n",
      " [  0.57077384  -5.13696453   0.57077384   0.57077384   0.57077384\n",
      "    0.57077384   0.57077384   0.57077384   0.57077384   0.57077384]\n",
      " [  0.57077384  -5.13696453   0.57077384   0.57077384   0.57077384\n",
      "    0.57077384   0.57077384   0.57077384   0.57077384   0.57077384]\n",
      " [  0.87600371   0.87600371  -7.88403341   0.87600371   0.87600371\n",
      "    0.87600371   0.87600371   0.87600371   0.87600371   0.87600371]\n",
      " [  0.57077384   0.57077384   0.57077384   0.57077384   0.57077384\n",
      "    0.57077384   0.57077384  -5.13696453   0.57077384   0.57077384]\n",
      " [  0.87600371   0.87600371   0.87600371   0.87600371   0.87600371\n",
      "    0.87600371   0.87600371   0.87600371  -7.88403341   0.87600371]\n",
      " [  1.75200742   1.75200742   1.75200742 -15.76806681   1.75200742\n",
      "    1.75200742   1.75200742   1.75200742   1.75200742   1.75200742]]\n",
      "targets:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3])\n",
      "f:  [[0.56076223 0.56076223 0.56076223 0.56076223 0.56076223 0.56076223\n",
      "  0.         0.56076223 0.56076223 0.56076223]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.56076223]\n",
      " [0.21418864 0.21418864 0.21418864 0.21418864 0.21418864 0.21418864\n",
      "  0.21418864 0.21418864 0.21418864 0.34657359]\n",
      " [0.56076223 0.56076223 0.56076223 0.56076223 0.         0.56076223\n",
      "  0.56076223 0.56076223 0.56076223 0.56076223]\n",
      " [0.         0.56076223 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.56076223 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.21418864 0.21418864 0.34657359 0.21418864 0.21418864 0.21418864\n",
      "  0.21418864 0.21418864 0.21418864 0.21418864]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.56076223 0.         0.        ]\n",
      " [0.21418864 0.21418864 0.21418864 0.21418864 0.21418864 0.21418864\n",
      "  0.21418864 0.21418864 0.34657359 0.21418864]\n",
      " [0.56076223 0.56076223 0.56076223 0.         0.56076223 0.56076223\n",
      "  0.56076223 0.56076223 0.56076223 0.56076223]]\n",
      "fexp:  [[1.75200742 1.75200742 1.75200742 1.75200742 1.75200742 1.75200742\n",
      "  0.         1.75200742 1.75200742 1.75200742]\n",
      " [0.57077384 0.57077384 0.57077384 0.57077384 0.57077384 0.57077384\n",
      "  0.57077384 0.57077384 0.57077384 0.        ]\n",
      " [0.87600371 0.87600371 0.87600371 0.87600371 0.87600371 0.87600371\n",
      "  0.87600371 0.87600371 0.87600371 0.        ]\n",
      " [1.75200742 1.75200742 1.75200742 1.75200742 0.         1.75200742\n",
      "  1.75200742 1.75200742 1.75200742 1.75200742]\n",
      " [0.57077384 0.         0.57077384 0.57077384 0.57077384 0.57077384\n",
      "  0.57077384 0.57077384 0.57077384 0.57077384]\n",
      " [0.57077384 0.         0.57077384 0.57077384 0.57077384 0.57077384\n",
      "  0.57077384 0.57077384 0.57077384 0.57077384]\n",
      " [0.87600371 0.87600371 0.         0.87600371 0.87600371 0.87600371\n",
      "  0.87600371 0.87600371 0.87600371 0.87600371]\n",
      " [0.57077384 0.57077384 0.57077384 0.57077384 0.57077384 0.57077384\n",
      "  0.57077384 0.         0.57077384 0.57077384]\n",
      " [0.87600371 0.87600371 0.87600371 0.87600371 0.87600371 0.87600371\n",
      "  0.87600371 0.87600371 0.         0.87600371]\n",
      " [1.75200742 1.75200742 1.75200742 0.         1.75200742 1.75200742\n",
      "  1.75200742 1.75200742 1.75200742 1.75200742]]\n",
      "Epoch 0\n",
      "Progress: 10000,  val accuracy: 0.2700\n",
      "Progress: 20000,  val accuracy: 0.2900\n",
      "Progress: 30000,  val accuracy: 0.3000\n",
      "Progress: 40000,  val accuracy: 0.2900\n",
      "Progress: 50000,  val accuracy: 0.3000\n",
      "Epoch 1\n",
      "Progress: 60000,  val accuracy: 0.3500\n",
      "Progress: 70000,  val accuracy: 0.3200\n",
      "Progress: 80000,  val accuracy: 0.2400\n",
      "Progress: 90000,  val accuracy: 0.3700\n",
      "Progress: 100000,  val accuracy: 0.3500\n",
      "Epoch 2\n",
      "Progress: 110000,  val accuracy: 0.3200\n",
      "Progress: 120000,  val accuracy: 0.3300\n",
      "Progress: 130000,  val accuracy: 0.3800\n",
      "Progress: 140000,  val accuracy: 0.2700\n",
      "Progress: 150000,  val accuracy: 0.3700\n",
      "Epoch 3\n",
      "Progress: 160000,  val accuracy: 0.3700\n",
      "Progress: 170000,  val accuracy: 0.3400\n",
      "Progress: 180000,  val accuracy: 0.3500\n",
      "Progress: 190000,  val accuracy: 0.3400\n",
      "Progress: 200000,  val accuracy: 0.3800\n",
      "Epoch 4\n",
      "Progress: 210000,  val accuracy: 0.4300\n",
      "Progress: 220000,  val accuracy: 0.3900\n",
      "Progress: 230000,  val accuracy: 0.4300\n",
      "Progress: 240000,  val accuracy: 0.4000\n",
      "Progress: 250000,  val accuracy: 0.4400\n",
      "Epoch 5\n",
      "Progress: 260000,  val accuracy: 0.3700\n",
      "Progress: 270000,  val accuracy: 0.4400\n",
      "Progress: 280000,  val accuracy: 0.4500\n",
      "Progress: 290000,  val accuracy: 0.4500\n",
      "Progress: 300000,  val accuracy: 0.4800\n",
      "Epoch 6\n",
      "Progress: 310000,  val accuracy: 0.4000\n",
      "Progress: 320000,  val accuracy: 0.4800\n",
      "Progress: 330000,  val accuracy: 0.4900\n",
      "Progress: 340000,  val accuracy: 0.5000\n",
      "Progress: 350000,  val accuracy: 0.5100\n",
      "Epoch 7\n",
      "Progress: 360000,  val accuracy: 0.4600\n",
      "Progress: 370000,  val accuracy: 0.5300\n",
      "Progress: 380000,  val accuracy: 0.5200\n",
      "Progress: 390000,  val accuracy: 0.4800\n",
      "Progress: 400000,  val accuracy: 0.5500\n",
      "Epoch 8\n",
      "Progress: 410000,  val accuracy: 0.5100\n",
      "Progress: 420000,  val accuracy: 0.5600\n",
      "Progress: 430000,  val accuracy: 0.5500\n",
      "Progress: 440000,  val accuracy: 0.5200\n",
      "Progress: 450000,  val accuracy: 0.4600\n",
      "Epoch 9\n",
      "Progress: 460000,  val accuracy: 0.6000\n",
      "Progress: 470000,  val accuracy: 0.5400\n",
      "Progress: 480000,  val accuracy: 0.5700\n",
      "Progress: 490000,  val accuracy: 0.5400\n",
      "Progress: 500000,  val accuracy: 0.5400\n",
      "Epoch 10\n",
      "Progress: 510000,  val accuracy: 0.5800\n",
      "Progress: 520000,  val accuracy: 0.5600\n",
      "Progress: 530000,  val accuracy: 0.5600\n",
      "Progress: 540000,  val accuracy: 0.6400\n",
      "Progress: 550000,  val accuracy: 0.5900\n",
      "Epoch 11\n",
      "Progress: 560000,  val accuracy: 0.6100\n",
      "Progress: 570000,  val accuracy: 0.5700\n",
      "Progress: 580000,  val accuracy: 0.5600\n",
      "Progress: 590000,  val accuracy: 0.6100\n",
      "Progress: 600000,  val accuracy: 0.5900\n",
      "Epoch 12\n",
      "Progress: 610000,  val accuracy: 0.6400\n",
      "Progress: 620000,  val accuracy: 0.5800\n",
      "Progress: 630000,  val accuracy: 0.6100\n",
      "Progress: 640000,  val accuracy: 0.6300\n",
      "Progress: 650000,  val accuracy: 0.5800\n",
      "Epoch 13\n",
      "Progress: 660000,  val accuracy: 0.5900\n",
      "Progress: 670000,  val accuracy: 0.5900\n",
      "Progress: 680000,  val accuracy: 0.6400\n",
      "Progress: 690000,  val accuracy: 0.6200\n",
      "Progress: 700000,  val accuracy: 0.6200\n",
      "Epoch 14\n",
      "Progress: 710000,  val accuracy: 0.6700\n",
      "Progress: 720000,  val accuracy: 0.6400\n",
      "Progress: 730000,  val accuracy: 0.6900\n",
      "Progress: 740000,  val accuracy: 0.6200\n",
      "Progress: 750000,  val accuracy: 0.5900\n",
      "Epoch 15\n",
      "Total training time1423.3346121311188\n",
      "After fit function:  1:33:42.371991\n",
      "Test accuracy of weak learner:  0.6634\n",
      "Training accuracy of weak learner:  0.685\n",
      "After train/test acc:  1:34:01.263987\n",
      "After allindices:  1:44:45.549276\n",
      "Predictions:  [5. 9. 9. 6. 1. 1. 4. 7. 8. 9.]\n",
      "Alpha:  0.17923944868862554\n",
      "before pessimistic update:  1:44:45.552302\n",
      "correct Indices Mask:  [False  True  True ...  True False False]\n",
      "incorrect Indices Mask:  [ True False False ... False  True  True]\n",
      "after pessimistic update:  1:44:45.559440\n",
      "t:  2 memory allocated: 143479296\n",
      "After WL  2  time elapsed(s):  6288\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training weak learner 3\n",
      "C_t:  [[  2.09593903   2.09593903   2.09593903   2.09593903   2.09593903\n",
      "    2.09593903 -18.86345129   2.09593903   2.09593903   2.09593903]\n",
      " [  0.47711311   0.47711311   0.47711311   0.47711311   0.47711311\n",
      "    0.47711311   0.47711311   0.47711311   0.47711311  -4.29401803]\n",
      " [  0.73225651   0.73225651   0.73225651   0.73225651   0.73225651\n",
      "    0.73225651   0.73225651   0.73225651   0.73225651  -6.59030861]\n",
      " [  2.09593903   2.09593903   2.09593903   2.09593903 -18.86345129\n",
      "    2.09593903   2.09593903   2.09593903   2.09593903   2.09593903]\n",
      " [  0.47711311  -4.29401803   0.47711311   0.47711311   0.47711311\n",
      "    0.47711311   0.47711311   0.47711311   0.47711311   0.47711311]\n",
      " [  0.47711311  -4.29401803   0.47711311   0.47711311   0.47711311\n",
      "    0.47711311   0.47711311   0.47711311   0.47711311   0.47711311]\n",
      " [  1.04796952   1.04796952  -9.43172565   1.04796952   1.04796952\n",
      "    1.04796952   1.04796952   1.04796952   1.04796952   1.04796952]\n",
      " [  0.47711311   0.47711311   0.47711311   0.47711311   0.47711311\n",
      "    0.47711311   0.47711311  -4.29401803   0.47711311   0.47711311]\n",
      " [  0.73225651   0.73225651   0.73225651   0.73225651   0.73225651\n",
      "    0.73225651   0.73225651   0.73225651  -6.59030861   0.73225651]\n",
      " [  2.09593903   2.09593903   2.09593903 -18.86345129   2.09593903\n",
      "    2.09593903   2.09593903   2.09593903   2.09593903   2.09593903]]\n",
      "targets:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3])\n",
      "f:  [[0.74000168 0.74000168 0.74000168 0.74000168 0.74000168 0.74000168\n",
      "  0.         0.74000168 0.74000168 0.74000168]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.74000168]\n",
      " [0.21418864 0.21418864 0.21418864 0.21418864 0.21418864 0.21418864\n",
      "  0.21418864 0.21418864 0.21418864 0.52581304]\n",
      " [0.74000168 0.74000168 0.74000168 0.74000168 0.         0.74000168\n",
      "  0.74000168 0.74000168 0.74000168 0.74000168]\n",
      " [0.         0.74000168 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.74000168 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.39342809 0.39342809 0.34657359 0.39342809 0.39342809 0.39342809\n",
      "  0.39342809 0.39342809 0.39342809 0.39342809]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.74000168 0.         0.        ]\n",
      " [0.21418864 0.21418864 0.21418864 0.21418864 0.21418864 0.21418864\n",
      "  0.21418864 0.21418864 0.52581304 0.21418864]\n",
      " [0.74000168 0.74000168 0.74000168 0.         0.74000168 0.74000168\n",
      "  0.74000168 0.74000168 0.74000168 0.74000168]]\n",
      "fexp:  [[2.09593903 2.09593903 2.09593903 2.09593903 2.09593903 2.09593903\n",
      "  0.         2.09593903 2.09593903 2.09593903]\n",
      " [0.47711311 0.47711311 0.47711311 0.47711311 0.47711311 0.47711311\n",
      "  0.47711311 0.47711311 0.47711311 0.        ]\n",
      " [0.73225651 0.73225651 0.73225651 0.73225651 0.73225651 0.73225651\n",
      "  0.73225651 0.73225651 0.73225651 0.        ]\n",
      " [2.09593903 2.09593903 2.09593903 2.09593903 0.         2.09593903\n",
      "  2.09593903 2.09593903 2.09593903 2.09593903]\n",
      " [0.47711311 0.         0.47711311 0.47711311 0.47711311 0.47711311\n",
      "  0.47711311 0.47711311 0.47711311 0.47711311]\n",
      " [0.47711311 0.         0.47711311 0.47711311 0.47711311 0.47711311\n",
      "  0.47711311 0.47711311 0.47711311 0.47711311]\n",
      " [1.04796952 1.04796952 0.         1.04796952 1.04796952 1.04796952\n",
      "  1.04796952 1.04796952 1.04796952 1.04796952]\n",
      " [0.47711311 0.47711311 0.47711311 0.47711311 0.47711311 0.47711311\n",
      "  0.47711311 0.         0.47711311 0.47711311]\n",
      " [0.73225651 0.73225651 0.73225651 0.73225651 0.73225651 0.73225651\n",
      "  0.73225651 0.73225651 0.         0.73225651]\n",
      " [2.09593903 2.09593903 2.09593903 0.         2.09593903 2.09593903\n",
      "  2.09593903 2.09593903 2.09593903 2.09593903]]\n",
      "Epoch 0\n",
      "Progress: 10000,  val accuracy: 0.2400\n",
      "Progress: 20000,  val accuracy: 0.2800\n",
      "Progress: 30000,  val accuracy: 0.2700\n",
      "Progress: 40000,  val accuracy: 0.3000\n",
      "Progress: 50000,  val accuracy: 0.3000\n",
      "Epoch 1\n",
      "Progress: 60000,  val accuracy: 0.2500\n",
      "Progress: 70000,  val accuracy: 0.3400\n",
      "Progress: 80000,  val accuracy: 0.2600\n",
      "Progress: 90000,  val accuracy: 0.3000\n",
      "Progress: 100000,  val accuracy: 0.3100\n",
      "Epoch 2\n",
      "Progress: 110000,  val accuracy: 0.3100\n",
      "Progress: 120000,  val accuracy: 0.3700\n",
      "Progress: 130000,  val accuracy: 0.2800\n",
      "Progress: 140000,  val accuracy: 0.2700\n",
      "Progress: 150000,  val accuracy: 0.3700\n",
      "Epoch 3\n",
      "Progress: 160000,  val accuracy: 0.4000\n",
      "Progress: 170000,  val accuracy: 0.3700\n",
      "Progress: 180000,  val accuracy: 0.4500\n",
      "Progress: 190000,  val accuracy: 0.3400\n",
      "Progress: 200000,  val accuracy: 0.3900\n",
      "Epoch 4\n",
      "Progress: 210000,  val accuracy: 0.3500\n",
      "Progress: 220000,  val accuracy: 0.4200\n",
      "Progress: 230000,  val accuracy: 0.3200\n",
      "Progress: 240000,  val accuracy: 0.4300\n",
      "Progress: 250000,  val accuracy: 0.3500\n",
      "Epoch 5\n",
      "Progress: 260000,  val accuracy: 0.3500\n",
      "Progress: 270000,  val accuracy: 0.4700\n",
      "Progress: 280000,  val accuracy: 0.3400\n",
      "Progress: 290000,  val accuracy: 0.4300\n",
      "Progress: 300000,  val accuracy: 0.4500\n",
      "Epoch 6\n",
      "Progress: 310000,  val accuracy: 0.4800\n",
      "Progress: 320000,  val accuracy: 0.4300\n",
      "Progress: 330000,  val accuracy: 0.4800\n",
      "Progress: 340000,  val accuracy: 0.4300\n",
      "Progress: 350000,  val accuracy: 0.4800\n",
      "Epoch 7\n",
      "Progress: 360000,  val accuracy: 0.4900\n",
      "Progress: 370000,  val accuracy: 0.4900\n",
      "Progress: 380000,  val accuracy: 0.5100\n",
      "Progress: 390000,  val accuracy: 0.5000\n",
      "Progress: 400000,  val accuracy: 0.5100\n",
      "Epoch 8\n",
      "Progress: 410000,  val accuracy: 0.4600\n",
      "Progress: 420000,  val accuracy: 0.5100\n",
      "Progress: 430000,  val accuracy: 0.5100\n",
      "Progress: 440000,  val accuracy: 0.5100\n",
      "Progress: 450000,  val accuracy: 0.4800\n",
      "Epoch 9\n",
      "Progress: 460000,  val accuracy: 0.5000\n",
      "Progress: 470000,  val accuracy: 0.4900\n",
      "Progress: 480000,  val accuracy: 0.4600\n",
      "Progress: 490000,  val accuracy: 0.5500\n",
      "Progress: 500000,  val accuracy: 0.5600\n",
      "Epoch 10\n",
      "Progress: 510000,  val accuracy: 0.5000\n",
      "Progress: 520000,  val accuracy: 0.5300\n",
      "Progress: 530000,  val accuracy: 0.5700\n",
      "Progress: 540000,  val accuracy: 0.5300\n",
      "Progress: 550000,  val accuracy: 0.5800\n",
      "Epoch 11\n",
      "Progress: 560000,  val accuracy: 0.5600\n",
      "Progress: 570000,  val accuracy: 0.5800\n",
      "Progress: 580000,  val accuracy: 0.5900\n",
      "Progress: 590000,  val accuracy: 0.5800\n",
      "Progress: 600000,  val accuracy: 0.5400\n",
      "Epoch 12\n",
      "Progress: 610000,  val accuracy: 0.6100\n",
      "Progress: 620000,  val accuracy: 0.6100\n",
      "Progress: 630000,  val accuracy: 0.4600\n",
      "Progress: 640000,  val accuracy: 0.6000\n",
      "Progress: 650000,  val accuracy: 0.5400\n",
      "Epoch 13\n",
      "Progress: 660000,  val accuracy: 0.6000\n",
      "Progress: 670000,  val accuracy: 0.5900\n",
      "Progress: 680000,  val accuracy: 0.6700\n",
      "Progress: 690000,  val accuracy: 0.5700\n",
      "Progress: 700000,  val accuracy: 0.5500\n",
      "Epoch 14\n",
      "Progress: 710000,  val accuracy: 0.6500\n",
      "Progress: 720000,  val accuracy: 0.5700\n",
      "Progress: 730000,  val accuracy: 0.6400\n",
      "Progress: 740000,  val accuracy: 0.6400\n",
      "Progress: 750000,  val accuracy: 0.5600\n",
      "Epoch 15\n",
      "Total training time1411.7624835968018\n",
      "After fit function:  2:08:20.603162\n",
      "Test accuracy of weak learner:  0.6117\n",
      "Training accuracy of weak learner:  0.63194\n",
      "After train/test acc:  2:08:39.331304\n",
      "After allindices:  2:19:23.714589\n",
      "Predictions:  [4. 1. 0. 6. 1. 1. 4. 4. 0. 9.]\n",
      "Alpha:  0.16705332648219723\n",
      "before pessimistic update:  2:19:23.717046\n",
      "correct Indices Mask:  [False False False ...  True  True False]\n",
      "incorrect Indices Mask:  [ True  True  True ... False False  True]\n",
      "after pessimistic update:  2:19:23.723656\n",
      "t:  3 memory allocated: 141873664\n",
      "After WL  3  time elapsed(s):  8366\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training weak learner 4\n",
      "C_t:  [[  2.47701698   2.47701698   2.47701698   2.47701698   2.47701698\n",
      "    2.47701698 -22.29315281   2.47701698   2.47701698   2.47701698]\n",
      " [  0.56386053   0.56386053   0.56386053   0.56386053   0.56386053\n",
      "    0.56386053   0.56386053   0.56386053   0.56386053  -5.07474474]\n",
      " [  0.8653934    0.8653934    0.8653934    0.8653934    0.8653934\n",
      "    0.8653934    0.8653934    0.8653934    0.8653934   -7.78854064]\n",
      " [  2.47701698   2.47701698   2.47701698   2.47701698 -22.29315281\n",
      "    2.47701698   2.47701698   2.47701698   2.47701698   2.47701698]\n",
      " [  0.4037114   -3.63340263   0.4037114    0.4037114    0.4037114\n",
      "    0.4037114    0.4037114    0.4037114    0.4037114    0.4037114 ]\n",
      " [  0.4037114   -3.63340263   0.4037114    0.4037114    0.4037114\n",
      "    0.4037114    0.4037114    0.4037114    0.4037114    0.4037114 ]\n",
      " [  1.23850849   1.23850849 -11.1465764    1.23850849   1.23850849\n",
      "    1.23850849   1.23850849   1.23850849   1.23850849   1.23850849]\n",
      " [  0.56386053   0.56386053   0.56386053   0.56386053   0.56386053\n",
      "    0.56386053   0.56386053  -5.07474474   0.56386053   0.56386053]\n",
      " [  0.8653934    0.8653934    0.8653934    0.8653934    0.8653934\n",
      "    0.8653934    0.8653934    0.8653934   -7.78854064   0.8653934 ]\n",
      " [  2.47701698   2.47701698   2.47701698 -22.29315281   2.47701698\n",
      "    2.47701698   2.47701698   2.47701698   2.47701698   2.47701698]]\n",
      "targets:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3])\n",
      "f:  [[0.907055   0.907055   0.907055   0.907055   0.907055   0.907055\n",
      "  0.         0.907055   0.907055   0.907055  ]\n",
      " [0.16705333 0.16705333 0.16705333 0.16705333 0.16705333 0.16705333\n",
      "  0.16705333 0.16705333 0.16705333 0.74000168]\n",
      " [0.38124197 0.38124197 0.38124197 0.38124197 0.38124197 0.38124197\n",
      "  0.38124197 0.38124197 0.38124197 0.52581304]\n",
      " [0.907055   0.907055   0.907055   0.907055   0.         0.907055\n",
      "  0.907055   0.907055   0.907055   0.907055  ]\n",
      " [0.         0.907055   0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.907055   0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.56048141 0.56048141 0.34657359 0.56048141 0.56048141 0.56048141\n",
      "  0.56048141 0.56048141 0.56048141 0.56048141]\n",
      " [0.16705333 0.16705333 0.16705333 0.16705333 0.16705333 0.16705333\n",
      "  0.16705333 0.74000168 0.16705333 0.16705333]\n",
      " [0.38124197 0.38124197 0.38124197 0.38124197 0.38124197 0.38124197\n",
      "  0.38124197 0.38124197 0.52581304 0.38124197]\n",
      " [0.907055   0.907055   0.907055   0.         0.907055   0.907055\n",
      "  0.907055   0.907055   0.907055   0.907055  ]]\n",
      "fexp:  [[2.47701698 2.47701698 2.47701698 2.47701698 2.47701698 2.47701698\n",
      "  0.         2.47701698 2.47701698 2.47701698]\n",
      " [0.56386053 0.56386053 0.56386053 0.56386053 0.56386053 0.56386053\n",
      "  0.56386053 0.56386053 0.56386053 0.        ]\n",
      " [0.8653934  0.8653934  0.8653934  0.8653934  0.8653934  0.8653934\n",
      "  0.8653934  0.8653934  0.8653934  0.        ]\n",
      " [2.47701698 2.47701698 2.47701698 2.47701698 0.         2.47701698\n",
      "  2.47701698 2.47701698 2.47701698 2.47701698]\n",
      " [0.4037114  0.         0.4037114  0.4037114  0.4037114  0.4037114\n",
      "  0.4037114  0.4037114  0.4037114  0.4037114 ]\n",
      " [0.4037114  0.         0.4037114  0.4037114  0.4037114  0.4037114\n",
      "  0.4037114  0.4037114  0.4037114  0.4037114 ]\n",
      " [1.23850849 1.23850849 0.         1.23850849 1.23850849 1.23850849\n",
      "  1.23850849 1.23850849 1.23850849 1.23850849]\n",
      " [0.56386053 0.56386053 0.56386053 0.56386053 0.56386053 0.56386053\n",
      "  0.56386053 0.         0.56386053 0.56386053]\n",
      " [0.8653934  0.8653934  0.8653934  0.8653934  0.8653934  0.8653934\n",
      "  0.8653934  0.8653934  0.         0.8653934 ]\n",
      " [2.47701698 2.47701698 2.47701698 0.         2.47701698 2.47701698\n",
      "  2.47701698 2.47701698 2.47701698 2.47701698]]\n",
      "Epoch 0\n",
      "Progress: 10000,  val accuracy: 0.2900\n",
      "Progress: 20000,  val accuracy: 0.2300\n",
      "Progress: 30000,  val accuracy: 0.3000\n",
      "Progress: 40000,  val accuracy: 0.2900\n",
      "Progress: 50000,  val accuracy: 0.3100\n",
      "Epoch 1\n",
      "Progress: 60000,  val accuracy: 0.3200\n",
      "Progress: 70000,  val accuracy: 0.2600\n",
      "Progress: 80000,  val accuracy: 0.2700\n",
      "Progress: 90000,  val accuracy: 0.3300\n",
      "Progress: 100000,  val accuracy: 0.3000\n",
      "Epoch 2\n",
      "Progress: 110000,  val accuracy: 0.2700\n",
      "Progress: 120000,  val accuracy: 0.3000\n",
      "Progress: 130000,  val accuracy: 0.2800\n",
      "Progress: 140000,  val accuracy: 0.3100\n",
      "Progress: 150000,  val accuracy: 0.3100\n",
      "Epoch 3\n",
      "Progress: 160000,  val accuracy: 0.3000\n",
      "Progress: 170000,  val accuracy: 0.3200\n",
      "Progress: 180000,  val accuracy: 0.3700\n",
      "Progress: 190000,  val accuracy: 0.3600\n",
      "Progress: 200000,  val accuracy: 0.3500\n",
      "Epoch 4\n",
      "Progress: 210000,  val accuracy: 0.3400\n",
      "Progress: 220000,  val accuracy: 0.4100\n",
      "Progress: 230000,  val accuracy: 0.2500\n",
      "Progress: 240000,  val accuracy: 0.3300\n",
      "Progress: 250000,  val accuracy: 0.3500\n",
      "Epoch 5\n",
      "Progress: 260000,  val accuracy: 0.4400\n",
      "Progress: 270000,  val accuracy: 0.4500\n",
      "Progress: 280000,  val accuracy: 0.3800\n",
      "Progress: 290000,  val accuracy: 0.3600\n",
      "Progress: 300000,  val accuracy: 0.4200\n",
      "Epoch 6\n",
      "Progress: 310000,  val accuracy: 0.4400\n",
      "Progress: 320000,  val accuracy: 0.4700\n",
      "Progress: 330000,  val accuracy: 0.4100\n",
      "Progress: 340000,  val accuracy: 0.4200\n",
      "Progress: 350000,  val accuracy: 0.5200\n",
      "Epoch 7\n",
      "Progress: 360000,  val accuracy: 0.4400\n",
      "Progress: 370000,  val accuracy: 0.4700\n",
      "Progress: 380000,  val accuracy: 0.4200\n",
      "Progress: 390000,  val accuracy: 0.4700\n",
      "Progress: 400000,  val accuracy: 0.4700\n",
      "Epoch 8\n",
      "Progress: 410000,  val accuracy: 0.4300\n",
      "Progress: 420000,  val accuracy: 0.4300\n",
      "Progress: 430000,  val accuracy: 0.5000\n",
      "Progress: 440000,  val accuracy: 0.4900\n",
      "Progress: 450000,  val accuracy: 0.4400\n",
      "Epoch 9\n",
      "Progress: 460000,  val accuracy: 0.3900\n",
      "Progress: 470000,  val accuracy: 0.5400\n",
      "Progress: 480000,  val accuracy: 0.5400\n",
      "Progress: 490000,  val accuracy: 0.4900\n",
      "Progress: 500000,  val accuracy: 0.5500\n",
      "Epoch 10\n",
      "Progress: 510000,  val accuracy: 0.5500\n",
      "Progress: 520000,  val accuracy: 0.5200\n",
      "Progress: 530000,  val accuracy: 0.5700\n",
      "Progress: 540000,  val accuracy: 0.5500\n",
      "Progress: 550000,  val accuracy: 0.5600\n",
      "Epoch 11\n",
      "Progress: 560000,  val accuracy: 0.5800\n",
      "Progress: 570000,  val accuracy: 0.6100\n",
      "Progress: 580000,  val accuracy: 0.5300\n",
      "Progress: 590000,  val accuracy: 0.5300\n",
      "Progress: 600000,  val accuracy: 0.5300\n",
      "Epoch 12\n",
      "Progress: 610000,  val accuracy: 0.6200\n",
      "Progress: 620000,  val accuracy: 0.5300\n",
      "Progress: 630000,  val accuracy: 0.5700\n",
      "Progress: 640000,  val accuracy: 0.5500\n",
      "Progress: 650000,  val accuracy: 0.5400\n",
      "Epoch 13\n",
      "Progress: 660000,  val accuracy: 0.6200\n",
      "Progress: 670000,  val accuracy: 0.5600\n",
      "Progress: 680000,  val accuracy: 0.6200\n",
      "Progress: 690000,  val accuracy: 0.6400\n",
      "Progress: 700000,  val accuracy: 0.5500\n",
      "Epoch 14\n",
      "Progress: 710000,  val accuracy: 0.5300\n",
      "Progress: 720000,  val accuracy: 0.6000\n",
      "Progress: 730000,  val accuracy: 0.6100\n",
      "Progress: 740000,  val accuracy: 0.6000\n",
      "Progress: 750000,  val accuracy: 0.5900\n",
      "Epoch 15\n",
      "Total training time1412.5022492408752\n",
      "After fit function:  2:42:59.511705\n",
      "Test accuracy of weak learner:  0.6253\n",
      "Training accuracy of weak learner:  0.64166\n",
      "After train/test acc:  2:43:18.263502\n",
      "After allindices:  2:54:02.471986\n",
      "Predictions:  [4. 1. 0. 6. 1. 1. 4. 7. 8. 9.]\n",
      "Alpha:  0.14276824944769614\n",
      "before pessimistic update:  2:54:02.474967\n",
      "correct Indices Mask:  [False False False ... False  True False]\n",
      "incorrect Indices Mask:  [ True  True  True ...  True False  True]\n",
      "after pessimistic update:  2:54:02.483588\n",
      "t:  4 memory allocated: 141873664\n",
      "After WL  4  time elapsed(s):  10445\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training weak learner 5\n",
      "C_t:  [[  2.85714609   2.85714609   2.85714609   2.85714609   2.85714609\n",
      "    2.85714609 -25.71431477   2.85714609   2.85714609   2.85714609]\n",
      " [  0.65039195   0.65039195   0.65039195   0.65039195   0.65039195\n",
      "    0.65039195   0.65039195   0.65039195   0.65039195  -5.85352752]\n",
      " [  0.9981988    0.9981988    0.9981988    0.9981988    0.9981988\n",
      "    0.9981988    0.9981988    0.9981988    0.9981988   -8.9837892 ]\n",
      " [  2.85714609   2.85714609   2.85714609   2.85714609 -25.71431477\n",
      "    2.85714609   2.85714609   2.85714609   2.85714609   2.85714609]\n",
      " [  0.3499996   -3.14999644   0.3499996    0.3499996    0.3499996\n",
      "    0.3499996    0.3499996    0.3499996    0.3499996    0.3499996 ]\n",
      " [  0.3499996   -3.14999644   0.3499996    0.3499996    0.3499996\n",
      "    0.3499996    0.3499996    0.3499996    0.3499996    0.3499996 ]\n",
      " [  1.42857304   1.42857304 -12.85715738   1.42857304   1.42857304\n",
      "    1.42857304   1.42857304   1.42857304   1.42857304   1.42857304]\n",
      " [  0.48884168   0.48884168   0.48884168   0.48884168   0.48884168\n",
      "    0.48884168   0.48884168  -4.39957513   0.48884168   0.48884168]\n",
      " [  0.75025711   0.75025711   0.75025711   0.75025711   0.75025711\n",
      "    0.75025711   0.75025711   0.75025711  -6.75231396   0.75025711]\n",
      " [  2.85714609   2.85714609   2.85714609 -25.71431477   2.85714609\n",
      "    2.85714609   2.85714609   2.85714609   2.85714609   2.85714609]]\n",
      "targets:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3])\n",
      "f:  [[1.04982325 1.04982325 1.04982325 1.04982325 1.04982325 1.04982325\n",
      "  0.         1.04982325 1.04982325 1.04982325]\n",
      " [0.30982158 0.30982158 0.30982158 0.30982158 0.30982158 0.30982158\n",
      "  0.30982158 0.30982158 0.30982158 0.74000168]\n",
      " [0.52401022 0.52401022 0.52401022 0.52401022 0.52401022 0.52401022\n",
      "  0.52401022 0.52401022 0.52401022 0.52581304]\n",
      " [1.04982325 1.04982325 1.04982325 1.04982325 0.         1.04982325\n",
      "  1.04982325 1.04982325 1.04982325 1.04982325]\n",
      " [0.         1.04982325 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         1.04982325 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.70324966 0.70324966 0.34657359 0.70324966 0.70324966 0.70324966\n",
      "  0.70324966 0.70324966 0.70324966 0.70324966]\n",
      " [0.16705333 0.16705333 0.16705333 0.16705333 0.16705333 0.16705333\n",
      "  0.16705333 0.88276993 0.16705333 0.16705333]\n",
      " [0.38124197 0.38124197 0.38124197 0.38124197 0.38124197 0.38124197\n",
      "  0.38124197 0.38124197 0.66858129 0.38124197]\n",
      " [1.04982325 1.04982325 1.04982325 0.         1.04982325 1.04982325\n",
      "  1.04982325 1.04982325 1.04982325 1.04982325]]\n",
      "fexp:  [[2.85714609 2.85714609 2.85714609 2.85714609 2.85714609 2.85714609\n",
      "  0.         2.85714609 2.85714609 2.85714609]\n",
      " [0.65039195 0.65039195 0.65039195 0.65039195 0.65039195 0.65039195\n",
      "  0.65039195 0.65039195 0.65039195 0.        ]\n",
      " [0.9981988  0.9981988  0.9981988  0.9981988  0.9981988  0.9981988\n",
      "  0.9981988  0.9981988  0.9981988  0.        ]\n",
      " [2.85714609 2.85714609 2.85714609 2.85714609 0.         2.85714609\n",
      "  2.85714609 2.85714609 2.85714609 2.85714609]\n",
      " [0.3499996  0.         0.3499996  0.3499996  0.3499996  0.3499996\n",
      "  0.3499996  0.3499996  0.3499996  0.3499996 ]\n",
      " [0.3499996  0.         0.3499996  0.3499996  0.3499996  0.3499996\n",
      "  0.3499996  0.3499996  0.3499996  0.3499996 ]\n",
      " [1.42857304 1.42857304 0.         1.42857304 1.42857304 1.42857304\n",
      "  1.42857304 1.42857304 1.42857304 1.42857304]\n",
      " [0.48884168 0.48884168 0.48884168 0.48884168 0.48884168 0.48884168\n",
      "  0.48884168 0.         0.48884168 0.48884168]\n",
      " [0.75025711 0.75025711 0.75025711 0.75025711 0.75025711 0.75025711\n",
      "  0.75025711 0.75025711 0.         0.75025711]\n",
      " [2.85714609 2.85714609 2.85714609 0.         2.85714609 2.85714609\n",
      "  2.85714609 2.85714609 2.85714609 2.85714609]]\n",
      "Epoch 0\n",
      "Progress: 10000,  val accuracy: 0.2400\n",
      "Progress: 20000,  val accuracy: 0.2800\n",
      "Progress: 30000,  val accuracy: 0.2800\n",
      "Progress: 40000,  val accuracy: 0.2600\n",
      "Progress: 50000,  val accuracy: 0.2600\n",
      "Epoch 1\n",
      "Progress: 60000,  val accuracy: 0.2300\n",
      "Progress: 70000,  val accuracy: 0.3300\n",
      "Progress: 80000,  val accuracy: 0.3300\n",
      "Progress: 90000,  val accuracy: 0.3900\n",
      "Progress: 100000,  val accuracy: 0.3200\n",
      "Epoch 2\n",
      "Progress: 110000,  val accuracy: 0.1900\n",
      "Progress: 120000,  val accuracy: 0.2800\n",
      "Progress: 130000,  val accuracy: 0.2700\n",
      "Progress: 140000,  val accuracy: 0.2500\n",
      "Progress: 150000,  val accuracy: 0.3100\n",
      "Epoch 3\n",
      "Progress: 160000,  val accuracy: 0.3500\n",
      "Progress: 170000,  val accuracy: 0.3100\n",
      "Progress: 180000,  val accuracy: 0.3600\n",
      "Progress: 190000,  val accuracy: 0.2800\n",
      "Progress: 200000,  val accuracy: 0.2500\n",
      "Epoch 4\n",
      "Progress: 210000,  val accuracy: 0.3700\n",
      "Progress: 220000,  val accuracy: 0.2300\n",
      "Progress: 230000,  val accuracy: 0.4000\n",
      "Progress: 240000,  val accuracy: 0.3600\n",
      "Progress: 250000,  val accuracy: 0.3100\n",
      "Epoch 5\n",
      "Progress: 260000,  val accuracy: 0.3500\n",
      "Progress: 270000,  val accuracy: 0.3000\n",
      "Progress: 280000,  val accuracy: 0.3200\n",
      "Progress: 290000,  val accuracy: 0.3200\n",
      "Progress: 300000,  val accuracy: 0.4100\n",
      "Epoch 6\n",
      "Progress: 310000,  val accuracy: 0.4300\n",
      "Progress: 320000,  val accuracy: 0.4400\n",
      "Progress: 330000,  val accuracy: 0.3400\n",
      "Progress: 340000,  val accuracy: 0.4500\n",
      "Progress: 350000,  val accuracy: 0.4200\n",
      "Epoch 7\n",
      "Progress: 360000,  val accuracy: 0.4400\n",
      "Progress: 370000,  val accuracy: 0.3500\n",
      "Progress: 380000,  val accuracy: 0.4900\n",
      "Progress: 390000,  val accuracy: 0.4900\n",
      "Progress: 400000,  val accuracy: 0.4100\n",
      "Epoch 8\n",
      "Progress: 410000,  val accuracy: 0.3600\n",
      "Progress: 420000,  val accuracy: 0.5000\n",
      "Progress: 430000,  val accuracy: 0.3800\n",
      "Progress: 440000,  val accuracy: 0.5000\n",
      "Progress: 450000,  val accuracy: 0.5300\n",
      "Epoch 9\n",
      "Progress: 460000,  val accuracy: 0.5000\n",
      "Progress: 470000,  val accuracy: 0.5000\n",
      "Progress: 480000,  val accuracy: 0.4700\n",
      "Progress: 490000,  val accuracy: 0.4300\n",
      "Progress: 500000,  val accuracy: 0.5400\n",
      "Epoch 10\n",
      "Progress: 510000,  val accuracy: 0.5500\n",
      "Progress: 520000,  val accuracy: 0.4800\n",
      "Progress: 530000,  val accuracy: 0.5900\n",
      "Progress: 540000,  val accuracy: 0.5300\n",
      "Progress: 550000,  val accuracy: 0.5100\n",
      "Epoch 11\n",
      "Progress: 560000,  val accuracy: 0.5100\n",
      "Progress: 570000,  val accuracy: 0.5400\n",
      "Progress: 580000,  val accuracy: 0.6200\n",
      "Progress: 590000,  val accuracy: 0.5800\n",
      "Progress: 600000,  val accuracy: 0.6100\n",
      "Epoch 12\n",
      "Progress: 610000,  val accuracy: 0.5300\n",
      "Progress: 620000,  val accuracy: 0.5200\n",
      "Progress: 630000,  val accuracy: 0.5700\n",
      "Progress: 640000,  val accuracy: 0.5800\n",
      "Progress: 650000,  val accuracy: 0.6400\n",
      "Epoch 13\n",
      "Progress: 660000,  val accuracy: 0.5600\n",
      "Progress: 670000,  val accuracy: 0.5000\n",
      "Progress: 680000,  val accuracy: 0.5600\n",
      "Progress: 690000,  val accuracy: 0.5300\n",
      "Progress: 700000,  val accuracy: 0.6200\n",
      "Epoch 14\n",
      "Progress: 710000,  val accuracy: 0.5400\n",
      "Progress: 720000,  val accuracy: 0.6400\n",
      "Progress: 730000,  val accuracy: 0.5900\n",
      "Progress: 740000,  val accuracy: 0.6300\n",
      "Progress: 750000,  val accuracy: 0.5300\n",
      "Epoch 15\n",
      "Total training time1413.8092522621155\n",
      "After fit function:  3:17:39.568855\n",
      "Test accuracy of weak learner:  0.5823\n",
      "Training accuracy of weak learner:  0.5976\n",
      "After train/test acc:  3:17:58.460860\n",
      "After allindices:  3:28:43.259398\n",
      "Predictions:  [6. 9. 0. 6. 1. 1. 4. 7. 0. 7.]\n",
      "Alpha:  0.10931882825271347\n",
      "before pessimistic update:  3:28:43.262405\n",
      "correct Indices Mask:  [ True  True False ... False  True False]\n",
      "incorrect Indices Mask:  [False False  True ...  True False  True]\n",
      "after pessimistic update:  3:28:43.269889\n",
      "t:  5 memory allocated: 141873664\n",
      "After WL  5  time elapsed(s):  12526\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training weak learner 6\n",
      "C_t:  [[  2.56127307   2.56127307   2.56127307   2.56127307   2.56127307\n",
      "    2.56127307 -23.05145759   2.56127307   2.56127307   2.56127307]\n",
      " [  0.58304032   0.58304032   0.58304032   0.58304032   0.58304032\n",
      "    0.58304032   0.58304032   0.58304032   0.58304032  -5.24736291]\n",
      " [  1.11350868   1.11350868   1.11350868   1.11350868   1.11350868\n",
      "    1.11350868   1.11350868   1.11350868   1.11350868 -10.02157814]\n",
      " [  3.18719775   3.18719775   3.18719775   3.18719775 -28.68477975\n",
      "    3.18719775   3.18719775   3.18719775   3.18719775   3.18719775]\n",
      " [  0.31375524  -2.82379717   0.31375524   0.31375524   0.31375524\n",
      "    0.31375524   0.31375524   0.31375524   0.31375524   0.31375524]\n",
      " [  0.31375524  -2.82379717   0.31375524   0.31375524   0.31375524\n",
      "    0.31375524   0.31375524   0.31375524   0.31375524   0.31375524]\n",
      " [  1.59359888   1.59359888 -14.34238988   1.59359888   1.59359888\n",
      "    1.59359888   1.59359888   1.59359888   1.59359888   1.59359888]\n",
      " [  0.43821947   0.43821947   0.43821947   0.43821947   0.43821947\n",
      "    0.43821947   0.43821947  -3.9439752    0.43821947   0.43821947]\n",
      " [  0.83692527   0.83692527   0.83692527   0.83692527   0.83692527\n",
      "    0.83692527   0.83692527   0.83692527  -7.53232744   0.83692527]\n",
      " [  3.18719775   3.18719775   3.18719775 -28.68477975   3.18719775\n",
      "    3.18719775   3.18719775   3.18719775   3.18719775   3.18719775]]\n",
      "targets:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3])\n",
      "f:  [[1.04982325 1.04982325 1.04982325 1.04982325 1.04982325 1.04982325\n",
      "  0.10931883 1.04982325 1.04982325 1.04982325]\n",
      " [0.30982158 0.30982158 0.30982158 0.30982158 0.30982158 0.30982158\n",
      "  0.30982158 0.30982158 0.30982158 0.84932051]\n",
      " [0.63332904 0.63332904 0.63332904 0.63332904 0.63332904 0.63332904\n",
      "  0.63332904 0.63332904 0.63332904 0.52581304]\n",
      " [1.15914208 1.15914208 1.15914208 1.15914208 0.         1.15914208\n",
      "  1.15914208 1.15914208 1.15914208 1.15914208]\n",
      " [0.         1.15914208 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         1.15914208 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.81256849 0.81256849 0.34657359 0.81256849 0.81256849 0.81256849\n",
      "  0.81256849 0.81256849 0.81256849 0.81256849]\n",
      " [0.16705333 0.16705333 0.16705333 0.16705333 0.16705333 0.16705333\n",
      "  0.16705333 0.99208876 0.16705333 0.16705333]\n",
      " [0.49056079 0.49056079 0.49056079 0.49056079 0.49056079 0.49056079\n",
      "  0.49056079 0.49056079 0.66858129 0.49056079]\n",
      " [1.15914208 1.15914208 1.15914208 0.         1.15914208 1.15914208\n",
      "  1.15914208 1.15914208 1.15914208 1.15914208]]\n",
      "fexp:  [[2.56127307 2.56127307 2.56127307 2.56127307 2.56127307 2.56127307\n",
      "  0.         2.56127307 2.56127307 2.56127307]\n",
      " [0.58304032 0.58304032 0.58304032 0.58304032 0.58304032 0.58304032\n",
      "  0.58304032 0.58304032 0.58304032 0.        ]\n",
      " [1.11350868 1.11350868 1.11350868 1.11350868 1.11350868 1.11350868\n",
      "  1.11350868 1.11350868 1.11350868 0.        ]\n",
      " [3.18719775 3.18719775 3.18719775 3.18719775 0.         3.18719775\n",
      "  3.18719775 3.18719775 3.18719775 3.18719775]\n",
      " [0.31375524 0.         0.31375524 0.31375524 0.31375524 0.31375524\n",
      "  0.31375524 0.31375524 0.31375524 0.31375524]\n",
      " [0.31375524 0.         0.31375524 0.31375524 0.31375524 0.31375524\n",
      "  0.31375524 0.31375524 0.31375524 0.31375524]\n",
      " [1.59359888 1.59359888 0.         1.59359888 1.59359888 1.59359888\n",
      "  1.59359888 1.59359888 1.59359888 1.59359888]\n",
      " [0.43821947 0.43821947 0.43821947 0.43821947 0.43821947 0.43821947\n",
      "  0.43821947 0.         0.43821947 0.43821947]\n",
      " [0.83692527 0.83692527 0.83692527 0.83692527 0.83692527 0.83692527\n",
      "  0.83692527 0.83692527 0.         0.83692527]\n",
      " [3.18719775 3.18719775 3.18719775 0.         3.18719775 3.18719775\n",
      "  3.18719775 3.18719775 3.18719775 3.18719775]]\n",
      "Epoch 0\n",
      "Progress: 10000,  val accuracy: 0.2500\n",
      "Progress: 20000,  val accuracy: 0.2600\n",
      "Progress: 30000,  val accuracy: 0.2500\n",
      "Progress: 40000,  val accuracy: 0.2900\n",
      "Progress: 50000,  val accuracy: 0.3000\n",
      "Epoch 1\n",
      "Progress: 60000,  val accuracy: 0.3200\n",
      "Progress: 70000,  val accuracy: 0.2400\n",
      "Progress: 80000,  val accuracy: 0.3100\n",
      "Progress: 90000,  val accuracy: 0.3700\n",
      "Progress: 100000,  val accuracy: 0.3100\n",
      "Epoch 2\n",
      "Progress: 110000,  val accuracy: 0.4100\n",
      "Progress: 120000,  val accuracy: 0.2600\n",
      "Progress: 130000,  val accuracy: 0.3300\n",
      "Progress: 140000,  val accuracy: 0.3100\n",
      "Progress: 150000,  val accuracy: 0.2900\n",
      "Epoch 3\n",
      "Progress: 160000,  val accuracy: 0.3000\n",
      "Progress: 170000,  val accuracy: 0.4100\n",
      "Progress: 180000,  val accuracy: 0.2900\n",
      "Progress: 190000,  val accuracy: 0.3200\n",
      "Progress: 200000,  val accuracy: 0.3700\n",
      "Epoch 4\n",
      "Progress: 210000,  val accuracy: 0.2000\n",
      "Progress: 220000,  val accuracy: 0.3300\n",
      "Progress: 230000,  val accuracy: 0.2700\n",
      "Progress: 240000,  val accuracy: 0.3100\n",
      "Progress: 250000,  val accuracy: 0.3200\n",
      "Epoch 5\n",
      "Progress: 260000,  val accuracy: 0.3900\n",
      "Progress: 270000,  val accuracy: 0.3400\n",
      "Progress: 280000,  val accuracy: 0.4100\n",
      "Progress: 290000,  val accuracy: 0.3900\n",
      "Progress: 300000,  val accuracy: 0.3800\n",
      "Epoch 6\n",
      "Progress: 310000,  val accuracy: 0.4400\n",
      "Progress: 320000,  val accuracy: 0.3100\n",
      "Progress: 330000,  val accuracy: 0.3100\n",
      "Progress: 340000,  val accuracy: 0.4600\n",
      "Progress: 350000,  val accuracy: 0.4100\n",
      "Epoch 7\n",
      "Progress: 360000,  val accuracy: 0.3600\n",
      "Progress: 370000,  val accuracy: 0.4300\n",
      "Progress: 380000,  val accuracy: 0.3900\n",
      "Progress: 390000,  val accuracy: 0.4800\n",
      "Progress: 400000,  val accuracy: 0.4400\n",
      "Epoch 8\n",
      "Progress: 410000,  val accuracy: 0.3300\n",
      "Progress: 420000,  val accuracy: 0.4200\n",
      "Progress: 430000,  val accuracy: 0.4800\n",
      "Progress: 440000,  val accuracy: 0.5100\n",
      "Progress: 450000,  val accuracy: 0.4300\n",
      "Epoch 9\n",
      "Progress: 460000,  val accuracy: 0.4500\n",
      "Progress: 470000,  val accuracy: 0.4300\n",
      "Progress: 480000,  val accuracy: 0.5300\n",
      "Progress: 490000,  val accuracy: 0.4600\n",
      "Progress: 500000,  val accuracy: 0.4100\n",
      "Epoch 10\n",
      "Progress: 510000,  val accuracy: 0.5300\n",
      "Progress: 520000,  val accuracy: 0.5300\n",
      "Progress: 530000,  val accuracy: 0.5000\n",
      "Progress: 540000,  val accuracy: 0.4500\n",
      "Progress: 550000,  val accuracy: 0.5600\n",
      "Epoch 11\n",
      "Progress: 560000,  val accuracy: 0.5800\n",
      "Progress: 570000,  val accuracy: 0.5500\n",
      "Progress: 580000,  val accuracy: 0.5400\n",
      "Progress: 590000,  val accuracy: 0.5900\n",
      "Progress: 600000,  val accuracy: 0.5300\n",
      "Epoch 12\n",
      "Progress: 610000,  val accuracy: 0.5200\n",
      "Progress: 620000,  val accuracy: 0.5200\n",
      "Progress: 630000,  val accuracy: 0.5800\n",
      "Progress: 640000,  val accuracy: 0.5300\n",
      "Progress: 650000,  val accuracy: 0.5600\n",
      "Epoch 13\n",
      "Progress: 660000,  val accuracy: 0.5900\n",
      "Progress: 670000,  val accuracy: 0.5800\n",
      "Progress: 680000,  val accuracy: 0.5300\n",
      "Progress: 690000,  val accuracy: 0.5600\n",
      "Progress: 700000,  val accuracy: 0.5700\n",
      "Epoch 14\n",
      "Progress: 710000,  val accuracy: 0.6100\n",
      "Progress: 720000,  val accuracy: 0.6300\n",
      "Progress: 730000,  val accuracy: 0.6100\n",
      "Progress: 740000,  val accuracy: 0.6600\n",
      "Progress: 750000,  val accuracy: 0.6400\n",
      "Epoch 15\n",
      "Total training time1413.0059020519257\n",
      "After fit function:  3:52:19.273432\n",
      "Test accuracy of weak learner:  0.629\n",
      "Training accuracy of weak learner:  0.64788\n",
      "After train/test acc:  3:52:38.288910\n",
      "After allindices:  4:03:22.786436\n",
      "Predictions:  [3. 9. 7. 6. 0. 1. 4. 7. 0. 3.]\n",
      "Alpha:  0.10888713184224057\n",
      "before pessimistic update:  4:03:22.789090\n",
      "correct Indices Mask:  [False  True False ... False False False]\n",
      "incorrect Indices Mask:  [ True False  True ...  True  True  True]\n",
      "after pessimistic update:  4:03:22.796263\n",
      "t:  6 memory allocated: 141873664\n",
      "After WL  6  time elapsed(s):  14605\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training weak learner 7\n",
      "C_t:  [[  2.85591293   2.85591293   2.85591293   2.85591293   2.85591293\n",
      "    2.85591293 -25.70321639   2.85591293   2.85591293   2.85591293]\n",
      " [  0.52288901   0.52288901   0.52288901   0.52288901   0.52288901\n",
      "    0.52288901   0.52288901   0.52288901   0.52288901  -4.70600106]\n",
      " [  1.24160281   1.24160281   1.24160281   1.24160281   1.24160281\n",
      "    1.24160281   1.24160281   1.24160281   1.24160281 -11.17442532]\n",
      " [  3.5538418    3.5538418    3.5538418    3.5538418  -31.98457617\n",
      "    3.5538418    3.5538418    3.5538418    3.5538418    3.5538418 ]\n",
      " [  0.34984854  -3.14863689   0.34984854   0.34984854   0.34984854\n",
      "    0.34984854   0.34984854   0.34984854   0.34984854   0.34984854]\n",
      " [  0.28138563  -2.53247064   0.28138563   0.28138563   0.28138563\n",
      "    0.28138563   0.28138563   0.28138563   0.28138563   0.28138563]\n",
      " [  1.7769209    1.7769209  -15.99228808   1.7769209    1.7769209\n",
      "    1.7769209    1.7769209    1.7769209    1.7769209    1.7769209 ]\n",
      " [  0.39300908   0.39300908   0.39300908   0.39300908   0.39300908\n",
      "    0.39300908   0.39300908  -3.53708173   0.39300908   0.39300908]\n",
      " [  0.93320222   0.93320222   0.93320222   0.93320222   0.93320222\n",
      "    0.93320222   0.93320222   0.93320222  -8.39881996   0.93320222]\n",
      " [  2.85837977   2.85837977   2.85837977 -25.72541794   2.85837977\n",
      "    2.85837977   2.85837977   2.85837977   2.85837977   2.85837977]]\n",
      "targets:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3])\n",
      "f:  [[1.15871039 1.15871039 1.15871039 1.15871039 1.15871039 1.15871039\n",
      "  0.10931883 1.15871039 1.15871039 1.15871039]\n",
      " [0.30982158 0.30982158 0.30982158 0.30982158 0.30982158 0.30982158\n",
      "  0.30982158 0.30982158 0.30982158 0.95820764]\n",
      " [0.74221618 0.74221618 0.74221618 0.74221618 0.74221618 0.74221618\n",
      "  0.74221618 0.74221618 0.74221618 0.52581304]\n",
      " [1.26802921 1.26802921 1.26802921 1.26802921 0.         1.26802921\n",
      "  1.26802921 1.26802921 1.26802921 1.26802921]\n",
      " [0.10888713 1.15914208 0.10888713 0.10888713 0.10888713 0.10888713\n",
      "  0.10888713 0.10888713 0.10888713 0.10888713]\n",
      " [0.         1.26802921 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.92145562 0.92145562 0.34657359 0.92145562 0.92145562 0.92145562\n",
      "  0.92145562 0.92145562 0.92145562 0.92145562]\n",
      " [0.16705333 0.16705333 0.16705333 0.16705333 0.16705333 0.16705333\n",
      "  0.16705333 1.10097589 0.16705333 0.16705333]\n",
      " [0.59944793 0.59944793 0.59944793 0.59944793 0.59944793 0.59944793\n",
      "  0.59944793 0.59944793 0.66858129 0.59944793]\n",
      " [1.15914208 1.15914208 1.15914208 0.10888713 1.15914208 1.15914208\n",
      "  1.15914208 1.15914208 1.15914208 1.15914208]]\n",
      "fexp:  [[2.85591293 2.85591293 2.85591293 2.85591293 2.85591293 2.85591293\n",
      "  0.         2.85591293 2.85591293 2.85591293]\n",
      " [0.52288901 0.52288901 0.52288901 0.52288901 0.52288901 0.52288901\n",
      "  0.52288901 0.52288901 0.52288901 0.        ]\n",
      " [1.24160281 1.24160281 1.24160281 1.24160281 1.24160281 1.24160281\n",
      "  1.24160281 1.24160281 1.24160281 0.        ]\n",
      " [3.5538418  3.5538418  3.5538418  3.5538418  0.         3.5538418\n",
      "  3.5538418  3.5538418  3.5538418  3.5538418 ]\n",
      " [0.34984854 0.         0.34984854 0.34984854 0.34984854 0.34984854\n",
      "  0.34984854 0.34984854 0.34984854 0.34984854]\n",
      " [0.28138563 0.         0.28138563 0.28138563 0.28138563 0.28138563\n",
      "  0.28138563 0.28138563 0.28138563 0.28138563]\n",
      " [1.7769209  1.7769209  0.         1.7769209  1.7769209  1.7769209\n",
      "  1.7769209  1.7769209  1.7769209  1.7769209 ]\n",
      " [0.39300908 0.39300908 0.39300908 0.39300908 0.39300908 0.39300908\n",
      "  0.39300908 0.         0.39300908 0.39300908]\n",
      " [0.93320222 0.93320222 0.93320222 0.93320222 0.93320222 0.93320222\n",
      "  0.93320222 0.93320222 0.         0.93320222]\n",
      " [2.85837977 2.85837977 2.85837977 0.         2.85837977 2.85837977\n",
      "  2.85837977 2.85837977 2.85837977 2.85837977]]\n",
      "Epoch 0\n",
      "Progress: 10000,  val accuracy: 0.2300\n",
      "Progress: 20000,  val accuracy: 0.2600\n",
      "Progress: 30000,  val accuracy: 0.2000\n",
      "Progress: 40000,  val accuracy: 0.1800\n",
      "Progress: 50000,  val accuracy: 0.2400\n",
      "Epoch 1\n",
      "Progress: 60000,  val accuracy: 0.2300\n",
      "Progress: 70000,  val accuracy: 0.3500\n",
      "Progress: 80000,  val accuracy: 0.3600\n",
      "Progress: 90000,  val accuracy: 0.3900\n",
      "Progress: 100000,  val accuracy: 0.2300\n",
      "Epoch 2\n",
      "Progress: 110000,  val accuracy: 0.3000\n",
      "Progress: 160000,  val accuracy: 0.3600\n",
      "Progress: 170000,  val accuracy: 0.3400\n",
      "Progress: 180000,  val accuracy: 0.3200\n",
      "Progress: 190000,  val accuracy: 0.2600\n",
      "Progress: 200000,  val accuracy: 0.3000\n",
      "Epoch 4\n",
      "Progress: 210000,  val accuracy: 0.3400\n",
      "Progress: 220000,  val accuracy: 0.3900\n",
      "Progress: 230000,  val accuracy: 0.3300\n",
      "Progress: 240000,  val accuracy: 0.3400\n",
      "Progress: 250000,  val accuracy: 0.2900\n",
      "Epoch 5\n",
      "Progress: 260000,  val accuracy: 0.3200\n",
      "Progress: 270000,  val accuracy: 0.3500\n",
      "Progress: 280000,  val accuracy: 0.2900\n",
      "Progress: 290000,  val accuracy: 0.3300\n",
      "Progress: 300000,  val accuracy: 0.3600\n",
      "Epoch 6\n",
      "Progress: 310000,  val accuracy: 0.2900\n",
      "Progress: 320000,  val accuracy: 0.3900\n",
      "Progress: 330000,  val accuracy: 0.3500\n",
      "Progress: 340000,  val accuracy: 0.3700\n",
      "Progress: 350000,  val accuracy: 0.4100\n",
      "Epoch 7\n",
      "Progress: 360000,  val accuracy: 0.3600\n",
      "Progress: 370000,  val accuracy: 0.3300\n",
      "Progress: 380000,  val accuracy: 0.4300\n",
      "Progress: 390000,  val accuracy: 0.3900\n",
      "Progress: 400000,  val accuracy: 0.3100\n",
      "Epoch 8\n",
      "Progress: 410000,  val accuracy: 0.4900\n",
      "Progress: 420000,  val accuracy: 0.4900\n",
      "Progress: 430000,  val accuracy: 0.4100\n",
      "Progress: 440000,  val accuracy: 0.5000\n",
      "Progress: 450000,  val accuracy: 0.4600\n",
      "Epoch 9\n",
      "Progress: 460000,  val accuracy: 0.4400\n",
      "Progress: 470000,  val accuracy: 0.4500\n",
      "Progress: 480000,  val accuracy: 0.4700\n",
      "Progress: 490000,  val accuracy: 0.4600\n",
      "Progress: 500000,  val accuracy: 0.4900\n",
      "Epoch 10\n",
      "Progress: 510000,  val accuracy: 0.4200\n",
      "Progress: 520000,  val accuracy: 0.4400\n",
      "Progress: 530000,  val accuracy: 0.4700\n",
      "Progress: 540000,  val accuracy: 0.4000\n",
      "Progress: 550000,  val accuracy: 0.4800\n",
      "Epoch 11\n",
      "Progress: 560000,  val accuracy: 0.5000\n",
      "Progress: 570000,  val accuracy: 0.4700\n",
      "Progress: 580000,  val accuracy: 0.5200\n",
      "Progress: 590000,  val accuracy: 0.5100\n",
      "Progress: 600000,  val accuracy: 0.6300\n",
      "Epoch 12\n",
      "Progress: 610000,  val accuracy: 0.5100\n",
      "Progress: 620000,  val accuracy: 0.5100\n",
      "Progress: 630000,  val accuracy: 0.6200\n",
      "Progress: 640000,  val accuracy: 0.5300\n",
      "Progress: 650000,  val accuracy: 0.6400\n",
      "Epoch 13\n",
      "Progress: 660000,  val accuracy: 0.4600\n",
      "Progress: 670000,  val accuracy: 0.5800\n",
      "Progress: 680000,  val accuracy: 0.5600\n",
      "Progress: 690000,  val accuracy: 0.5200\n",
      "Progress: 700000,  val accuracy: 0.5700\n",
      "Epoch 14\n",
      "Progress: 710000,  val accuracy: 0.6500\n",
      "Progress: 720000,  val accuracy: 0.5300\n",
      "Progress: 730000,  val accuracy: 0.6100\n",
      "Progress: 740000,  val accuracy: 0.5300\n",
      "Progress: 750000,  val accuracy: 0.6300\n",
      "Epoch 15\n",
      "Total training time1411.8281388282776\n",
      "After fit function:  4:26:57.669943\n",
      "Test accuracy of weak learner:  0.6238\n",
      "Training accuracy of weak learner:  0.63816\n",
      "After train/test acc:  4:27:16.497409\n",
      "After allindices:  4:38:00.826980\n",
      "Predictions:  [3. 9. 0. 6. 1. 1. 6. 7. 0. 7.]\n",
      "Alpha:  0.08572364172565784\n",
      "before pessimistic update:  4:38:00.829665\n",
      "correct Indices Mask:  [False  True False ... False False False]\n",
      "incorrect Indices Mask:  [ True False  True ...  True  True  True]\n",
      "after pessimistic update:  4:38:00.837049\n",
      "t:  7 memory allocated: 141873664\n",
      "After WL  7  time elapsed(s):  16683\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training weak learner 8\n",
      "C_t:  [[  3.11153197   3.11153197   3.11153197   3.11153197   3.11153197\n",
      "    3.11153197 -28.00378773   3.11153197   3.11153197   3.11153197]\n",
      " [  0.47993255   0.47993255   0.47993255   0.47993255   0.47993255\n",
      "    0.47993255   0.47993255   0.47993255   0.47993255  -4.31939296]\n",
      " [  1.35273271   1.35273271   1.35273271   1.35273271   1.35273271\n",
      "    1.35273271   1.35273271   1.35273271   1.35273271 -12.17459442]\n",
      " [  3.87192909   3.87192909   3.87192909   3.87192909 -34.84736183\n",
      "    3.87192909   3.87192909   3.87192909   3.87192909   3.87192909]\n",
      " [  0.32110773  -2.88996961   0.32110773   0.32110773   0.32110773\n",
      "    0.32110773   0.32110773   0.32110773   0.32110773   0.32110773]\n",
      " [  0.25826919  -2.32442273   0.25826919   0.25826919   0.25826919\n",
      "    0.25826919   0.25826919   0.25826919   0.25826919   0.25826919]\n",
      " [  1.93596455   1.93596455 -17.42368091   1.93596455   1.93596455\n",
      "    1.93596455   1.93596455   1.93596455   1.93596455   1.93596455]\n",
      " [  0.36072254   0.36072254   0.36072254   0.36072254   0.36072254\n",
      "    0.36072254   0.36072254  -3.24650286   0.36072254   0.36072254]\n",
      " [  1.01672866   1.01672866   1.01672866   1.01672866   1.01672866\n",
      "    1.01672866   1.01672866   1.01672866  -9.15055796   1.01672866]\n",
      " [  3.1142196    3.1142196    3.1142196  -28.02797644   3.1142196\n",
      "    3.1142196    3.1142196    3.1142196    3.1142196    3.1142196 ]]\n",
      "targets:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3])\n",
      "f:  [[1.24443403 1.24443403 1.24443403 1.24443403 1.24443403 1.24443403\n",
      "  0.10931883 1.24443403 1.24443403 1.24443403]\n",
      " [0.30982158 0.30982158 0.30982158 0.30982158 0.30982158 0.30982158\n",
      "  0.30982158 0.30982158 0.30982158 1.04393128]\n",
      " [0.82793982 0.82793982 0.82793982 0.82793982 0.82793982 0.82793982\n",
      "  0.82793982 0.82793982 0.82793982 0.52581304]\n",
      " [1.35375286 1.35375286 1.35375286 1.35375286 0.         1.35375286\n",
      "  1.35375286 1.35375286 1.35375286 1.35375286]\n",
      " [0.10888713 1.24486572 0.10888713 0.10888713 0.10888713 0.10888713\n",
      "  0.10888713 0.10888713 0.10888713 0.10888713]\n",
      " [0.         1.35375286 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [1.00717927 1.00717927 0.34657359 1.00717927 1.00717927 1.00717927\n",
      "  1.00717927 1.00717927 1.00717927 1.00717927]\n",
      " [0.16705333 0.16705333 0.16705333 0.16705333 0.16705333 0.16705333\n",
      "  0.16705333 1.18669953 0.16705333 0.16705333]\n",
      " [0.68517157 0.68517157 0.68517157 0.68517157 0.68517157 0.68517157\n",
      "  0.68517157 0.68517157 0.66858129 0.68517157]\n",
      " [1.24486572 1.24486572 1.24486572 0.10888713 1.24486572 1.24486572\n",
      "  1.24486572 1.24486572 1.24486572 1.24486572]]\n",
      "fexp:  [[3.11153197 3.11153197 3.11153197 3.11153197 3.11153197 3.11153197\n",
      "  0.         3.11153197 3.11153197 3.11153197]\n",
      " [0.47993255 0.47993255 0.47993255 0.47993255 0.47993255 0.47993255\n",
      "  0.47993255 0.47993255 0.47993255 0.        ]\n",
      " [1.35273271 1.35273271 1.35273271 1.35273271 1.35273271 1.35273271\n",
      "  1.35273271 1.35273271 1.35273271 0.        ]\n",
      " [3.87192909 3.87192909 3.87192909 3.87192909 0.         3.87192909\n",
      "  3.87192909 3.87192909 3.87192909 3.87192909]\n",
      " [0.32110773 0.         0.32110773 0.32110773 0.32110773 0.32110773\n",
      "  0.32110773 0.32110773 0.32110773 0.32110773]\n",
      " [0.25826919 0.         0.25826919 0.25826919 0.25826919 0.25826919\n",
      "  0.25826919 0.25826919 0.25826919 0.25826919]\n",
      " [1.93596455 1.93596455 0.         1.93596455 1.93596455 1.93596455\n",
      "  1.93596455 1.93596455 1.93596455 1.93596455]\n",
      " [0.36072254 0.36072254 0.36072254 0.36072254 0.36072254 0.36072254\n",
      "  0.36072254 0.         0.36072254 0.36072254]\n",
      " [1.01672866 1.01672866 1.01672866 1.01672866 1.01672866 1.01672866\n",
      "  1.01672866 1.01672866 0.         1.01672866]\n",
      " [3.1142196  3.1142196  3.1142196  0.         3.1142196  3.1142196\n",
      "  3.1142196  3.1142196  3.1142196  3.1142196 ]]\n",
      "Epoch 0\n",
      "Progress: 10000,  val accuracy: 0.2300\n",
      "Progress: 20000,  val accuracy: 0.2000\n",
      "Progress: 30000,  val accuracy: 0.2700\n",
      "Progress: 40000,  val accuracy: 0.2000\n",
      "Progress: 50000,  val accuracy: 0.2400\n",
      "Epoch 1\n",
      "Progress: 60000,  val accuracy: 0.2400\n",
      "Progress: 70000,  val accuracy: 0.3500\n",
      "Progress: 80000,  val accuracy: 0.2400\n",
      "Progress: 90000,  val accuracy: 0.2700\n",
      "Progress: 100000,  val accuracy: 0.3000\n",
      "Epoch 2\n",
      "Progress: 110000,  val accuracy: 0.2500\n",
      "Progress: 120000,  val accuracy: 0.3400\n",
      "Progress: 130000,  val accuracy: 0.3100\n",
      "Progress: 140000,  val accuracy: 0.1800\n",
      "Progress: 150000,  val accuracy: 0.3300\n",
      "Epoch 3\n",
      "Progress: 160000,  val accuracy: 0.3400\n",
      "Progress: 170000,  val accuracy: 0.3200\n",
      "Progress: 180000,  val accuracy: 0.2700\n",
      "Progress: 190000,  val accuracy: 0.2300\n",
      "Progress: 200000,  val accuracy: 0.3900\n",
      "Epoch 4\n",
      "Progress: 210000,  val accuracy: 0.3300\n",
      "Progress: 220000,  val accuracy: 0.3500\n",
      "Progress: 230000,  val accuracy: 0.3900\n",
      "Progress: 240000,  val accuracy: 0.3400\n",
      "Progress: 250000,  val accuracy: 0.4100\n",
      "Epoch 5\n",
      "Progress: 260000,  val accuracy: 0.2900\n",
      "Progress: 270000,  val accuracy: 0.3300\n",
      "Progress: 280000,  val accuracy: 0.2900\n",
      "Progress: 290000,  val accuracy: 0.3200\n",
      "Progress: 300000,  val accuracy: 0.2000\n",
      "Epoch 6\n",
      "Progress: 310000,  val accuracy: 0.3200\n",
      "Progress: 320000,  val accuracy: 0.3500\n",
      "Progress: 330000,  val accuracy: 0.3300\n",
      "Progress: 340000,  val accuracy: 0.4100\n",
      "Progress: 350000,  val accuracy: 0.4200\n",
      "Epoch 7\n",
      "Progress: 360000,  val accuracy: 0.3400\n",
      "Progress: 370000,  val accuracy: 0.4200\n",
      "Progress: 380000,  val accuracy: 0.4200\n",
      "Progress: 390000,  val accuracy: 0.3500\n",
      "Progress: 400000,  val accuracy: 0.4200\n",
      "Epoch 8\n",
      "Progress: 410000,  val accuracy: 0.4600\n",
      "Progress: 420000,  val accuracy: 0.3300\n",
      "Progress: 430000,  val accuracy: 0.4600\n",
      "Progress: 440000,  val accuracy: 0.5200\n",
      "Progress: 450000,  val accuracy: 0.4100\n",
      "Epoch 9\n",
      "Progress: 460000,  val accuracy: 0.4600\n",
      "Progress: 470000,  val accuracy: 0.4400\n",
      "Progress: 480000,  val accuracy: 0.3900\n",
      "Progress: 490000,  val accuracy: 0.4800\n",
      "Progress: 500000,  val accuracy: 0.4300\n",
      "Epoch 10\n",
      "Progress: 510000,  val accuracy: 0.3700\n",
      "Progress: 520000,  val accuracy: 0.4700\n",
      "Progress: 530000,  val accuracy: 0.5000\n",
      "Progress: 540000,  val accuracy: 0.5600\n",
      "Progress: 550000,  val accuracy: 0.4900\n",
      "Epoch 11\n",
      "Progress: 560000,  val accuracy: 0.5100\n",
      "Progress: 570000,  val accuracy: 0.4700\n",
      "Progress: 580000,  val accuracy: 0.4700\n",
      "Progress: 590000,  val accuracy: 0.5200\n",
      "Progress: 600000,  val accuracy: 0.5900\n",
      "Epoch 12\n",
      "Progress: 610000,  val accuracy: 0.5300\n",
      "Progress: 620000,  val accuracy: 0.5100\n",
      "Progress: 630000,  val accuracy: 0.4800\n",
      "Progress: 640000,  val accuracy: 0.5500\n",
      "Progress: 650000,  val accuracy: 0.5000\n",
      "Epoch 13\n",
      "Progress: 660000,  val accuracy: 0.4900\n",
      "Progress: 670000,  val accuracy: 0.5700\n",
      "Progress: 680000,  val accuracy: 0.5600\n",
      "Progress: 690000,  val accuracy: 0.5100\n",
      "Progress: 700000,  val accuracy: 0.5300\n",
      "Epoch 14\n",
      "Progress: 710000,  val accuracy: 0.4500\n",
      "Progress: 720000,  val accuracy: 0.5700\n",
      "Progress: 730000,  val accuracy: 0.5500\n",
      "Progress: 740000,  val accuracy: 0.5800\n",
      "Progress: 750000,  val accuracy: 0.5800\n",
      "Epoch 15\n",
      "Total training time1412.2966885566711\n",
      "After fit function:  5:01:36.289385\n",
      "Test accuracy of weak learner:  0.5969\n",
      "Training accuracy of weak learner:  0.61288\n",
      "After train/test acc:  5:01:54.852068\n",
      "After allindices:  5:12:39.118667\n",
      "Predictions:  [4. 9. 7. 4. 1. 6. 4. 7. 0. 4.]\n",
      "Alpha:  0.09824763542581066\n",
      "before pessimistic update:  5:12:39.121820\n",
      "correct Indices Mask:  [False  True False ...  True False False]\n",
      "incorrect Indices Mask:  [ True False  True ... False  True  True]\n",
      "after pessimistic update:  5:12:39.130260\n",
      "t:  8 memory allocated: 141873664\n",
      "After WL  8  time elapsed(s):  18762\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training weak learner 9\n",
      "C_t:  [[  3.43275393   3.43275393   3.43275393   3.43275393   3.43275393\n",
      "    3.43275393 -30.8947854    3.43275393   3.43275393   3.43275393]\n",
      " [  0.43502258   0.43502258   0.43502258   0.43502258   0.43502258\n",
      "    0.43502258   0.43502258   0.43502258   0.43502258  -3.91520323]\n",
      " [  1.49238336   1.49238336   1.49238336   1.49238336   1.49238336\n",
      "    1.49238336   1.49238336   1.49238336   1.49238336 -13.43145026]\n",
      " [  3.50961105   3.50961105   3.50961105   3.50961105 -31.58649949\n",
      "    3.50961105   3.50961105   3.50961105   3.50961105   3.50961105]\n",
      " [  0.29105989  -2.619539     0.29105989   0.29105989   0.29105989\n",
      "    0.29105989   0.29105989   0.29105989   0.29105989   0.29105989]\n",
      " [  0.28493186  -2.56438673   0.28493186   0.28493186   0.28493186\n",
      "    0.28493186   0.28493186   0.28493186   0.28493186   0.28493186]\n",
      " [  2.13582569   2.13582569 -19.22243119   2.13582569   2.13582569\n",
      "    2.13582569   2.13582569   2.13582569   2.13582569   2.13582569]\n",
      " [  0.32696772   0.32696772   0.32696772   0.32696772   0.32696772\n",
      "    0.32696772   0.32696772  -2.94270945   0.32696772   0.32696772]\n",
      " [  1.12169161   1.12169161   1.12169161   1.12169161   1.12169161\n",
      "    1.12169161   1.12169161   1.12169161 -10.09522452   1.12169161]\n",
      " [  3.43571903   3.43571903   3.43571903 -30.92147126   3.43571903\n",
      "    3.43571903   3.43571903   3.43571903   3.43571903   3.43571903]]\n",
      "targets:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3])\n",
      "f:  [[1.34268166 1.34268166 1.34268166 1.34268166 1.34268166 1.34268166\n",
      "  0.10931883 1.34268166 1.34268166 1.34268166]\n",
      " [0.30982158 0.30982158 0.30982158 0.30982158 0.30982158 0.30982158\n",
      "  0.30982158 0.30982158 0.30982158 1.14217892]\n",
      " [0.92618745 0.92618745 0.92618745 0.92618745 0.92618745 0.92618745\n",
      "  0.92618745 0.92618745 0.92618745 0.52581304]\n",
      " [1.35375286 1.35375286 1.35375286 1.35375286 0.09824764 1.35375286\n",
      "  1.35375286 1.35375286 1.35375286 1.35375286]\n",
      " [0.10888713 1.34311336 0.10888713 0.10888713 0.10888713 0.10888713\n",
      "  0.10888713 0.10888713 0.10888713 0.10888713]\n",
      " [0.09824764 1.35375286 0.09824764 0.09824764 0.09824764 0.09824764\n",
      "  0.09824764 0.09824764 0.09824764 0.09824764]\n",
      " [1.1054269  1.1054269  0.34657359 1.1054269  1.1054269  1.1054269\n",
      "  1.1054269  1.1054269  1.1054269  1.1054269 ]\n",
      " [0.16705333 0.16705333 0.16705333 0.16705333 0.16705333 0.16705333\n",
      "  0.16705333 1.28494717 0.16705333 0.16705333]\n",
      " [0.7834192  0.7834192  0.7834192  0.7834192  0.7834192  0.7834192\n",
      "  0.7834192  0.7834192  0.66858129 0.7834192 ]\n",
      " [1.34311336 1.34311336 1.34311336 0.10888713 1.34311336 1.34311336\n",
      "  1.34311336 1.34311336 1.34311336 1.34311336]]\n",
      "fexp:  [[3.43275393 3.43275393 3.43275393 3.43275393 3.43275393 3.43275393\n",
      "  0.         3.43275393 3.43275393 3.43275393]\n",
      " [0.43502258 0.43502258 0.43502258 0.43502258 0.43502258 0.43502258\n",
      "  0.43502258 0.43502258 0.43502258 0.        ]\n",
      " [1.49238336 1.49238336 1.49238336 1.49238336 1.49238336 1.49238336\n",
      "  1.49238336 1.49238336 1.49238336 0.        ]\n",
      " [3.50961105 3.50961105 3.50961105 3.50961105 0.         3.50961105\n",
      "  3.50961105 3.50961105 3.50961105 3.50961105]\n",
      " [0.29105989 0.         0.29105989 0.29105989 0.29105989 0.29105989\n",
      "  0.29105989 0.29105989 0.29105989 0.29105989]\n",
      " [0.28493186 0.         0.28493186 0.28493186 0.28493186 0.28493186\n",
      "  0.28493186 0.28493186 0.28493186 0.28493186]\n",
      " [2.13582569 2.13582569 0.         2.13582569 2.13582569 2.13582569\n",
      "  2.13582569 2.13582569 2.13582569 2.13582569]\n",
      " [0.32696772 0.32696772 0.32696772 0.32696772 0.32696772 0.32696772\n",
      "  0.32696772 0.         0.32696772 0.32696772]\n",
      " [1.12169161 1.12169161 1.12169161 1.12169161 1.12169161 1.12169161\n",
      "  1.12169161 1.12169161 0.         1.12169161]\n",
      " [3.43571903 3.43571903 3.43571903 0.         3.43571903 3.43571903\n",
      "  3.43571903 3.43571903 3.43571903 3.43571903]]\n",
      "Epoch 0\n",
      "Progress: 10000,  val accuracy: 0.2000\n",
      "Progress: 20000,  val accuracy: 0.2500\n",
      "Progress: 30000,  val accuracy: 0.2200\n",
      "Progress: 40000,  val accuracy: 0.3000\n",
      "Progress: 50000,  val accuracy: 0.2100\n",
      "Epoch 1\n",
      "Progress: 60000,  val accuracy: 0.2400\n",
      "Progress: 70000,  val accuracy: 0.2400\n",
      "Progress: 80000,  val accuracy: 0.3000\n",
      "Progress: 90000,  val accuracy: 0.2700\n",
      "Progress: 100000,  val accuracy: 0.2900\n",
      "Epoch 2\n",
      "Progress: 110000,  val accuracy: 0.2400\n",
      "Progress: 120000,  val accuracy: 0.2300\n",
      "Progress: 170000,  val accuracy: 0.3200\n",
      "Progress: 180000,  val accuracy: 0.3400\n",
      "Progress: 190000,  val accuracy: 0.3300\n",
      "Progress: 200000,  val accuracy: 0.2900\n",
      "Epoch 4\n",
      "Progress: 210000,  val accuracy: 0.2500\n",
      "Progress: 220000,  val accuracy: 0.2600\n",
      "Progress: 230000,  val accuracy: 0.3400\n",
      "Progress: 240000,  val accuracy: 0.2600\n",
      "Progress: 250000,  val accuracy: 0.3200\n",
      "Epoch 5\n",
      "Progress: 260000,  val accuracy: 0.2800\n",
      "Progress: 270000,  val accuracy: 0.3500\n",
      "Progress: 280000,  val accuracy: 0.3300\n",
      "Progress: 290000,  val accuracy: 0.2800\n",
      "Progress: 300000,  val accuracy: 0.3600\n",
      "Epoch 6\n",
      "Progress: 310000,  val accuracy: 0.2800\n",
      "Progress: 320000,  val accuracy: 0.4000\n",
      "Progress: 330000,  val accuracy: 0.3200\n",
      "Progress: 340000,  val accuracy: 0.3800\n",
      "Progress: 350000,  val accuracy: 0.3400\n",
      "Epoch 7\n",
      "Progress: 360000,  val accuracy: 0.2400\n",
      "Progress: 370000,  val accuracy: 0.4000\n",
      "Progress: 380000,  val accuracy: 0.4000\n",
      "Progress: 390000,  val accuracy: 0.3500\n",
      "Progress: 400000,  val accuracy: 0.4100\n",
      "Epoch 8\n",
      "Progress: 410000,  val accuracy: 0.3600\n",
      "Progress: 420000,  val accuracy: 0.3600\n",
      "Progress: 430000,  val accuracy: 0.3100\n",
      "Progress: 440000,  val accuracy: 0.3700\n",
      "Progress: 450000,  val accuracy: 0.3500\n",
      "Epoch 9\n",
      "Progress: 460000,  val accuracy: 0.3800\n",
      "Progress: 470000,  val accuracy: 0.3000\n",
      "Progress: 480000,  val accuracy: 0.4100\n",
      "Progress: 490000,  val accuracy: 0.3200\n",
      "Progress: 500000,  val accuracy: 0.5300\n",
      "Epoch 10\n",
      "Progress: 510000,  val accuracy: 0.3700\n",
      "Progress: 520000,  val accuracy: 0.4400\n",
      "Progress: 530000,  val accuracy: 0.4700\n",
      "Progress: 540000,  val accuracy: 0.5400\n",
      "Progress: 550000,  val accuracy: 0.3900\n",
      "Epoch 11\n",
      "Progress: 560000,  val accuracy: 0.4900\n",
      "Progress: 570000,  val accuracy: 0.4900\n",
      "Progress: 580000,  val accuracy: 0.4500\n",
      "Progress: 590000,  val accuracy: 0.5000\n",
      "Progress: 600000,  val accuracy: 0.4100\n",
      "Epoch 12\n",
      "Progress: 610000,  val accuracy: 0.4900\n",
      "Progress: 620000,  val accuracy: 0.4800\n",
      "Progress: 630000,  val accuracy: 0.6000\n",
      "Progress: 640000,  val accuracy: 0.4800\n",
      "Progress: 650000,  val accuracy: 0.5400\n",
      "Epoch 13\n",
      "Progress: 660000,  val accuracy: 0.5000\n",
      "Progress: 670000,  val accuracy: 0.4600\n",
      "Progress: 680000,  val accuracy: 0.5500\n",
      "Progress: 690000,  val accuracy: 0.4600\n",
      "Progress: 700000,  val accuracy: 0.4800\n",
      "Epoch 14\n",
      "Progress: 710000,  val accuracy: 0.5400\n",
      "Progress: 720000,  val accuracy: 0.5500\n",
      "Progress: 730000,  val accuracy: 0.5500\n",
      "Progress: 740000,  val accuracy: 0.5700\n",
      "Progress: 750000,  val accuracy: 0.5200\n",
      "Epoch 15\n",
      "Total training time1412.3956952095032\n",
      "After fit function:  5:36:14.973687\n",
      "Test accuracy of weak learner:  0.55\n",
      "Training accuracy of weak learner:  0.55754\n",
      "After train/test acc:  5:36:33.865323\n",
      "After allindices:  5:47:18.536787\n",
      "Predictions:  [3. 9. 7. 6. 9. 9. 4. 4. 0. 4.]\n",
      "Alpha:  0.07605269180803588\n",
      "before pessimistic update:  5:47:18.539424\n",
      "correct Indices Mask:  [False  True False ...  True False False]\n",
      "incorrect Indices Mask:  [ True False  True ... False  True  True]\n",
      "after pessimistic update:  5:47:18.546788\n",
      "t:  9 memory allocated: 141873664\n",
      "After WL  9  time elapsed(s):  20841\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training weak learner 10\n",
      "C_t:  [[  3.70400819   3.70400819   3.70400819   3.70400819   3.70400819\n",
      "    3.70400819 -33.33607368   3.70400819   3.70400819   3.70400819]\n",
      " [  0.40316473   0.40316473   0.40316473   0.40316473   0.40316473\n",
      "    0.40316473   0.40316473   0.40316473   0.40316473  -3.62848261]\n",
      " [  1.61031064   1.61031064   1.61031064   1.61031064   1.61031064\n",
      "    1.61031064   1.61031064   1.61031064   1.61031064 -14.49279577]\n",
      " [  3.78693851   3.78693851   3.78693851   3.78693851 -34.08244662\n",
      "    3.78693851   3.78693851   3.78693851   3.78693851   3.78693851]\n",
      " [  0.31405928  -2.82653348   0.31405928   0.31405928   0.31405928\n",
      "    0.31405928   0.31405928   0.31405928   0.31405928   0.31405928]\n",
      " [  0.30744701  -2.7670231    0.30744701   0.30744701   0.30744701\n",
      "    0.30744701   0.30744701   0.30744701   0.30744701   0.30744701]\n",
      " [  2.30459741   2.30459741 -20.74137671   2.30459741   2.30459741\n",
      "    2.30459741   2.30459741   2.30459741   2.30459741   2.30459741]\n",
      " [  0.35280452   0.35280452   0.35280452   0.35280452   0.35280452\n",
      "    0.35280452   0.35280452  -3.17524067   0.35280452   0.35280452]\n",
      " [  1.21032704   1.21032704   1.21032704   1.21032704   1.21032704\n",
      "    1.21032704   1.21032704   1.21032704 -10.89294339   1.21032704]\n",
      " [  3.70720758   3.70720758   3.70720758 -33.36486823   3.70720758\n",
      "    3.70720758   3.70720758   3.70720758   3.70720758   3.70720758]]\n",
      "targets:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3])\n",
      "f:  [[1.41873436 1.41873436 1.41873436 1.41873436 1.41873436 1.41873436\n",
      "  0.10931883 1.41873436 1.41873436 1.41873436]\n",
      " [0.30982158 0.30982158 0.30982158 0.30982158 0.30982158 0.30982158\n",
      "  0.30982158 0.30982158 0.30982158 1.21823161]\n",
      " [1.00224014 1.00224014 1.00224014 1.00224014 1.00224014 1.00224014\n",
      "  1.00224014 1.00224014 1.00224014 0.52581304]\n",
      " [1.42980555 1.42980555 1.42980555 1.42980555 0.09824764 1.42980555\n",
      "  1.42980555 1.42980555 1.42980555 1.42980555]\n",
      " [0.18493982 1.34311336 0.18493982 0.18493982 0.18493982 0.18493982\n",
      "  0.18493982 0.18493982 0.18493982 0.18493982]\n",
      " [0.17430033 1.35375286 0.17430033 0.17430033 0.17430033 0.17430033\n",
      "  0.17430033 0.17430033 0.17430033 0.17430033]\n",
      " [1.18147959 1.18147959 0.34657359 1.18147959 1.18147959 1.18147959\n",
      "  1.18147959 1.18147959 1.18147959 1.18147959]\n",
      " [0.24310602 0.24310602 0.24310602 0.24310602 0.24310602 0.24310602\n",
      "  0.24310602 1.28494717 0.24310602 0.24310602]\n",
      " [0.85947189 0.85947189 0.85947189 0.85947189 0.85947189 0.85947189\n",
      "  0.85947189 0.85947189 0.66858129 0.85947189]\n",
      " [1.41916605 1.41916605 1.41916605 0.10888713 1.41916605 1.41916605\n",
      "  1.41916605 1.41916605 1.41916605 1.41916605]]\n",
      "fexp:  [[3.70400819 3.70400819 3.70400819 3.70400819 3.70400819 3.70400819\n",
      "  0.         3.70400819 3.70400819 3.70400819]\n",
      " [0.40316473 0.40316473 0.40316473 0.40316473 0.40316473 0.40316473\n",
      "  0.40316473 0.40316473 0.40316473 0.        ]\n",
      " [1.61031064 1.61031064 1.61031064 1.61031064 1.61031064 1.61031064\n",
      "  1.61031064 1.61031064 1.61031064 0.        ]\n",
      " [3.78693851 3.78693851 3.78693851 3.78693851 0.         3.78693851\n",
      "  3.78693851 3.78693851 3.78693851 3.78693851]\n",
      " [0.31405928 0.         0.31405928 0.31405928 0.31405928 0.31405928\n",
      "  0.31405928 0.31405928 0.31405928 0.31405928]\n",
      " [0.30744701 0.         0.30744701 0.30744701 0.30744701 0.30744701\n",
      "  0.30744701 0.30744701 0.30744701 0.30744701]\n",
      " [2.30459741 2.30459741 0.         2.30459741 2.30459741 2.30459741\n",
      "  2.30459741 2.30459741 2.30459741 2.30459741]\n",
      " [0.35280452 0.35280452 0.35280452 0.35280452 0.35280452 0.35280452\n",
      "  0.35280452 0.         0.35280452 0.35280452]\n",
      " [1.21032704 1.21032704 1.21032704 1.21032704 1.21032704 1.21032704\n",
      "  1.21032704 1.21032704 0.         1.21032704]\n",
      " [3.70720758 3.70720758 3.70720758 0.         3.70720758 3.70720758\n",
      "  3.70720758 3.70720758 3.70720758 3.70720758]]\n",
      "Epoch 0\n",
      "Progress: 10000,  val accuracy: 0.2200\n",
      "Progress: 20000,  val accuracy: 0.2400\n",
      "Progress: 30000,  val accuracy: 0.2500\n",
      "Progress: 40000,  val accuracy: 0.2600\n",
      "Progress: 50000,  val accuracy: 0.1800\n",
      "Epoch 1\n",
      "Progress: 60000,  val accuracy: 0.2600\n",
      "Progress: 70000,  val accuracy: 0.2500\n",
      "Progress: 80000,  val accuracy: 0.2500\n",
      "Progress: 90000,  val accuracy: 0.2600\n",
      "Progress: 100000,  val accuracy: 0.2800\n",
      "Epoch 2\n",
      "Progress: 110000,  val accuracy: 0.2800\n",
      "Progress: 120000,  val accuracy: 0.2000\n",
      "Progress: 130000,  val accuracy: 0.3100\n",
      "Progress: 140000,  val accuracy: 0.3800\n",
      "Progress: 150000,  val accuracy: 0.2500\n",
      "Epoch 3\n",
      "Progress: 160000,  val accuracy: 0.2600\n",
      "Progress: 170000,  val accuracy: 0.2900\n",
      "Progress: 180000,  val accuracy: 0.3000\n",
      "Progress: 190000,  val accuracy: 0.2400\n",
      "Progress: 200000,  val accuracy: 0.2800\n",
      "Epoch 4\n",
      "Progress: 210000,  val accuracy: 0.2800\n",
      "Progress: 220000,  val accuracy: 0.2700\n",
      "Progress: 230000,  val accuracy: 0.2700\n",
      "Progress: 240000,  val accuracy: 0.2700\n",
      "Progress: 250000,  val accuracy: 0.2600\n",
      "Epoch 5\n",
      "Progress: 260000,  val accuracy: 0.3300\n",
      "Progress: 270000,  val accuracy: 0.3100\n",
      "Progress: 280000,  val accuracy: 0.2700\n",
      "Progress: 290000,  val accuracy: 0.3200\n",
      "Progress: 300000,  val accuracy: 0.2900\n",
      "Epoch 6\n",
      "Progress: 310000,  val accuracy: 0.3100\n",
      "Progress: 320000,  val accuracy: 0.3600\n",
      "Progress: 330000,  val accuracy: 0.3100\n",
      "Progress: 340000,  val accuracy: 0.3900\n",
      "Progress: 350000,  val accuracy: 0.3700\n",
      "Epoch 7\n",
      "Progress: 360000,  val accuracy: 0.2800\n",
      "Progress: 370000,  val accuracy: 0.3100\n",
      "Progress: 380000,  val accuracy: 0.3800\n",
      "Progress: 390000,  val accuracy: 0.3900\n",
      "Progress: 400000,  val accuracy: 0.3800\n",
      "Epoch 8\n",
      "Progress: 410000,  val accuracy: 0.2700\n",
      "Progress: 420000,  val accuracy: 0.4300\n",
      "Progress: 430000,  val accuracy: 0.3600\n",
      "Progress: 440000,  val accuracy: 0.3500\n",
      "Progress: 450000,  val accuracy: 0.3500\n",
      "Epoch 9\n",
      "Progress: 460000,  val accuracy: 0.4600\n",
      "Progress: 470000,  val accuracy: 0.3900\n",
      "Progress: 480000,  val accuracy: 0.4200\n",
      "Progress: 490000,  val accuracy: 0.3900\n",
      "Progress: 500000,  val accuracy: 0.3600\n",
      "Epoch 10\n",
      "Progress: 510000,  val accuracy: 0.4500\n",
      "Progress: 520000,  val accuracy: 0.4200\n",
      "Progress: 530000,  val accuracy: 0.3700\n",
      "Progress: 540000,  val accuracy: 0.4900\n",
      "Progress: 550000,  val accuracy: 0.5500\n",
      "Epoch 11\n",
      "Progress: 560000,  val accuracy: 0.4600\n",
      "Progress: 570000,  val accuracy: 0.4900\n",
      "Progress: 580000,  val accuracy: 0.4500\n",
      "Progress: 590000,  val accuracy: 0.4200\n",
      "Progress: 600000,  val accuracy: 0.4700\n",
      "Epoch 12\n",
      "Progress: 610000,  val accuracy: 0.4800\n",
      "Progress: 620000,  val accuracy: 0.4300\n",
      "Progress: 630000,  val accuracy: 0.4600\n",
      "Progress: 640000,  val accuracy: 0.3900\n",
      "Progress: 650000,  val accuracy: 0.4700\n",
      "Epoch 13\n",
      "Progress: 660000,  val accuracy: 0.4900\n",
      "Progress: 670000,  val accuracy: 0.4600\n",
      "Progress: 680000,  val accuracy: 0.5400\n",
      "Progress: 690000,  val accuracy: 0.5000\n",
      "Progress: 700000,  val accuracy: 0.5500\n",
      "Epoch 14\n",
      "Progress: 710000,  val accuracy: 0.4300\n",
      "Progress: 720000,  val accuracy: 0.4700\n",
      "Progress: 730000,  val accuracy: 0.5600\n",
      "Progress: 740000,  val accuracy: 0.5500\n",
      "Progress: 750000,  val accuracy: 0.5700\n",
      "Epoch 15\n",
      "Total training time1411.7996203899384\n",
      "After fit function:  6:10:53.997168\n",
      "Test accuracy of weak learner:  0.5917\n",
      "Training accuracy of weak learner:  0.614\n",
      "After train/test acc:  6:11:12.617536\n",
      "After allindices:  6:21:56.712896\n",
      "Predictions:  [5. 9. 7. 6. 1. 9. 4. 7. 0. 7.]\n",
      "Alpha:  0.06724847176347157\n",
      "before pessimistic update:  6:21:56.715370\n",
      "correct Indices Mask:  [False  True False ... False False False]\n",
      "incorrect Indices Mask:  [ True False  True ...  True  True  True]\n",
      "after pessimistic update:  6:21:56.722162\n",
      "t:  10 memory allocated: 141873664\n",
      "After WL  10  time elapsed(s):  22919\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training weak learner 11\n",
      "C_t:  [[  3.96166344   3.96166344   3.96166344   3.96166344   3.96166344\n",
      "    3.96166344 -35.65497099   3.96166344   3.96166344   3.96166344]\n",
      " [  0.37694405   0.37694405   0.37694405   0.37694405   0.37694405\n",
      "    0.37694405   0.37694405   0.37694405   0.37694405  -3.39249648]\n",
      " [  1.72232578   1.72232578   1.72232578   1.72232578   1.72232578\n",
      "    1.72232578   1.72232578   1.72232578   1.72232578 -15.50093205]\n",
      " [  4.0503625    4.0503625    4.0503625    4.0503625  -36.45326254\n",
      "    4.0503625    4.0503625    4.0503625    4.0503625    4.0503625 ]\n",
      " [  0.29363376  -2.64270383   0.29363376   0.29363376   0.29363376\n",
      "    0.29363376   0.29363376   0.29363376   0.29363376   0.29363376]\n",
      " [  0.32883339  -2.95950055   0.32883339   0.32883339   0.32883339\n",
      "    0.32883339   0.32883339   0.32883339   0.32883339   0.32883339]\n",
      " [  2.46490798   2.46490798 -22.18417179   2.46490798   2.46490798\n",
      "    2.46490798   2.46490798   2.46490798   2.46490798   2.46490798]\n",
      " [  0.32985912   0.32985912   0.32985912   0.32985912   0.32985912\n",
      "    0.32985912   0.32985912  -2.9687321    0.32985912   0.32985912]\n",
      " [  1.29451885   1.29451885   1.29451885   1.29451885   1.29451885\n",
      "    1.29451885   1.29451885   1.29451885 -11.65066961   1.29451885]\n",
      " [  3.96508539   3.96508539   3.96508539 -35.68576853   3.96508539\n",
      "    3.96508539   3.96508539   3.96508539   3.96508539   3.96508539]]\n",
      "targets:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3])\n",
      "f:  [[1.48598283 1.48598283 1.48598283 1.48598283 1.48598283 1.48598283\n",
      "  0.10931883 1.48598283 1.48598283 1.48598283]\n",
      " [0.30982158 0.30982158 0.30982158 0.30982158 0.30982158 0.30982158\n",
      "  0.30982158 0.30982158 0.30982158 1.28548008]\n",
      " [1.06948862 1.06948862 1.06948862 1.06948862 1.06948862 1.06948862\n",
      "  1.06948862 1.06948862 1.06948862 0.52581304]\n",
      " [1.49705402 1.49705402 1.49705402 1.49705402 0.09824764 1.49705402\n",
      "  1.49705402 1.49705402 1.49705402 1.49705402]\n",
      " [0.18493982 1.41036183 0.18493982 0.18493982 0.18493982 0.18493982\n",
      "  0.18493982 0.18493982 0.18493982 0.18493982]\n",
      " [0.2415488  1.35375286 0.2415488  0.2415488  0.2415488  0.2415488\n",
      "  0.2415488  0.2415488  0.2415488  0.2415488 ]\n",
      " [1.24872806 1.24872806 0.34657359 1.24872806 1.24872806 1.24872806\n",
      "  1.24872806 1.24872806 1.24872806 1.24872806]\n",
      " [0.24310602 0.24310602 0.24310602 0.24310602 0.24310602 0.24310602\n",
      "  0.24310602 1.35219564 0.24310602 0.24310602]\n",
      " [0.92672037 0.92672037 0.92672037 0.92672037 0.92672037 0.92672037\n",
      "  0.92672037 0.92672037 0.66858129 0.92672037]\n",
      " [1.48641452 1.48641452 1.48641452 0.10888713 1.48641452 1.48641452\n",
      "  1.48641452 1.48641452 1.48641452 1.48641452]]\n",
      "fexp:  [[3.96166344 3.96166344 3.96166344 3.96166344 3.96166344 3.96166344\n",
      "  0.         3.96166344 3.96166344 3.96166344]\n",
      " [0.37694405 0.37694405 0.37694405 0.37694405 0.37694405 0.37694405\n",
      "  0.37694405 0.37694405 0.37694405 0.        ]\n",
      " [1.72232578 1.72232578 1.72232578 1.72232578 1.72232578 1.72232578\n",
      "  1.72232578 1.72232578 1.72232578 0.        ]\n",
      " [4.0503625  4.0503625  4.0503625  4.0503625  0.         4.0503625\n",
      "  4.0503625  4.0503625  4.0503625  4.0503625 ]\n",
      " [0.29363376 0.         0.29363376 0.29363376 0.29363376 0.29363376\n",
      "  0.29363376 0.29363376 0.29363376 0.29363376]\n",
      " [0.32883339 0.         0.32883339 0.32883339 0.32883339 0.32883339\n",
      "  0.32883339 0.32883339 0.32883339 0.32883339]\n",
      " [2.46490798 2.46490798 0.         2.46490798 2.46490798 2.46490798\n",
      "  2.46490798 2.46490798 2.46490798 2.46490798]\n",
      " [0.32985912 0.32985912 0.32985912 0.32985912 0.32985912 0.32985912\n",
      "  0.32985912 0.         0.32985912 0.32985912]\n",
      " [1.29451885 1.29451885 1.29451885 1.29451885 1.29451885 1.29451885\n",
      "  1.29451885 1.29451885 0.         1.29451885]\n",
      " [3.96508539 3.96508539 3.96508539 0.         3.96508539 3.96508539\n",
      "  3.96508539 3.96508539 3.96508539 3.96508539]]\n",
      "Epoch 0\n",
      "Progress: 10000,  val accuracy: 0.2300\n",
      "Progress: 20000,  val accuracy: 0.2300\n",
      "Progress: 30000,  val accuracy: 0.2500\n",
      "Progress: 40000,  val accuracy: 0.2500\n",
      "Progress: 50000,  val accuracy: 0.2400\n",
      "Epoch 1\n",
      "Progress: 60000,  val accuracy: 0.2300\n",
      "Progress: 70000,  val accuracy: 0.2000\n",
      "Progress: 80000,  val accuracy: 0.1700\n",
      "Progress: 90000,  val accuracy: 0.2600\n",
      "Progress: 100000,  val accuracy: 0.2400\n",
      "Epoch 2\n",
      "Progress: 110000,  val accuracy: 0.2400\n",
      "Progress: 120000,  val accuracy: 0.2400\n",
      "Progress: 130000,  val accuracy: 0.2900\n",
      "Progress: 140000,  val accuracy: 0.2900\n",
      "Progress: 150000,  val accuracy: 0.2600\n",
      "Epoch 3\n",
      "Progress: 160000,  val accuracy: 0.2500\n",
      "Progress: 170000,  val accuracy: 0.3600\n",
      "Progress: 180000,  val accuracy: 0.2800\n",
      "Progress: 190000,  val accuracy: 0.3200\n",
      "Progress: 200000,  val accuracy: 0.2600\n",
      "Epoch 4\n",
      "Progress: 210000,  val accuracy: 0.3300\n",
      "Progress: 220000,  val accuracy: 0.2800\n",
      "Progress: 230000,  val accuracy: 0.2800\n",
      "Progress: 240000,  val accuracy: 0.3200\n",
      "Progress: 250000,  val accuracy: 0.3100\n",
      "Epoch 5\n",
      "Progress: 260000,  val accuracy: 0.3200\n",
      "Progress: 270000,  val accuracy: 0.3200\n",
      "Progress: 280000,  val accuracy: 0.3400\n",
      "Progress: 290000,  val accuracy: 0.3500\n",
      "Progress: 300000,  val accuracy: 0.2900\n",
      "Epoch 6\n",
      "Progress: 310000,  val accuracy: 0.3700\n",
      "Progress: 320000,  val accuracy: 0.2800\n",
      "Progress: 330000,  val accuracy: 0.3000\n",
      "Progress: 340000,  val accuracy: 0.3600\n",
      "Progress: 350000,  val accuracy: 0.3800\n",
      "Epoch 7\n",
      "Progress: 360000,  val accuracy: 0.2900\n",
      "Progress: 370000,  val accuracy: 0.3700\n",
      "Progress: 380000,  val accuracy: 0.3300\n",
      "Progress: 390000,  val accuracy: 0.3500\n",
      "Progress: 400000,  val accuracy: 0.3400\n",
      "Epoch 8\n",
      "Progress: 410000,  val accuracy: 0.3600\n",
      "Progress: 420000,  val accuracy: 0.4500\n",
      "Progress: 430000,  val accuracy: 0.3100\n",
      "Progress: 440000,  val accuracy: 0.3700\n",
      "Progress: 450000,  val accuracy: 0.3700\n",
      "Epoch 9\n",
      "Progress: 460000,  val accuracy: 0.3000\n",
      "Progress: 470000,  val accuracy: 0.4300\n",
      "Progress: 480000,  val accuracy: 0.4600\n",
      "Progress: 490000,  val accuracy: 0.3800\n",
      "Progress: 500000,  val accuracy: 0.3300\n",
      "Epoch 10\n",
      "Progress: 510000,  val accuracy: 0.3900\n",
      "Progress: 520000,  val accuracy: 0.3700\n",
      "Progress: 530000,  val accuracy: 0.4700\n",
      "Progress: 540000,  val accuracy: 0.4300\n",
      "Progress: 550000,  val accuracy: 0.4700\n",
      "Epoch 11\n",
      "Progress: 560000,  val accuracy: 0.4700\n",
      "Progress: 570000,  val accuracy: 0.4900\n",
      "Progress: 580000,  val accuracy: 0.3900\n",
      "Progress: 590000,  val accuracy: 0.5300\n",
      "Progress: 600000,  val accuracy: 0.4600\n",
      "Epoch 12\n",
      "Progress: 610000,  val accuracy: 0.4400\n",
      "Progress: 620000,  val accuracy: 0.5800\n",
      "Progress: 630000,  val accuracy: 0.4900\n",
      "Progress: 640000,  val accuracy: 0.5400\n",
      "Progress: 650000,  val accuracy: 0.4800\n",
      "Epoch 13\n",
      "Progress: 660000,  val accuracy: 0.4100\n",
      "Progress: 670000,  val accuracy: 0.4400\n",
      "Progress: 680000,  val accuracy: 0.4900\n",
      "Progress: 690000,  val accuracy: 0.4900\n",
      "Progress: 700000,  val accuracy: 0.5700\n",
      "Epoch 14\n",
      "Progress: 710000,  val accuracy: 0.5500\n",
      "Progress: 720000,  val accuracy: 0.4900\n",
      "Progress: 730000,  val accuracy: 0.4900\n",
      "Progress: 740000,  val accuracy: 0.4900\n",
      "Progress: 750000,  val accuracy: 0.5200\n",
      "Epoch 15\n",
      "Total training time1410.4263815879822\n",
      "After fit function:  6:45:30.331714\n",
      "Test accuracy of weak learner:  0.5046\n",
      "Training accuracy of weak learner:  0.51638\n",
      "After train/test acc:  6:45:49.083149\n",
      "After allindices:  6:56:33.512911\n",
      "Predictions:  [4. 9. 9. 4. 9. 9. 4. 7. 8. 7.]\n",
      "Alpha:  0.06434688327509575\n",
      "before pessimistic update:  6:56:33.515417\n",
      "correct Indices Mask:  [False  True  True ...  True False False]\n",
      "incorrect Indices Mask:  [ True False False ... False  True  True]\n",
      "after pessimistic update:  6:56:33.522957\n",
      "t:  11 memory allocated: 141873664\n",
      "After WL  11  time elapsed(s):  24996\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training weak learner 12\n",
      "C_t:  [[  4.2249646    4.2249646    4.2249646    4.2249646    4.2249646\n",
      "    4.2249646  -38.02468139   4.2249646    4.2249646    4.2249646 ]\n",
      " [  0.35345278   0.35345278   0.35345278   0.35345278   0.35345278\n",
      "    0.35345278   0.35345278   0.35345278   0.35345278  -3.181075  ]\n",
      " [  1.61498989   1.61498989   1.61498989   1.61498989   1.61498989\n",
      "    1.61498989   1.61498989   1.61498989   1.61498989 -14.53490897]\n",
      " [  3.79794261   3.79794261   3.79794261   3.79794261 -34.18148347\n",
      "    3.79794261   3.79794261   3.79794261   3.79794261   3.79794261]\n",
      " [  0.31314933  -2.81834393   0.31314933   0.31314933   0.31314933\n",
      "    0.31314933   0.31314933   0.31314933   0.31314933   0.31314933]\n",
      " [  0.35068841  -3.15619568   0.35068841   0.35068841   0.35068841\n",
      "    0.35068841   0.35068841   0.35068841   0.35068841   0.35068841]\n",
      " [  2.62873136   2.62873136 -23.65858226   2.62873136   2.62873136\n",
      "    2.62873136   2.62873136   2.62873136   2.62873136   2.62873136]\n",
      " [  0.3093022    0.3093022    0.3093022    0.3093022    0.3093022\n",
      "    0.3093022    0.3093022   -2.78371976   0.3093022    0.3093022 ]\n",
      " [  1.21384401   1.21384401   1.21384401   1.21384401   1.21384401\n",
      "    1.21384401   1.21384401   1.21384401 -10.92459613   1.21384401]\n",
      " [  4.22861398   4.22861398   4.22861398 -38.05752581   4.22861398\n",
      "    4.22861398   4.22861398   4.22861398   4.22861398   4.22861398]]\n",
      "targets:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3])\n",
      "f:  [[1.55032971 1.55032971 1.55032971 1.55032971 1.55032971 1.55032971\n",
      "  0.10931883 1.55032971 1.55032971 1.55032971]\n",
      " [0.30982158 0.30982158 0.30982158 0.30982158 0.30982158 0.30982158\n",
      "  0.30982158 0.30982158 0.30982158 1.34982696]\n",
      " [1.06948862 1.06948862 1.06948862 1.06948862 1.06948862 1.06948862\n",
      "  1.06948862 1.06948862 1.06948862 0.59015992]\n",
      " [1.49705402 1.49705402 1.49705402 1.49705402 0.16259452 1.49705402\n",
      "  1.49705402 1.49705402 1.49705402 1.49705402]\n",
      " [0.24928671 1.41036183 0.24928671 0.24928671 0.24928671 0.24928671\n",
      "  0.24928671 0.24928671 0.24928671 0.24928671]\n",
      " [0.30589568 1.35375286 0.30589568 0.30589568 0.30589568 0.30589568\n",
      "  0.30589568 0.30589568 0.30589568 0.30589568]\n",
      " [1.31307495 1.31307495 0.34657359 1.31307495 1.31307495 1.31307495\n",
      "  1.31307495 1.31307495 1.31307495 1.31307495]\n",
      " [0.24310602 0.24310602 0.24310602 0.24310602 0.24310602 0.24310602\n",
      "  0.24310602 1.41654252 0.24310602 0.24310602]\n",
      " [0.92672037 0.92672037 0.92672037 0.92672037 0.92672037 0.92672037\n",
      "  0.92672037 0.92672037 0.73292817 0.92672037]\n",
      " [1.55076141 1.55076141 1.55076141 0.10888713 1.55076141 1.55076141\n",
      "  1.55076141 1.55076141 1.55076141 1.55076141]]\n",
      "fexp:  [[4.2249646  4.2249646  4.2249646  4.2249646  4.2249646  4.2249646\n",
      "  0.         4.2249646  4.2249646  4.2249646 ]\n",
      " [0.35345278 0.35345278 0.35345278 0.35345278 0.35345278 0.35345278\n",
      "  0.35345278 0.35345278 0.35345278 0.        ]\n",
      " [1.61498989 1.61498989 1.61498989 1.61498989 1.61498989 1.61498989\n",
      "  1.61498989 1.61498989 1.61498989 0.        ]\n",
      " [3.79794261 3.79794261 3.79794261 3.79794261 0.         3.79794261\n",
      "  3.79794261 3.79794261 3.79794261 3.79794261]\n",
      " [0.31314933 0.         0.31314933 0.31314933 0.31314933 0.31314933\n",
      "  0.31314933 0.31314933 0.31314933 0.31314933]\n",
      " [0.35068841 0.         0.35068841 0.35068841 0.35068841 0.35068841\n",
      "  0.35068841 0.35068841 0.35068841 0.35068841]\n",
      " [2.62873136 2.62873136 0.         2.62873136 2.62873136 2.62873136\n",
      "  2.62873136 2.62873136 2.62873136 2.62873136]\n",
      " [0.3093022  0.3093022  0.3093022  0.3093022  0.3093022  0.3093022\n",
      "  0.3093022  0.         0.3093022  0.3093022 ]\n",
      " [1.21384401 1.21384401 1.21384401 1.21384401 1.21384401 1.21384401\n",
      "  1.21384401 1.21384401 0.         1.21384401]\n",
      " [4.22861398 4.22861398 4.22861398 0.         4.22861398 4.22861398\n",
      "  4.22861398 4.22861398 4.22861398 4.22861398]]\n",
      "Epoch 0\n",
      "Progress: 10000,  val accuracy: 0.2300\n",
      "Progress: 20000,  val accuracy: 0.1800\n",
      "Progress: 30000,  val accuracy: 0.1800\n",
      "Progress: 40000,  val accuracy: 0.2100\n",
      "Progress: 50000,  val accuracy: 0.2100\n",
      "Epoch 1\n",
      "Progress: 60000,  val accuracy: 0.2300\n",
      "Progress: 70000,  val accuracy: 0.2100\n",
      "Progress: 80000,  val accuracy: 0.2600\n",
      "Progress: 90000,  val accuracy: 0.2400\n",
      "Progress: 100000,  val accuracy: 0.2300\n",
      "Epoch 2\n",
      "Progress: 110000,  val accuracy: 0.2800\n",
      "Progress: 120000,  val accuracy: 0.2600\n",
      "Progress: 130000,  val accuracy: 0.3100\n",
      "Progress: 140000,  val accuracy: 0.2100\n",
      "Progress: 150000,  val accuracy: 0.1800\n",
      "Epoch 3\n",
      "Progress: 160000,  val accuracy: 0.2700\n",
      "Progress: 170000,  val accuracy: 0.2500\n",
      "Progress: 180000,  val accuracy: 0.3000\n",
      "Progress: 190000,  val accuracy: 0.2800\n",
      "Progress: 200000,  val accuracy: 0.1700\n",
      "Epoch 4\n",
      "Progress: 210000,  val accuracy: 0.2800\n",
      "Progress: 220000,  val accuracy: 0.2700\n",
      "Progress: 230000,  val accuracy: 0.2500\n",
      "Progress: 240000,  val accuracy: 0.3100\n",
      "Progress: 250000,  val accuracy: 0.3100\n",
      "Epoch 5\n",
      "Progress: 260000,  val accuracy: 0.2900\n",
      "Progress: 270000,  val accuracy: 0.2700\n",
      "Progress: 280000,  val accuracy: 0.2300\n",
      "Progress: 290000,  val accuracy: 0.2200\n",
      "Progress: 300000,  val accuracy: 0.2500\n",
      "Epoch 6\n",
      "Progress: 310000,  val accuracy: 0.3200\n",
      "Progress: 320000,  val accuracy: 0.3000\n",
      "Progress: 330000,  val accuracy: 0.2700\n",
      "Progress: 340000,  val accuracy: 0.2500\n",
      "Progress: 350000,  val accuracy: 0.2800\n",
      "Epoch 7\n",
      "Progress: 360000,  val accuracy: 0.4000\n",
      "Progress: 370000,  val accuracy: 0.3000\n",
      "Progress: 380000,  val accuracy: 0.3200\n",
      "Progress: 390000,  val accuracy: 0.3300\n",
      "Progress: 400000,  val accuracy: 0.2900\n",
      "Epoch 8\n",
      "Progress: 410000,  val accuracy: 0.3200\n",
      "Progress: 420000,  val accuracy: 0.4400\n",
      "Progress: 430000,  val accuracy: 0.3300\n",
      "Progress: 440000,  val accuracy: 0.4300\n",
      "Progress: 450000,  val accuracy: 0.3500\n",
      "Epoch 9\n",
      "Progress: 460000,  val accuracy: 0.2600\n",
      "Progress: 470000,  val accuracy: 0.4600\n",
      "Progress: 480000,  val accuracy: 0.3600\n",
      "Progress: 490000,  val accuracy: 0.4200\n",
      "Progress: 500000,  val accuracy: 0.4200\n",
      "Epoch 10\n",
      "Progress: 510000,  val accuracy: 0.3600\n",
      "Progress: 520000,  val accuracy: 0.3800\n",
      "Progress: 530000,  val accuracy: 0.4000\n",
      "Progress: 540000,  val accuracy: 0.4500\n",
      "Progress: 550000,  val accuracy: 0.3500\n",
      "Epoch 11\n",
      "Progress: 560000,  val accuracy: 0.4300\n",
      "Progress: 570000,  val accuracy: 0.4800\n",
      "Progress: 580000,  val accuracy: 0.4300\n",
      "Progress: 590000,  val accuracy: 0.4800\n",
      "Progress: 600000,  val accuracy: 0.4200\n",
      "Epoch 12\n",
      "Progress: 610000,  val accuracy: 0.4700\n",
      "Progress: 620000,  val accuracy: 0.5400\n",
      "Progress: 630000,  val accuracy: 0.5300\n",
      "Progress: 640000,  val accuracy: 0.4900\n",
      "Progress: 650000,  val accuracy: 0.4000\n",
      "Epoch 13\n",
      "Progress: 660000,  val accuracy: 0.4400\n",
      "Progress: 670000,  val accuracy: 0.4900\n",
      "Progress: 680000,  val accuracy: 0.5200\n",
      "Progress: 690000,  val accuracy: 0.5500\n",
      "Progress: 700000,  val accuracy: 0.5400\n",
      "Epoch 14\n",
      "Progress: 710000,  val accuracy: 0.4200\n",
      "Progress: 720000,  val accuracy: 0.4800\n",
      "Progress: 730000,  val accuracy: 0.4400\n",
      "Progress: 740000,  val accuracy: 0.5400\n",
      "Progress: 750000,  val accuracy: 0.5500\n",
      "Epoch 15\n",
      "Total training time1411.2302236557007\n",
      "After fit function:  7:20:08.100243\n",
      "Test accuracy of weak learner:  0.5113\n",
      "Training accuracy of weak learner:  0.52922\n",
      "After train/test acc:  7:20:27.008508\n",
      "After allindices:  7:31:11.325928\n",
      "Predictions:  [5. 9. 4. 4. 1. 9. 4. 7. 0. 1.]\n",
      "Alpha:  0.05809093355408436\n",
      "before pessimistic update:  7:31:11.328709\n",
      "correct Indices Mask:  [False  True False ...  True  True False]\n",
      "incorrect Indices Mask:  [ True False  True ... False False  True]\n",
      "after pessimistic update:  7:31:11.335735\n",
      "t:  12 memory allocated: 141873664\n",
      "After WL  12  time elapsed(s):  27074\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training weak learner 13\n",
      "C_t:  [[  4.47766549   4.47766549   4.47766549   4.47766549   4.47766549\n",
      "    4.47766549 -40.29898944   4.47766549   4.47766549   4.47766549]\n",
      " [  0.33350537   0.33350537   0.33350537   0.33350537   0.33350537\n",
      "    0.33350537   0.33350537   0.33350537   0.33350537  -3.00154831]\n",
      " [  1.71158463   1.71158463   1.71158463   1.71158463   1.71158463\n",
      "    1.71158463   1.71158463   1.71158463   1.71158463 -15.4042617 ]\n",
      " [  3.58360246   3.58360246   3.58360246   3.58360246 -32.25242212\n",
      "    3.58360246   3.58360246   3.58360246   3.58360246   3.58360246]\n",
      " [  0.29547647  -2.65928827   0.29547647   0.29547647   0.29547647\n",
      "    0.29547647   0.29547647   0.29547647   0.29547647   0.29547647]\n",
      " [  0.37166356  -3.34497205   0.37166356   0.37166356   0.37166356\n",
      "    0.37166356   0.37166356   0.37166356   0.37166356   0.37166356]\n",
      " [  2.78595937   2.78595937 -25.07363433   2.78595937   2.78595937\n",
      "    2.78595937   2.78595937   2.78595937   2.78595937   2.78595937]\n",
      " [  0.29184646   0.29184646   0.29184646   0.29184646   0.29184646\n",
      "    0.29184646   0.29184646  -2.62661814   0.29184646   0.29184646]\n",
      " [  1.28644568   1.28644568   1.28644568   1.28644568   1.28644568\n",
      "    1.28644568   1.28644568   1.28644568 -11.57801112   1.28644568]\n",
      " [  4.48153315   4.48153315   4.48153315 -40.33379833   4.48153315\n",
      "    4.48153315   4.48153315   4.48153315   4.48153315   4.48153315]]\n",
      "targets:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3])\n",
      "f:  [[1.60842064 1.60842064 1.60842064 1.60842064 1.60842064 1.60842064\n",
      "  0.10931883 1.60842064 1.60842064 1.60842064]\n",
      " [0.30982158 0.30982158 0.30982158 0.30982158 0.30982158 0.30982158\n",
      "  0.30982158 0.30982158 0.30982158 1.4079179 ]\n",
      " [1.12757955 1.12757955 1.12757955 1.12757955 1.12757955 1.12757955\n",
      "  1.12757955 1.12757955 1.12757955 0.59015992]\n",
      " [1.49705402 1.49705402 1.49705402 1.49705402 0.22068545 1.49705402\n",
      "  1.49705402 1.49705402 1.49705402 1.49705402]\n",
      " [0.24928671 1.46845277 0.24928671 0.24928671 0.24928671 0.24928671\n",
      "  0.24928671 0.24928671 0.24928671 0.24928671]\n",
      " [0.36398662 1.35375286 0.36398662 0.36398662 0.36398662 0.36398662\n",
      "  0.36398662 0.36398662 0.36398662 0.36398662]\n",
      " [1.37116588 1.37116588 0.34657359 1.37116588 1.37116588 1.37116588\n",
      "  1.37116588 1.37116588 1.37116588 1.37116588]\n",
      " [0.24310602 0.24310602 0.24310602 0.24310602 0.24310602 0.24310602\n",
      "  0.24310602 1.47463345 0.24310602 0.24310602]\n",
      " [0.9848113  0.9848113  0.9848113  0.9848113  0.9848113  0.9848113\n",
      "  0.9848113  0.9848113  0.73292817 0.9848113 ]\n",
      " [1.60885234 1.60885234 1.60885234 0.10888713 1.60885234 1.60885234\n",
      "  1.60885234 1.60885234 1.60885234 1.60885234]]\n",
      "fexp:  [[4.47766549 4.47766549 4.47766549 4.47766549 4.47766549 4.47766549\n",
      "  0.         4.47766549 4.47766549 4.47766549]\n",
      " [0.33350537 0.33350537 0.33350537 0.33350537 0.33350537 0.33350537\n",
      "  0.33350537 0.33350537 0.33350537 0.        ]\n",
      " [1.71158463 1.71158463 1.71158463 1.71158463 1.71158463 1.71158463\n",
      "  1.71158463 1.71158463 1.71158463 0.        ]\n",
      " [3.58360246 3.58360246 3.58360246 3.58360246 0.         3.58360246\n",
      "  3.58360246 3.58360246 3.58360246 3.58360246]\n",
      " [0.29547647 0.         0.29547647 0.29547647 0.29547647 0.29547647\n",
      "  0.29547647 0.29547647 0.29547647 0.29547647]\n",
      " [0.37166356 0.         0.37166356 0.37166356 0.37166356 0.37166356\n",
      "  0.37166356 0.37166356 0.37166356 0.37166356]\n",
      " [2.78595937 2.78595937 0.         2.78595937 2.78595937 2.78595937\n",
      "  2.78595937 2.78595937 2.78595937 2.78595937]\n",
      " [0.29184646 0.29184646 0.29184646 0.29184646 0.29184646 0.29184646\n",
      "  0.29184646 0.         0.29184646 0.29184646]\n",
      " [1.28644568 1.28644568 1.28644568 1.28644568 1.28644568 1.28644568\n",
      "  1.28644568 1.28644568 0.         1.28644568]\n",
      " [4.48153315 4.48153315 4.48153315 0.         4.48153315 4.48153315\n",
      "  4.48153315 4.48153315 4.48153315 4.48153315]]\n",
      "Epoch 0\n",
      "Progress: 10000,  val accuracy: 0.1900\n",
      "Progress: 20000,  val accuracy: 0.1900\n",
      "Progress: 30000,  val accuracy: 0.2300\n",
      "Progress: 40000,  val accuracy: 0.2400\n",
      "Progress: 50000,  val accuracy: 0.2600\n",
      "Epoch 1\n",
      "Progress: 60000,  val accuracy: 0.2500\n",
      "Progress: 70000,  val accuracy: 0.2800\n",
      "Progress: 80000,  val accuracy: 0.2700\n",
      "Progress: 90000,  val accuracy: 0.2500\n",
      "Progress: 100000,  val accuracy: 0.2300\n",
      "Epoch 2\n",
      "Progress: 110000,  val accuracy: 0.2900\n",
      "Progress: 120000,  val accuracy: 0.2400\n",
      "Progress: 130000,  val accuracy: 0.2800\n",
      "Progress: 140000,  val accuracy: 0.1900\n",
      "Progress: 150000,  val accuracy: 0.2200\n",
      "Epoch 3\n",
      "Progress: 160000,  val accuracy: 0.1800\n",
      "Progress: 170000,  val accuracy: 0.2200\n",
      "Progress: 180000,  val accuracy: 0.2700\n",
      "Progress: 190000,  val accuracy: 0.2100\n",
      "Progress: 200000,  val accuracy: 0.2700\n",
      "Epoch 4\n",
      "Progress: 210000,  val accuracy: 0.3100\n",
      "Progress: 220000,  val accuracy: 0.2400\n",
      "Progress: 230000,  val accuracy: 0.3000\n",
      "Progress: 240000,  val accuracy: 0.2500\n",
      "Progress: 250000,  val accuracy: 0.2700\n",
      "Epoch 5\n",
      "Progress: 260000,  val accuracy: 0.2800\n",
      "Progress: 270000,  val accuracy: 0.2900\n",
      "Progress: 280000,  val accuracy: 0.2500\n",
      "Progress: 290000,  val accuracy: 0.3300\n",
      "Progress: 300000,  val accuracy: 0.2400\n",
      "Epoch 6\n",
      "Progress: 310000,  val accuracy: 0.2700\n",
      "Progress: 320000,  val accuracy: 0.2700\n",
      "Progress: 330000,  val accuracy: 0.3400\n",
      "Progress: 340000,  val accuracy: 0.2900\n",
      "Progress: 350000,  val accuracy: 0.3400\n",
      "Epoch 7\n",
      "Progress: 360000,  val accuracy: 0.3000\n",
      "Progress: 370000,  val accuracy: 0.2900\n",
      "Progress: 380000,  val accuracy: 0.3400\n",
      "Progress: 390000,  val accuracy: 0.3600\n",
      "Progress: 400000,  val accuracy: 0.2400\n",
      "Epoch 8\n",
      "Progress: 410000,  val accuracy: 0.2900\n",
      "Progress: 420000,  val accuracy: 0.3000\n",
      "Progress: 430000,  val accuracy: 0.2700\n",
      "Progress: 440000,  val accuracy: 0.3500\n",
      "Progress: 450000,  val accuracy: 0.4500\n",
      "Epoch 9\n",
      "Progress: 460000,  val accuracy: 0.3500\n",
      "Progress: 470000,  val accuracy: 0.4300\n",
      "Progress: 480000,  val accuracy: 0.4100\n",
      "Progress: 490000,  val accuracy: 0.2900\n",
      "Progress: 500000,  val accuracy: 0.3600\n",
      "Epoch 10\n",
      "Progress: 510000,  val accuracy: 0.4100\n",
      "Progress: 520000,  val accuracy: 0.3900\n",
      "Progress: 530000,  val accuracy: 0.4000\n",
      "Progress: 540000,  val accuracy: 0.5000\n",
      "Progress: 550000,  val accuracy: 0.4100\n",
      "Epoch 11\n",
      "Progress: 560000,  val accuracy: 0.3400\n",
      "Progress: 570000,  val accuracy: 0.4400\n",
      "Progress: 580000,  val accuracy: 0.3400\n",
      "Progress: 590000,  val accuracy: 0.4000\n",
      "Progress: 600000,  val accuracy: 0.4300\n",
      "Epoch 12\n",
      "Progress: 610000,  val accuracy: 0.5100\n",
      "Progress: 620000,  val accuracy: 0.4700\n",
      "Progress: 630000,  val accuracy: 0.4400\n",
      "Progress: 640000,  val accuracy: 0.4500\n",
      "Progress: 650000,  val accuracy: 0.4500\n",
      "Epoch 13\n",
      "Progress: 660000,  val accuracy: 0.3600\n",
      "Progress: 670000,  val accuracy: 0.4800\n",
      "Progress: 680000,  val accuracy: 0.5100\n",
      "Progress: 690000,  val accuracy: 0.4100\n",
      "Progress: 700000,  val accuracy: 0.4700\n",
      "Epoch 14\n",
      "Progress: 710000,  val accuracy: 0.4700\n",
      "Progress: 720000,  val accuracy: 0.4900\n",
      "Progress: 730000,  val accuracy: 0.4700\n",
      "Progress: 740000,  val accuracy: 0.4800\n",
      "Progress: 750000,  val accuracy: 0.5300\n",
      "Epoch 15\n",
      "Total training time1410.9133863449097\n",
      "After fit function:  7:54:45.404919\n",
      "Test accuracy of weak learner:  0.4811\n",
      "Training accuracy of weak learner:  0.4909\n",
      "After train/test acc:  7:55:04.246538\n",
      "After allindices:  8:05:48.452700\n",
      "Predictions:  [5. 9. 9. 4. 9. 9. 4. 7. 8. 9.]\n",
      "Alpha:  0.06696606802846267\n",
      "before pessimistic update:  8:05:48.456486\n",
      "correct Indices Mask:  [False  True  True ...  True False False]\n",
      "incorrect Indices Mask:  [ True False False ... False  True  True]\n",
      "after pessimistic update:  8:05:48.464704\n",
      "t:  13 memory allocated: 141873664\n",
      "After WL  13  time elapsed(s):  29151\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training weak learner 14\n",
      "C_t:  [[  4.787785     4.787785     4.787785     4.787785     4.787785\n",
      "    4.787785   -43.09006503   4.787785     4.787785     4.787785  ]\n",
      " [  0.3119032    0.3119032    0.3119032    0.3119032    0.3119032\n",
      "    0.3119032    0.3119032    0.3119032    0.3119032   -2.80712882]\n",
      " [  1.60072005   1.60072005   1.60072005   1.60072005   1.60072005\n",
      "    1.60072005   1.60072005   1.60072005   1.60072005 -14.40648045]\n",
      " [  3.35148154   3.35148154   3.35148154   3.35148154 -30.16333388\n",
      "    3.35148154   3.35148154   3.35148154   3.35148154   3.35148154]\n",
      " [  0.31594094  -2.84346844   0.31594094   0.31594094   0.31594094\n",
      "    0.31594094   0.31594094   0.31594094   0.31594094   0.31594094]\n",
      " [  0.39740468  -3.57664212   0.39740468   0.39740468   0.39740468\n",
      "    0.39740468   0.39740468   0.39740468   0.39740468   0.39740468]\n",
      " [  2.97891267   2.97891267 -26.81021407   2.97891267   2.97891267\n",
      "    2.97891267   2.97891267   2.97891267   2.97891267   2.97891267]\n",
      " [  0.27294267   0.27294267   0.27294267   0.27294267   0.27294267\n",
      "    0.27294267   0.27294267  -2.45648404   0.27294267   0.27294267]\n",
      " [  1.20311865   1.20311865   1.20311865   1.20311865   1.20311865\n",
      "    1.20311865   1.20311865   1.20311865 -10.82806785   1.20311865]\n",
      " [  4.79192053   4.79192053   4.79192053 -43.12728475   4.79192053\n",
      "    4.79192053   4.79192053   4.79192053   4.79192053   4.79192053]]\n",
      "targets:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3])\n",
      "f:  [[1.67538671 1.67538671 1.67538671 1.67538671 1.67538671 1.67538671\n",
      "  0.10931883 1.67538671 1.67538671 1.67538671]\n",
      " [0.30982158 0.30982158 0.30982158 0.30982158 0.30982158 0.30982158\n",
      "  0.30982158 0.30982158 0.30982158 1.47488396]\n",
      " [1.12757955 1.12757955 1.12757955 1.12757955 1.12757955 1.12757955\n",
      "  1.12757955 1.12757955 1.12757955 0.65712599]\n",
      " [1.49705402 1.49705402 1.49705402 1.49705402 0.28765152 1.49705402\n",
      "  1.49705402 1.49705402 1.49705402 1.49705402]\n",
      " [0.31625277 1.46845277 0.31625277 0.31625277 0.31625277 0.31625277\n",
      "  0.31625277 0.31625277 0.31625277 0.31625277]\n",
      " [0.43095268 1.35375286 0.43095268 0.43095268 0.43095268 0.43095268\n",
      "  0.43095268 0.43095268 0.43095268 0.43095268]\n",
      " [1.43813195 1.43813195 0.34657359 1.43813195 1.43813195 1.43813195\n",
      "  1.43813195 1.43813195 1.43813195 1.43813195]\n",
      " [0.24310602 0.24310602 0.24310602 0.24310602 0.24310602 0.24310602\n",
      "  0.24310602 1.54159952 0.24310602 0.24310602]\n",
      " [0.9848113  0.9848113  0.9848113  0.9848113  0.9848113  0.9848113\n",
      "  0.9848113  0.9848113  0.79989424 0.9848113 ]\n",
      " [1.67581841 1.67581841 1.67581841 0.10888713 1.67581841 1.67581841\n",
      "  1.67581841 1.67581841 1.67581841 1.67581841]]\n",
      "fexp:  [[4.787785   4.787785   4.787785   4.787785   4.787785   4.787785\n",
      "  0.         4.787785   4.787785   4.787785  ]\n",
      " [0.3119032  0.3119032  0.3119032  0.3119032  0.3119032  0.3119032\n",
      "  0.3119032  0.3119032  0.3119032  0.        ]\n",
      " [1.60072005 1.60072005 1.60072005 1.60072005 1.60072005 1.60072005\n",
      "  1.60072005 1.60072005 1.60072005 0.        ]\n",
      " [3.35148154 3.35148154 3.35148154 3.35148154 0.         3.35148154\n",
      "  3.35148154 3.35148154 3.35148154 3.35148154]\n",
      " [0.31594094 0.         0.31594094 0.31594094 0.31594094 0.31594094\n",
      "  0.31594094 0.31594094 0.31594094 0.31594094]\n",
      " [0.39740468 0.         0.39740468 0.39740468 0.39740468 0.39740468\n",
      "  0.39740468 0.39740468 0.39740468 0.39740468]\n",
      " [2.97891267 2.97891267 0.         2.97891267 2.97891267 2.97891267\n",
      "  2.97891267 2.97891267 2.97891267 2.97891267]\n",
      " [0.27294267 0.27294267 0.27294267 0.27294267 0.27294267 0.27294267\n",
      "  0.27294267 0.         0.27294267 0.27294267]\n",
      " [1.20311865 1.20311865 1.20311865 1.20311865 1.20311865 1.20311865\n",
      "  1.20311865 1.20311865 0.         1.20311865]\n",
      " [4.79192053 4.79192053 4.79192053 0.         4.79192053 4.79192053\n",
      "  4.79192053 4.79192053 4.79192053 4.79192053]]\n",
      "Epoch 0\n",
      "Progress: 10000,  val accuracy: 0.1900\n",
      "Progress: 20000,  val accuracy: 0.2200\n",
      "Progress: 30000,  val accuracy: 0.1800\n",
      "Progress: 40000,  val accuracy: 0.2100\n",
      "Progress: 50000,  val accuracy: 0.1600\n",
      "Epoch 1\n",
      "Progress: 60000,  val accuracy: 0.1600\n",
      "Progress: 70000,  val accuracy: 0.2300\n",
      "Progress: 80000,  val accuracy: 0.1900\n",
      "Progress: 90000,  val accuracy: 0.2800\n",
      "Progress: 100000,  val accuracy: 0.2200\n",
      "Epoch 2\n",
      "Progress: 110000,  val accuracy: 0.2600\n",
      "Progress: 120000,  val accuracy: 0.2400\n",
      "Progress: 130000,  val accuracy: 0.2700\n",
      "Progress: 140000,  val accuracy: 0.2400\n",
      "Progress: 150000,  val accuracy: 0.1700\n",
      "Epoch 3\n",
      "Progress: 160000,  val accuracy: 0.1800\n",
      "Progress: 170000,  val accuracy: 0.2400\n",
      "Progress: 180000,  val accuracy: 0.1900\n",
      "Progress: 190000,  val accuracy: 0.1900\n",
      "Progress: 200000,  val accuracy: 0.1800\n",
      "Epoch 4\n",
      "Progress: 210000,  val accuracy: 0.2000\n",
      "Progress: 220000,  val accuracy: 0.3100\n",
      "Progress: 230000,  val accuracy: 0.2600\n",
      "Progress: 240000,  val accuracy: 0.2300\n",
      "Progress: 250000,  val accuracy: 0.2000\n",
      "Epoch 5\n",
      "Progress: 260000,  val accuracy: 0.2500\n",
      "Progress: 270000,  val accuracy: 0.3100\n",
      "Progress: 280000,  val accuracy: 0.2900\n",
      "Progress: 290000,  val accuracy: 0.2200\n",
      "Progress: 300000,  val accuracy: 0.2400\n",
      "Epoch 6\n",
      "Progress: 310000,  val accuracy: 0.3100\n",
      "Progress: 320000,  val accuracy: 0.3100\n",
      "Progress: 330000,  val accuracy: 0.2900\n",
      "Progress: 340000,  val accuracy: 0.3500\n",
      "Progress: 350000,  val accuracy: 0.2300\n",
      "Epoch 7\n",
      "Progress: 360000,  val accuracy: 0.3200\n",
      "Progress: 370000,  val accuracy: 0.3800\n",
      "Progress: 380000,  val accuracy: 0.3600\n",
      "Progress: 390000,  val accuracy: 0.3100\n",
      "Progress: 400000,  val accuracy: 0.3000\n",
      "Epoch 8\n",
      "Progress: 410000,  val accuracy: 0.3600\n",
      "Progress: 420000,  val accuracy: 0.3800\n",
      "Progress: 430000,  val accuracy: 0.2900\n",
      "Progress: 440000,  val accuracy: 0.3000\n",
      "Progress: 450000,  val accuracy: 0.3100\n",
      "Epoch 9\n",
      "Progress: 460000,  val accuracy: 0.3600\n",
      "Progress: 470000,  val accuracy: 0.3700\n",
      "Progress: 480000,  val accuracy: 0.3900\n",
      "Progress: 490000,  val accuracy: 0.3700\n",
      "Progress: 500000,  val accuracy: 0.4600\n",
      "Epoch 10\n",
      "Progress: 510000,  val accuracy: 0.4100\n",
      "Progress: 520000,  val accuracy: 0.2400\n",
      "Progress: 530000,  val accuracy: 0.2500\n",
      "Progress: 540000,  val accuracy: 0.3300\n",
      "Progress: 550000,  val accuracy: 0.4800\n",
      "Epoch 11\n",
      "Progress: 560000,  val accuracy: 0.4600\n",
      "Progress: 570000,  val accuracy: 0.4300\n",
      "Progress: 580000,  val accuracy: 0.3400\n",
      "Progress: 590000,  val accuracy: 0.4100\n",
      "Progress: 600000,  val accuracy: 0.5100\n",
      "Epoch 12\n",
      "Progress: 610000,  val accuracy: 0.4000\n",
      "Progress: 620000,  val accuracy: 0.3400\n",
      "Progress: 630000,  val accuracy: 0.3800\n",
      "Progress: 640000,  val accuracy: 0.4100\n",
      "Progress: 650000,  val accuracy: 0.4200\n",
      "Epoch 13\n",
      "Progress: 660000,  val accuracy: 0.4400\n",
      "Progress: 670000,  val accuracy: 0.4000\n",
      "Progress: 680000,  val accuracy: 0.4400\n",
      "Progress: 690000,  val accuracy: 0.4800\n",
      "Progress: 700000,  val accuracy: 0.4500\n",
      "Epoch 14\n",
      "Progress: 710000,  val accuracy: 0.5100\n",
      "Progress: 720000,  val accuracy: 0.4900\n",
      "Progress: 730000,  val accuracy: 0.5400\n",
      "Progress: 740000,  val accuracy: 0.4700\n",
      "Progress: 750000,  val accuracy: 0.5500\n",
      "Epoch 15\n",
      "Total training time1411.162077665329\n",
      "After fit function:  8:29:22.734642\n",
      "Test accuracy of weak learner:  0.5643\n",
      "After allindices:  8:40:25.193653\n",
      "Predictions:  [4. 9. 7. 4. 9. 1. 4. 4. 0. 3.]\n",
      "Alpha:  0.045173829935313554\n",
      "before pessimistic update:  8:40:25.196022\n",
      "correct Indices Mask:  [False  True False ...  True  True False]\n",
      "incorrect Indices Mask:  [ True False  True ... False False  True]\n",
      "after pessimistic update:  8:40:25.202578\n",
      "t:  14 memory allocated: 141873664\n",
      "After WL  14  time elapsed(s):  31228\n",
      "weights: [0.3465735902799726, 0.2141886394353441, 0.17923944868862554, 0.16705332648219723, 0.14276824944769614, 0.10931882825271347, 0.10888713184224057, 0.08572364172565784, 0.09824763542581066, 0.07605269180803588, 0.06724847176347157, 0.06434688327509575, 0.05809093355408436, 0.06696606802846267, 0.045173829935313554]\n",
      "Finished in 31236.251298  s\n",
      "number of wl in ensemble: 15\n"
     ]
    }
   ],
   "source": [
    "# from Boosting import SchapireWongMulticlassBoosting\n",
    "from AdversarialAttacks import attack_fgsm, attack_pgd\n",
    "import gc\n",
    "# Ensemble.record_accuracies, Ensemble.calc_accuracies, attack_fgsm, Ensemble.schapireContinuousPredict\n",
    "for i in range(len(maxSamples_vals)):\n",
    "    print(\"len(maxSamples_vals):\", len(maxSamples_vals))\n",
    "    maxSamples = maxSamples_vals[i]\n",
    "    print(\"maxSamples:\", maxSamples)\n",
    "    ensemble = runBoosting(num_wl, maxSamples, dataset=datasets.CIFAR10, weakLearnerType = WongNeuralNetCIFAR10, val_attacks = [], \n",
    "                               attack_eps_nn=epsilons, attack_eps_ensemble=epsilons, train_eps_nn=train_eps_nn, adv_train_prefix=num_wl, batch_size=batch_size, val_flag=False)\n",
    "    print(\"number of wl in ensemble:\", len(ensemble.weakLearners))\n",
    "    ensembles.append(ensemble)\n",
    "    gc.collect()\n",
    "#     for obj in gc.get_objects():\n",
    "#         try:\n",
    "#             if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n",
    "#                 print(type(obj), obj.size())\n",
    "#         except:\n",
    "#             pass\n",
    "    \n",
    "#     path_head = f'results/plots/cifar10/train_eps_{train_eps_nn}/'\n",
    "#     try:\n",
    "#         os.mkdir(path_head)\n",
    "#     except OSError:\n",
    "#         pass\n",
    "#     acc_file = path_head + f'acc_maxSamples_{maxSamples}.png'\n",
    "#     adv_acc_file = path_head + f'adv_acc_maxSamples_{maxSamples}.png'\n",
    "#     loss_file = path_head + f'loss_maxSamples_{maxSamples}.png'\n",
    "#     wl_train_acc_file = path_head + f'wl_train_acc_maxSamples_{maxSamples}.png'\n",
    "#     ensemble.plot_accuracies(acc_file)\n",
    "#     ensemble.plot_loss(loss_file)\n",
    "#     ensemble.plot_adversarial_accuracies(adv_acc_file)\n",
    "#     ensemble.plot_wl_acc(wl_train_acc_file)\n",
    "#0.3445499534706232, 0.35256058576867955, 0.3492509840145725"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Weak Learner  0 .  Time Elapsed (s):  0\n",
      "{'val': 0.73, <function attack_pgd at 0x7f5f0d4b5400>: [0.5]}\n",
      "{'val': 0.67, <function attack_pgd at 0x7f5f0d4b5400>: [0.42]}\n",
      "{'val': 0.67, <function attack_pgd at 0x7f5f0d4b5400>: [0.44]}\n",
      "{'val': 0.66, <function attack_pgd at 0x7f5f0d4b5400>: [0.38]}\n",
      "{'val': 0.66, <function attack_pgd at 0x7f5f0d4b5400>: [0.39]}\n",
      "{'val': 0.69, <function attack_pgd at 0x7f5f0d4b5400>: [0.45]}\n",
      "{'val': 0.63, <function attack_pgd at 0x7f5f0d4b5400>: [0.43]}\n",
      "{'val': 0.63, <function attack_pgd at 0x7f5f0d4b5400>: [0.45]}\n",
      "{'val': 0.68, <function attack_pgd at 0x7f5f0d4b5400>: [0.47]}\n",
      "{'val': 0.65, <function attack_pgd at 0x7f5f0d4b5400>: [0.37]}\n",
      "{'val': 0.69, <function attack_pgd at 0x7f5f0d4b5400>: [0.5]}\n",
      "{'val': 0.73, <function attack_pgd at 0x7f5f0d4b5400>: [0.47]}\n",
      "{'val': 0.62, <function attack_pgd at 0x7f5f0d4b5400>: [0.4]}\n",
      "{'val': 0.64, <function attack_pgd at 0x7f5f0d4b5400>: [0.39]}\n",
      "ensemble accuracies: {'train': [0.67], 'val': [0.6678571428571428], 'attack_fgsm': [], 'attack_pgd': [[0.4328571428571429]], 'wl_train': [], 'wl_val': []}\n",
      "Weak Learner  1 .  Time Elapsed (s):  70\n",
      "{'val': 0.68, <function attack_pgd at 0x7f5f0d4b5400>: [0.44]}\n",
      "{'val': 0.65, <function attack_pgd at 0x7f5f0d4b5400>: [0.34]}\n",
      "{'val': 0.67, <function attack_pgd at 0x7f5f0d4b5400>: [0.41]}\n",
      "{'val': 0.8, <function attack_pgd at 0x7f5f0d4b5400>: [0.48]}\n",
      "{'val': 0.69, <function attack_pgd at 0x7f5f0d4b5400>: [0.44]}\n",
      "{'val': 0.7, <function attack_pgd at 0x7f5f0d4b5400>: [0.35]}\n",
      "{'val': 0.74, <function attack_pgd at 0x7f5f0d4b5400>: [0.55]}\n",
      "{'val': 0.71, <function attack_pgd at 0x7f5f0d4b5400>: [0.43]}\n",
      "{'val': 0.71, <function attack_pgd at 0x7f5f0d4b5400>: [0.38]}\n",
      "{'val': 0.72, <function attack_pgd at 0x7f5f0d4b5400>: [0.4]}\n",
      "{'val': 0.71, <function attack_pgd at 0x7f5f0d4b5400>: [0.43]}\n",
      "{'val': 0.75, <function attack_pgd at 0x7f5f0d4b5400>: [0.49]}\n",
      "{'val': 0.67, <function attack_pgd at 0x7f5f0d4b5400>: [0.47]}\n",
      "{'val': 0.75, <function attack_pgd at 0x7f5f0d4b5400>: [0.43]}\n",
      "ensemble accuracies: {'train': [0.67, 0.79], 'val': [0.6678571428571428, 0.7107142857142856], 'attack_fgsm': [], 'attack_pgd': [[0.4328571428571429, 0.4314285714285714]], 'wl_train': [], 'wl_val': []}\n",
      "Weak Learner  2 .  Time Elapsed (s):  204\n",
      "{'val': 0.75, <function attack_pgd at 0x7f5f0d4b5400>: [0.44]}\n",
      "{'val': 0.69, <function attack_pgd at 0x7f5f0d4b5400>: [0.39]}\n",
      "{'val': 0.77, <function attack_pgd at 0x7f5f0d4b5400>: [0.43]}\n",
      "{'val': 0.69, <function attack_pgd at 0x7f5f0d4b5400>: [0.43]}\n",
      "{'val': 0.72, <function attack_pgd at 0x7f5f0d4b5400>: [0.51]}\n",
      "{'val': 0.75, <function attack_pgd at 0x7f5f0d4b5400>: [0.45]}\n",
      "{'val': 0.74, <function attack_pgd at 0x7f5f0d4b5400>: [0.49]}\n",
      "{'val': 0.69, <function attack_pgd at 0x7f5f0d4b5400>: [0.42]}\n",
      "{'val': 0.71, <function attack_pgd at 0x7f5f0d4b5400>: [0.39]}\n",
      "{'val': 0.66, <function attack_pgd at 0x7f5f0d4b5400>: [0.36]}\n",
      "{'val': 0.71, <function attack_pgd at 0x7f5f0d4b5400>: [0.47]}\n",
      "{'val': 0.66, <function attack_pgd at 0x7f5f0d4b5400>: [0.42]}\n",
      "{'val': 0.74, <function attack_pgd at 0x7f5f0d4b5400>: [0.45]}\n",
      "{'val': 0.78, <function attack_pgd at 0x7f5f0d4b5400>: [0.48]}\n",
      "ensemble accuracies: {'train': [0.67, 0.79, 0.84], 'val': [0.6678571428571428, 0.7107142857142856, 0.7185714285714286], 'attack_fgsm': [], 'attack_pgd': [[0.4328571428571429, 0.4314285714285714, 0.4378571428571429]], 'wl_train': [], 'wl_val': []}\n",
      "Weak Learner  3 .  Time Elapsed (s):  402\n",
      "{'val': 0.72, <function attack_pgd at 0x7f5f0d4b5400>: [0.4]}\n",
      "{'val': 0.76, <function attack_pgd at 0x7f5f0d4b5400>: [0.46]}\n",
      "{'val': 0.79, <function attack_pgd at 0x7f5f0d4b5400>: [0.47]}\n",
      "{'val': 0.78, <function attack_pgd at 0x7f5f0d4b5400>: [0.48]}\n",
      "{'val': 0.83, <function attack_pgd at 0x7f5f0d4b5400>: [0.54]}\n",
      "{'val': 0.66, <function attack_pgd at 0x7f5f0d4b5400>: [0.4]}\n",
      "{'val': 0.74, <function attack_pgd at 0x7f5f0d4b5400>: [0.42]}\n",
      "{'val': 0.7, <function attack_pgd at 0x7f5f0d4b5400>: [0.44]}\n",
      "{'val': 0.74, <function attack_pgd at 0x7f5f0d4b5400>: [0.5]}\n",
      "{'val': 0.67, <function attack_pgd at 0x7f5f0d4b5400>: [0.44]}\n",
      "{'val': 0.71, <function attack_pgd at 0x7f5f0d4b5400>: [0.43]}\n",
      "{'val': 0.63, <function attack_pgd at 0x7f5f0d4b5400>: [0.39]}\n",
      "{'val': 0.69, <function attack_pgd at 0x7f5f0d4b5400>: [0.46]}\n",
      "{'val': 0.7, <function attack_pgd at 0x7f5f0d4b5400>: [0.44]}\n",
      "ensemble accuracies: {'train': [0.67, 0.79, 0.84, 0.76], 'val': [0.6678571428571428, 0.7107142857142856, 0.7185714285714286, 0.722857142857143], 'attack_fgsm': [], 'attack_pgd': [[0.4328571428571429, 0.4314285714285714, 0.4378571428571429, 0.44785714285714284]], 'wl_train': [], 'wl_val': []}\n",
      "Weak Learner  4 .  Time Elapsed (s):  665\n",
      "{'val': 0.75, <function attack_pgd at 0x7f5f0d4b5400>: [0.44]}\n",
      "{'val': 0.77, <function attack_pgd at 0x7f5f0d4b5400>: [0.52]}\n",
      "{'val': 0.74, <function attack_pgd at 0x7f5f0d4b5400>: [0.54]}\n",
      "{'val': 0.76, <function attack_pgd at 0x7f5f0d4b5400>: [0.35]}\n",
      "{'val': 0.74, <function attack_pgd at 0x7f5f0d4b5400>: [0.49]}\n",
      "{'val': 0.72, <function attack_pgd at 0x7f5f0d4b5400>: [0.47]}\n",
      "{'val': 0.68, <function attack_pgd at 0x7f5f0d4b5400>: [0.4]}\n",
      "{'val': 0.71, <function attack_pgd at 0x7f5f0d4b5400>: [0.44]}\n",
      "{'val': 0.74, <function attack_pgd at 0x7f5f0d4b5400>: [0.49]}\n",
      "{'val': 0.71, <function attack_pgd at 0x7f5f0d4b5400>: [0.49]}\n",
      "{'val': 0.62, <function attack_pgd at 0x7f5f0d4b5400>: [0.38]}\n",
      "{'val': 0.82, <function attack_pgd at 0x7f5f0d4b5400>: [0.49]}\n",
      "{'val': 0.74, <function attack_pgd at 0x7f5f0d4b5400>: [0.41]}\n",
      "{'val': 0.81, <function attack_pgd at 0x7f5f0d4b5400>: [0.43]}\n",
      "ensemble accuracies: {'train': [0.67, 0.79, 0.84, 0.76, 0.84], 'val': [0.6678571428571428, 0.7107142857142856, 0.7185714285714286, 0.722857142857143, 0.7364285714285714], 'attack_fgsm': [], 'attack_pgd': [[0.4328571428571429, 0.4314285714285714, 0.4378571428571429, 0.44785714285714284, 0.45285714285714285]], 'wl_train': [], 'wl_val': []}\n",
      "Weak Learner  5 .  Time Elapsed (s):  992\n",
      "{'val': 0.68, <function attack_pgd at 0x7f5f0d4b5400>: [0.43]}\n",
      "{'val': 0.76, <function attack_pgd at 0x7f5f0d4b5400>: [0.45]}\n",
      "{'val': 0.75, <function attack_pgd at 0x7f5f0d4b5400>: [0.42]}\n",
      "{'val': 0.85, <function attack_pgd at 0x7f5f0d4b5400>: [0.55]}\n",
      "{'val': 0.74, <function attack_pgd at 0x7f5f0d4b5400>: [0.46]}\n",
      "{'val': 0.81, <function attack_pgd at 0x7f5f0d4b5400>: [0.46]}\n",
      "{'val': 0.72, <function attack_pgd at 0x7f5f0d4b5400>: [0.49]}\n",
      "{'val': 0.72, <function attack_pgd at 0x7f5f0d4b5400>: [0.5]}\n",
      "{'val': 0.78, <function attack_pgd at 0x7f5f0d4b5400>: [0.53]}\n",
      "{'val': 0.79, <function attack_pgd at 0x7f5f0d4b5400>: [0.47]}\n",
      "{'val': 0.76, <function attack_pgd at 0x7f5f0d4b5400>: [0.4]}\n",
      "{'val': 0.78, <function attack_pgd at 0x7f5f0d4b5400>: [0.48]}\n",
      "{'val': 0.77, <function attack_pgd at 0x7f5f0d4b5400>: [0.37]}\n",
      "{'val': 0.75, <function attack_pgd at 0x7f5f0d4b5400>: [0.38]}\n",
      "ensemble accuracies: {'train': [0.67, 0.79, 0.84, 0.76, 0.84, 0.77], 'val': [0.6678571428571428, 0.7107142857142856, 0.7185714285714286, 0.722857142857143, 0.7364285714285714, 0.7614285714285713], 'attack_fgsm': [], 'attack_pgd': [[0.4328571428571429, 0.4314285714285714, 0.4378571428571429, 0.44785714285714284, 0.45285714285714285, 0.45642857142857146]], 'wl_train': [], 'wl_val': []}\n",
      "Weak Learner  6 .  Time Elapsed (s):  1384\n",
      "{'val': 0.64, <function attack_pgd at 0x7f5f0d4b5400>: [0.37]}\n",
      "{'val': 0.76, <function attack_pgd at 0x7f5f0d4b5400>: [0.52]}\n",
      "{'val': 0.76, <function attack_pgd at 0x7f5f0d4b5400>: [0.41]}\n",
      "{'val': 0.72, <function attack_pgd at 0x7f5f0d4b5400>: [0.37]}\n",
      "{'val': 0.63, <function attack_pgd at 0x7f5f0d4b5400>: [0.33]}\n",
      "{'val': 0.79, <function attack_pgd at 0x7f5f0d4b5400>: [0.43]}\n",
      "{'val': 0.74, <function attack_pgd at 0x7f5f0d4b5400>: [0.4]}\n",
      "{'val': 0.75, <function attack_pgd at 0x7f5f0d4b5400>: [0.39]}\n",
      "{'val': 0.77, <function attack_pgd at 0x7f5f0d4b5400>: [0.46]}\n",
      "{'val': 0.8, <function attack_pgd at 0x7f5f0d4b5400>: [0.52]}\n",
      "{'val': 0.74, <function attack_pgd at 0x7f5f0d4b5400>: [0.37]}\n",
      "{'val': 0.78, <function attack_pgd at 0x7f5f0d4b5400>: [0.48]}\n",
      "{'val': 0.7, <function attack_pgd at 0x7f5f0d4b5400>: [0.34]}\n",
      "{'val': 0.69, <function attack_pgd at 0x7f5f0d4b5400>: [0.42]}\n",
      "ensemble accuracies: {'train': [0.67, 0.79, 0.84, 0.76, 0.84, 0.77, 0.79], 'val': [0.6678571428571428, 0.7107142857142856, 0.7185714285714286, 0.722857142857143, 0.7364285714285714, 0.7614285714285713, 0.7335714285714284], 'attack_fgsm': [], 'attack_pgd': [[0.4328571428571429, 0.4314285714285714, 0.4378571428571429, 0.44785714285714284, 0.45285714285714285, 0.45642857142857146, 0.41500000000000004]], 'wl_train': [], 'wl_val': []}\n",
      "Weak Learner  7 .  Time Elapsed (s):  1840\n",
      "{'val': 0.74, <function attack_pgd at 0x7f5f0d4b5400>: [0.43]}\n",
      "{'val': 0.72, <function attack_pgd at 0x7f5f0d4b5400>: [0.4]}\n",
      "{'val': 0.74, <function attack_pgd at 0x7f5f0d4b5400>: [0.45]}\n",
      "{'val': 0.72, <function attack_pgd at 0x7f5f0d4b5400>: [0.44]}\n",
      "{'val': 0.74, <function attack_pgd at 0x7f5f0d4b5400>: [0.49]}\n",
      "{'val': 0.75, <function attack_pgd at 0x7f5f0d4b5400>: [0.44]}\n",
      "{'val': 0.74, <function attack_pgd at 0x7f5f0d4b5400>: [0.39]}\n",
      "{'val': 0.72, <function attack_pgd at 0x7f5f0d4b5400>: [0.43]}\n",
      "{'val': 0.76, <function attack_pgd at 0x7f5f0d4b5400>: [0.45]}\n",
      "{'val': 0.72, <function attack_pgd at 0x7f5f0d4b5400>: [0.36]}\n",
      "{'val': 0.71, <function attack_pgd at 0x7f5f0d4b5400>: [0.44]}\n",
      "{'val': 0.73, <function attack_pgd at 0x7f5f0d4b5400>: [0.43]}\n",
      "{'val': 0.82, <function attack_pgd at 0x7f5f0d4b5400>: [0.54]}\n",
      "{'val': 0.69, <function attack_pgd at 0x7f5f0d4b5400>: [0.38]}\n",
      "ensemble accuracies: {'train': [0.67, 0.79, 0.84, 0.76, 0.84, 0.77, 0.79, 0.77], 'val': [0.6678571428571428, 0.7107142857142856, 0.7185714285714286, 0.722857142857143, 0.7364285714285714, 0.7614285714285713, 0.7335714285714284, 0.7357142857142857], 'attack_fgsm': [], 'attack_pgd': [[0.4328571428571429, 0.4314285714285714, 0.4378571428571429, 0.44785714285714284, 0.45285714285714285, 0.45642857142857146, 0.41500000000000004, 0.4335714285714286]], 'wl_train': [], 'wl_val': []}\n",
      "Weak Learner  8 .  Time Elapsed (s):  2359\n",
      "{'val': 0.7, <function attack_pgd at 0x7f5f0d4b5400>: [0.35]}\n",
      "{'val': 0.71, <function attack_pgd at 0x7f5f0d4b5400>: [0.36]}\n",
      "{'val': 0.75, <function attack_pgd at 0x7f5f0d4b5400>: [0.45]}\n",
      "{'val': 0.75, <function attack_pgd at 0x7f5f0d4b5400>: [0.44]}\n",
      "{'val': 0.74, <function attack_pgd at 0x7f5f0d4b5400>: [0.49]}\n",
      "{'val': 0.72, <function attack_pgd at 0x7f5f0d4b5400>: [0.43]}\n",
      "{'val': 0.76, <function attack_pgd at 0x7f5f0d4b5400>: [0.42]}\n",
      "{'val': 0.82, <function attack_pgd at 0x7f5f0d4b5400>: [0.49]}\n",
      "{'val': 0.78, <function attack_pgd at 0x7f5f0d4b5400>: [0.43]}\n",
      "{'val': 0.74, <function attack_pgd at 0x7f5f0d4b5400>: [0.47]}\n",
      "{'val': 0.73, <function attack_pgd at 0x7f5f0d4b5400>: [0.45]}\n",
      "{'val': 0.69, <function attack_pgd at 0x7f5f0d4b5400>: [0.43]}\n",
      "{'val': 0.68, <function attack_pgd at 0x7f5f0d4b5400>: [0.33]}\n",
      "{'val': 0.75, <function attack_pgd at 0x7f5f0d4b5400>: [0.35]}\n",
      "ensemble accuracies: {'train': [0.67, 0.79, 0.84, 0.76, 0.84, 0.77, 0.79, 0.77, 0.74], 'val': [0.6678571428571428, 0.7107142857142856, 0.7185714285714286, 0.722857142857143, 0.7364285714285714, 0.7614285714285713, 0.7335714285714284, 0.7357142857142857, 0.7371428571428572], 'attack_fgsm': [], 'attack_pgd': [[0.4328571428571429, 0.4314285714285714, 0.4378571428571429, 0.44785714285714284, 0.45285714285714285, 0.45642857142857146, 0.41500000000000004, 0.4335714285714286, 0.4207142857142857]], 'wl_train': [], 'wl_val': []}\n",
      "Weak Learner  9 .  Time Elapsed (s):  2945\n",
      "{'val': 0.74, <function attack_pgd at 0x7f5f0d4b5400>: [0.38]}\n",
      "{'val': 0.7, <function attack_pgd at 0x7f5f0d4b5400>: [0.37]}\n",
      "{'val': 0.8, <function attack_pgd at 0x7f5f0d4b5400>: [0.47]}\n",
      "{'val': 0.72, <function attack_pgd at 0x7f5f0d4b5400>: [0.46]}\n",
      "{'val': 0.76, <function attack_pgd at 0x7f5f0d4b5400>: [0.51]}\n",
      "{'val': 0.79, <function attack_pgd at 0x7f5f0d4b5400>: [0.37]}\n",
      "{'val': 0.76, <function attack_pgd at 0x7f5f0d4b5400>: [0.37]}\n",
      "{'val': 0.69, <function attack_pgd at 0x7f5f0d4b5400>: [0.34]}\n",
      "{'val': 0.77, <function attack_pgd at 0x7f5f0d4b5400>: [0.44]}\n",
      "{'val': 0.71, <function attack_pgd at 0x7f5f0d4b5400>: [0.4]}\n",
      "{'val': 0.77, <function attack_pgd at 0x7f5f0d4b5400>: [0.49]}\n",
      "{'val': 0.73, <function attack_pgd at 0x7f5f0d4b5400>: [0.48]}\n",
      "{'val': 0.74, <function attack_pgd at 0x7f5f0d4b5400>: [0.44]}\n",
      "{'val': 0.71, <function attack_pgd at 0x7f5f0d4b5400>: [0.42]}\n",
      "ensemble accuracies: {'train': [0.67, 0.79, 0.84, 0.76, 0.84, 0.77, 0.79, 0.77, 0.74, 0.75], 'val': [0.6678571428571428, 0.7107142857142856, 0.7185714285714286, 0.722857142857143, 0.7364285714285714, 0.7614285714285713, 0.7335714285714284, 0.7357142857142857, 0.7371428571428572, 0.7421428571428572], 'attack_fgsm': [], 'attack_pgd': [[0.4328571428571429, 0.4314285714285714, 0.4378571428571429, 0.44785714285714284, 0.45285714285714285, 0.45642857142857146, 0.41500000000000004, 0.4335714285714286, 0.4207142857142857, 0.4242857142857143]], 'wl_train': [], 'wl_val': []}\n",
      "Weak Learner  10 .  Time Elapsed (s):  3598\n",
      "{'val': 0.8, <function attack_pgd at 0x7f5f0d4b5400>: [0.44]}\n",
      "{'val': 0.8, <function attack_pgd at 0x7f5f0d4b5400>: [0.41]}\n",
      "{'val': 0.75, <function attack_pgd at 0x7f5f0d4b5400>: [0.4]}\n",
      "{'val': 0.8, <function attack_pgd at 0x7f5f0d4b5400>: [0.44]}\n",
      "{'val': 0.77, <function attack_pgd at 0x7f5f0d4b5400>: [0.4]}\n",
      "{'val': 0.79, <function attack_pgd at 0x7f5f0d4b5400>: [0.42]}\n",
      "{'val': 0.68, <function attack_pgd at 0x7f5f0d4b5400>: [0.43]}\n",
      "{'val': 0.7, <function attack_pgd at 0x7f5f0d4b5400>: [0.36]}\n",
      "{'val': 0.74, <function attack_pgd at 0x7f5f0d4b5400>: [0.45]}\n",
      "{'val': 0.73, <function attack_pgd at 0x7f5f0d4b5400>: [0.38]}\n",
      "{'val': 0.71, <function attack_pgd at 0x7f5f0d4b5400>: [0.47]}\n",
      "{'val': 0.68, <function attack_pgd at 0x7f5f0d4b5400>: [0.38]}\n",
      "ensemble accuracies: {'train': [0.67, 0.79, 0.84, 0.76, 0.84, 0.77, 0.79, 0.77, 0.74, 0.75, 0.79], 'val': [0.6678571428571428, 0.7107142857142856, 0.7185714285714286, 0.722857142857143, 0.7364285714285714, 0.7614285714285713, 0.7335714285714284, 0.7357142857142857, 0.7371428571428572, 0.7421428571428572, 0.7435714285714285], 'attack_fgsm': [], 'attack_pgd': [[0.4328571428571429, 0.4314285714285714, 0.4378571428571429, 0.44785714285714284, 0.45285714285714285, 0.45642857142857146, 0.41500000000000004, 0.4335714285714286, 0.4207142857142857, 0.4242857142857143, 0.41428571428571426]], 'wl_train': [], 'wl_val': []}\n",
      "Weak Learner  11 .  Time Elapsed (s):  4316\n",
      "{'val': 0.68, <function attack_pgd at 0x7f5f0d4b5400>: [0.41]}\n",
      "{'val': 0.7, <function attack_pgd at 0x7f5f0d4b5400>: [0.43]}\n",
      "{'val': 0.77, <function attack_pgd at 0x7f5f0d4b5400>: [0.45]}\n",
      "{'val': 0.77, <function attack_pgd at 0x7f5f0d4b5400>: [0.46]}\n",
      "{'val': 0.79, <function attack_pgd at 0x7f5f0d4b5400>: [0.54]}\n",
      "{'val': 0.78, <function attack_pgd at 0x7f5f0d4b5400>: [0.36]}\n",
      "{'val': 0.78, <function attack_pgd at 0x7f5f0d4b5400>: [0.46]}\n",
      "{'val': 0.77, <function attack_pgd at 0x7f5f0d4b5400>: [0.48]}\n",
      "{'val': 0.76, <function attack_pgd at 0x7f5f0d4b5400>: [0.47]}\n",
      "{'val': 0.75, <function attack_pgd at 0x7f5f0d4b5400>: [0.44]}\n",
      "{'val': 0.79, <function attack_pgd at 0x7f5f0d4b5400>: [0.45]}\n",
      "{'val': 0.76, <function attack_pgd at 0x7f5f0d4b5400>: [0.42]}\n",
      "{'val': 0.75, <function attack_pgd at 0x7f5f0d4b5400>: [0.44]}\n",
      "{'val': 0.79, <function attack_pgd at 0x7f5f0d4b5400>: [0.38]}\n",
      "ensemble accuracies: {'train': [0.67, 0.79, 0.84, 0.76, 0.84, 0.77, 0.79, 0.77, 0.74, 0.75, 0.79, 0.83], 'val': [0.6678571428571428, 0.7107142857142856, 0.7185714285714286, 0.722857142857143, 0.7364285714285714, 0.7614285714285713, 0.7335714285714284, 0.7357142857142857, 0.7371428571428572, 0.7421428571428572, 0.7435714285714285, 0.76], 'attack_fgsm': [], 'attack_pgd': [[0.4328571428571429, 0.4314285714285714, 0.4378571428571429, 0.44785714285714284, 0.45285714285714285, 0.45642857142857146, 0.41500000000000004, 0.4335714285714286, 0.4207142857142857, 0.4242857142857143, 0.41428571428571426, 0.44214285714285717]], 'wl_train': [], 'wl_val': []}\n",
      "Weak Learner  12 .  Time Elapsed (s):  5099\n",
      "{'val': 0.81, <function attack_pgd at 0x7f5f0d4b5400>: [0.39]}\n",
      "{'val': 0.79, <function attack_pgd at 0x7f5f0d4b5400>: [0.48]}\n",
      "{'val': 0.74, <function attack_pgd at 0x7f5f0d4b5400>: [0.37]}\n",
      "{'val': 0.73, <function attack_pgd at 0x7f5f0d4b5400>: [0.4]}\n",
      "{'val': 0.71, <function attack_pgd at 0x7f5f0d4b5400>: [0.34]}\n",
      "{'val': 0.65, <function attack_pgd at 0x7f5f0d4b5400>: [0.37]}\n",
      "{'val': 0.75, <function attack_pgd at 0x7f5f0d4b5400>: [0.41]}\n",
      "{'val': 0.81, <function attack_pgd at 0x7f5f0d4b5400>: [0.49]}\n",
      "{'val': 0.73, <function attack_pgd at 0x7f5f0d4b5400>: [0.43]}\n",
      "{'val': 0.71, <function attack_pgd at 0x7f5f0d4b5400>: [0.38]}\n",
      "{'val': 0.78, <function attack_pgd at 0x7f5f0d4b5400>: [0.46]}\n",
      "{'val': 0.79, <function attack_pgd at 0x7f5f0d4b5400>: [0.43]}\n",
      "{'val': 0.74, <function attack_pgd at 0x7f5f0d4b5400>: [0.34]}\n",
      "{'val': 0.65, <function attack_pgd at 0x7f5f0d4b5400>: [0.31]}\n",
      "ensemble accuracies: {'train': [0.67, 0.79, 0.84, 0.76, 0.84, 0.77, 0.79, 0.77, 0.74, 0.75, 0.79, 0.83, 0.8], 'val': [0.6678571428571428, 0.7107142857142856, 0.7185714285714286, 0.722857142857143, 0.7364285714285714, 0.7614285714285713, 0.7335714285714284, 0.7357142857142857, 0.7371428571428572, 0.7421428571428572, 0.7435714285714285, 0.76, 0.7421428571428572], 'attack_fgsm': [], 'attack_pgd': [[0.4328571428571429, 0.4314285714285714, 0.4378571428571429, 0.44785714285714284, 0.45285714285714285, 0.45642857142857146, 0.41500000000000004, 0.4335714285714286, 0.4207142857142857, 0.4242857142857143, 0.41428571428571426, 0.44214285714285717, 0.39999999999999997]], 'wl_train': [], 'wl_val': []}\n",
      "Weak Learner  13 .  Time Elapsed (s):  5943\n",
      "{'val': 0.69, <function attack_pgd at 0x7f5f0d4b5400>: [0.41]}\n",
      "{'val': 0.73, <function attack_pgd at 0x7f5f0d4b5400>: [0.33]}\n",
      "{'val': 0.73, <function attack_pgd at 0x7f5f0d4b5400>: [0.37]}\n",
      "{'val': 0.77, <function attack_pgd at 0x7f5f0d4b5400>: [0.37]}\n",
      "{'val': 0.78, <function attack_pgd at 0x7f5f0d4b5400>: [0.42]}\n",
      "{'val': 0.71, <function attack_pgd at 0x7f5f0d4b5400>: [0.4]}\n",
      "{'val': 0.7, <function attack_pgd at 0x7f5f0d4b5400>: [0.38]}\n",
      "{'val': 0.67, <function attack_pgd at 0x7f5f0d4b5400>: [0.37]}\n",
      "{'val': 0.68, <function attack_pgd at 0x7f5f0d4b5400>: [0.43]}\n",
      "{'val': 0.75, <function attack_pgd at 0x7f5f0d4b5400>: [0.48]}\n",
      "{'val': 0.77, <function attack_pgd at 0x7f5f0d4b5400>: [0.42]}\n",
      "{'val': 0.72, <function attack_pgd at 0x7f5f0d4b5400>: [0.36]}\n",
      "{'val': 0.82, <function attack_pgd at 0x7f5f0d4b5400>: [0.46]}\n",
      "{'val': 0.79, <function attack_pgd at 0x7f5f0d4b5400>: [0.4]}\n",
      "ensemble accuracies: {'train': [0.67, 0.79, 0.84, 0.76, 0.84, 0.77, 0.79, 0.77, 0.74, 0.75, 0.79, 0.83, 0.8, 0.79], 'val': [0.6678571428571428, 0.7107142857142856, 0.7185714285714286, 0.722857142857143, 0.7364285714285714, 0.7614285714285713, 0.7335714285714284, 0.7357142857142857, 0.7371428571428572, 0.7421428571428572, 0.7435714285714285, 0.76, 0.7421428571428572, 0.7364285714285715], 'attack_fgsm': [], 'attack_pgd': [[0.4328571428571429, 0.4314285714285714, 0.4378571428571429, 0.44785714285714284, 0.45285714285714285, 0.45642857142857146, 0.41500000000000004, 0.4335714285714286, 0.4207142857142857, 0.4242857142857143, 0.41428571428571426, 0.44214285714285717, 0.39999999999999997, 0.4]], 'wl_train': [], 'wl_val': []}\n",
      "Weak Learner  14 .  Time Elapsed (s):  6854\n",
      "{'val': 0.67, <function attack_pgd at 0x7f5f0d4b5400>: [0.45]}\n",
      "{'val': 0.71, <function attack_pgd at 0x7f5f0d4b5400>: [0.42]}\n",
      "{'val': 0.71, <function attack_pgd at 0x7f5f0d4b5400>: [0.4]}\n",
      "{'val': 0.68, <function attack_pgd at 0x7f5f0d4b5400>: [0.39]}\n",
      "{'val': 0.66, <function attack_pgd at 0x7f5f0d4b5400>: [0.37]}\n",
      "{'val': 0.76, <function attack_pgd at 0x7f5f0d4b5400>: [0.43]}\n",
      "{'val': 0.8, <function attack_pgd at 0x7f5f0d4b5400>: [0.44]}\n",
      "{'val': 0.71, <function attack_pgd at 0x7f5f0d4b5400>: [0.4]}\n",
      "{'val': 0.67, <function attack_pgd at 0x7f5f0d4b5400>: [0.35]}\n",
      "{'val': 0.72, <function attack_pgd at 0x7f5f0d4b5400>: [0.41]}\n",
      "{'val': 0.78, <function attack_pgd at 0x7f5f0d4b5400>: [0.45]}\n",
      "{'val': 0.79, <function attack_pgd at 0x7f5f0d4b5400>: [0.43]}\n",
      "{'val': 0.75, <function attack_pgd at 0x7f5f0d4b5400>: [0.44]}\n",
      "{'val': 0.73, <function attack_pgd at 0x7f5f0d4b5400>: [0.38]}\n",
      "ensemble accuracies: {'train': [0.67, 0.79, 0.84, 0.76, 0.84, 0.77, 0.79, 0.77, 0.74, 0.75, 0.79, 0.83, 0.8, 0.79, 0.78], 'val': [0.6678571428571428, 0.7107142857142856, 0.7185714285714286, 0.722857142857143, 0.7364285714285714, 0.7614285714285713, 0.7335714285714284, 0.7357142857142857, 0.7371428571428572, 0.7421428571428572, 0.7435714285714285, 0.76, 0.7421428571428572, 0.7364285714285715, 0.7242857142857143], 'attack_fgsm': [], 'attack_pgd': [[0.4328571428571429, 0.4314285714285714, 0.4378571428571429, 0.44785714285714284, 0.45285714285714285, 0.45642857142857146, 0.41500000000000004, 0.4335714285714286, 0.4207142857142857, 0.4242857142857143, 0.41428571428571426, 0.44214285714285717, 0.39999999999999997, 0.4, 0.4114285714285715]], 'wl_train': [], 'wl_val': []}\n"
     ]
    }
   ],
   "source": [
    "from Testing import testEnsemble\n",
    "path = f'./models/{maxSamples_vals[0]}Eps{train_eps_nn}/'\n",
    "attack=attack_pgd\n",
    "attackStr=\"attack_pgd\"\n",
    "ensemble = testEnsemble(path, [attack], num_wl, numsamples_train=200, numsamples_val=1500, attack_eps_ensemble=epsilons) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "maxSamples = maxSamples_vals[0]\n",
    "resultsPath = f'results/plots/cifar10/train_eps_{train_eps_nn}/{attackStr}/'\n",
    "acc_file = resultsPath + f'acc_maxSamples_{maxSamples}.png'\n",
    "adv_acc_file = resultsPath + f'adv_acc_maxSamples_{maxSamples}.png'\n",
    "loss_file = resultsPath + f'loss_maxSamples_{maxSamples}.png'\n",
    "# wl_train_acc_file = resultsPath + f'wl_train_acc_maxSamples_{maxSamples}.png'\n",
    "ensemble.plot_accuracies(acc_file)\n",
    "ensemble.plot_loss(loss_file)\n",
    "ensemble.plot_adversarial_accuracies(adv_acc_file)\n",
    "# ensemble.plot_wl_acc(wl_train_acc_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Checking PGD Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestNN = WongNeuralNetCIFAR10(attack_eps=[0.127])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "TestNN.model.load_state_dict(torch.load(\"./models/750004Eps8/wl_0.pth\"))\n",
    "TestNN.model.cuda()\n",
    "TestNN.model.eval()\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2471, 0.2435, 0.2616)),\n",
    "])\n",
    "\n",
    "# Normalize the test set same as training set without augmentation\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2471, 0.2435, 0.2616)),\n",
    "])\n",
    "\n",
    "trainset = datasets.CIFAR10(\n",
    "    root=\"./data\", train=True, download=True, transform=transform_train)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=100, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = datasets.CIFAR10(\n",
    "    root=\"./data\", train=False, download=True, transform=transform_test)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=1000, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check a single WL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.387\n",
      "0.379\n",
      "0.402\n",
      "0.389\n",
      "0.38\n",
      "0.396\n",
      "0.394\n",
      "0.378\n",
      "0.414\n",
      "0.403\n",
      "Overall acc:  0.39220000000000005\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "accs = []\n",
    "for data in test_loader:\n",
    "    X = data[0]\n",
    "    y = data[1]\n",
    "    losses, acc = TestNN.calc_accuracies(X.cuda(), y.cuda(), val_attacks=[attack_pgd], attack_iters=20)\n",
    "    print(acc[attack_pgd][0])\n",
    "    accs.append(acc[attack_pgd][0])\n",
    "    \n",
    "    if i>20:\n",
    "        break\n",
    "    i+=1\n",
    "print(\"Overall acc: \", sum(accs)/len(accs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check for PGD (single weak learner and ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgd_steps = [20, 40, 60, 80, 100]\n",
    "# pgd_steps = [20]\n",
    "attack_eps = [0.03]\n",
    "num_wl = 1\n",
    "from Testing import testEnsemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2471, 0.2435, 0.2616)),\n",
    "])\n",
    "\n",
    "# Normalize the test set same as training set without augmentation\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2471, 0.2435, 0.2616)),\n",
    "])\n",
    "\n",
    "trainset = datasets.CIFAR10(\n",
    "    root=\"./data\", train=True, download=True, transform=transform_train)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=100, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = datasets.CIFAR10(\n",
    "    root=\"./data\", train=False, download=True, transform=transform_test)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=1000, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 126.00 MiB (GPU 0; 15.90 GiB total capacity; 14.80 GiB already allocated; 9.50 MiB free; 15.23 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-6ef666d733a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTestNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc_accuracies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_attacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mattack_pgd\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattack_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattack_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mwl_accuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mattack_pgd\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/storage/AdversarialBoostingNeuralNets/BaseModels.py\u001b[0m in \u001b[0;36mcalc_accuracies\u001b[0;34m(self, X, y, data_type, val_attacks, alpha, attack_iters, restarts, y_pred)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;31m# accuracy on clean examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# did this to debug memory issues (1 / 18)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m             \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/storage/AdversarialBoostingNeuralNets/WeakLearners.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;31m# class FreeNeuralNetCIFAR10(BaseNeuralNet):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/storage/AdversarialBoostingNeuralNets/BaseModels.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/storage/AdversarialBoostingNeuralNets/BaseModels.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mshortcut\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshortcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shortcut'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mshortcut\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             self.weight, self.bias, bn_training, exponential_average_factor, self.eps)\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2056\u001b[0m     return torch.batch_norm(\n\u001b[1;32m   2057\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2058\u001b[0;31m         \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2059\u001b[0m     )\n\u001b[1;32m   2060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 126.00 MiB (GPU 0; 15.90 GiB total capacity; 14.80 GiB already allocated; 9.50 MiB free; 15.23 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "#Test pgd on weak learners\n",
    "wl_accuracies = []\n",
    "for attack_iters in pgd_steps:\n",
    "    TestNN = WongNeuralNetCIFAR10(attack_eps=attack_eps)\n",
    "    TestNN.model.load_state_dict(torch.load(\"./models/750000Eps8/wl_0.pth\"))\n",
    "    TestNN.model.cuda()\n",
    "    TestNN.model.eval()\n",
    "    print(\"\")\n",
    "    \n",
    "    i = 0\n",
    "#     accs = []\n",
    "    for data in test_loader:\n",
    "        X = data[0]\n",
    "        y = data[1]\n",
    "        losses, acc = TestNN.calc_accuracies(X.cuda(), y.cuda(), val_attacks=[attack_pgd], attack_iters=attack_iters)\n",
    "        print(acc)\n",
    "        wl_accuracies.append(acc[attack_pgd])\n",
    "\n",
    "    #     if i>5:\n",
    "        break\n",
    "        i+=1\n",
    "print(\"pgd_steps:\", pgd_steps)\n",
    "print(\"wl_accuracies:\", wl_accuracies)\n",
    "plt.subplots()\n",
    "plt.plot(pgd_steps, wl_accuracies)\n",
    "plt.xlabel('PGD iterations')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Weak learner accuracy vs PGD iterations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'val'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-7c3dce3759fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#         print(\"before ens acc\", ensemble.accuracies)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mensemble\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_accuracies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_attacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mattack_pgd\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattack_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattack_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ensemble accuracies:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensemble\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/storage/AdversarialBoostingNeuralNets/Ensemble.py\u001b[0m in \u001b[0;36mrecord_accuracies\u001b[0;34m(self, progress, train_loader, test_loader, numsamples_train, numsamples_val, val_attacks, attack_iters)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0mval_loss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss_dicts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mval_acc_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_acc_dicts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_acc_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_loss_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'val'"
     ]
    }
   ],
   "source": [
    "# Test pgd on ensemble (1 wl)\n",
    "for attack_iters in pgd_steps:\n",
    "    ensemble = Ensemble(weakLearners=[], weakLearnerWeights=[], weakLearnerType=WongNeuralNetCIFAR10, attack_eps=[0.03])\n",
    "    ensemble.losses[\"val\"]\n",
    "    for i in range(1):\n",
    "        ensemble.addWeakLearner(\"./models/750000Eps8/wl_0.pth\", 1)\n",
    "#         print(\"before ens acc\", ensemble.accuracies)\n",
    "\n",
    "        ensemble.record_accuracies(i, train_loader, test_loader, 1000, 1000, val_attacks=[attack_pgd], attack_iters=attack_iters)\n",
    "        print(\"ensemble accuracies:\", ensemble.accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Weak Learner  0 .  Time Elapsed (s):  0\n",
      "Weak Learner  1 .  Time Elapsed (s):  0\n",
      "Weak Learner  2 .  Time Elapsed (s):  0\n",
      "Weak Learner  3 .  Time Elapsed (s):  0\n",
      "pgd called with 0.127 0.01 20 1\n",
      "{'val': 0.71, <function attack_pgd at 0x7f20b4390400>: [0.39]}\n",
      "pgd called with 0.127 0.01 20 1\n",
      "{'val': 0.73, <function attack_pgd at 0x7f20b4390400>: [0.41]}\n",
      "pgd called with 0.127 0.01 20 1\n",
      "{'val': 0.74, <function attack_pgd at 0x7f20b4390400>: [0.47]}\n",
      "ensemble accuracies: {'train': [0.7466666666666667], 'val': [0.7266666666666666], 'attack_fgsm': [], 'attack_pgd': [[0.42333333333333334]], 'wl_train': [], 'wl_val': []}\n",
      "Weak Learner  4 .  Time Elapsed (s):  60\n",
      "Weak Learner  5 .  Time Elapsed (s):  60\n",
      "Weak Learner  6 .  Time Elapsed (s):  60\n",
      "Weak Learner  7 .  Time Elapsed (s):  60\n",
      "Weak Learner  8 .  Time Elapsed (s):  60\n",
      "Weak Learner  9 .  Time Elapsed (s):  60\n",
      "Weak Learner  10 .  Time Elapsed (s):  60\n",
      "Weak Learner  11 .  Time Elapsed (s):  60\n",
      "Weak Learner  12 .  Time Elapsed (s):  60\n",
      "Weak Learner  13 .  Time Elapsed (s):  60\n",
      "Weak Learner  14 .  Time Elapsed (s):  60\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'accuracies' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-e6563a780e5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mattackStr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"attack_pgd\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mensemble\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestEnsemble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mattack\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_wl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumsamples_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumsamples_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattack_eps_ensemble\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepsilons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattack_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattack_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0maccuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensemble\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mattackStr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cur acc:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'accuracies' is not defined"
     ]
    }
   ],
   "source": [
    "# test pgd on ensemble (more wl)\n",
    "num_wl = 15\n",
    "train_eps_nn = 8\n",
    "for attack_iters in pgd_steps:\n",
    "    path = f'./models/{maxSamples_vals[0]}Eps{train_eps_nn}/'\n",
    "    attack=attack_pgd\n",
    "    attackStr=\"attack_pgd\"\n",
    "    ensemble = testEnsemble(path, [attack], num_wl, numsamples_train=400, numsamples_val=400, attack_eps_ensemble=epsilons, attack_iters=attack_iters)\n",
    "    accuracies.append(ensemble.accuracies[attackStr])\n",
    "    print(\"cur acc:\", accuracies[-1])\n",
    "    \n",
    "    \n",
    "#     maxSamples = maxSamples_vals[0]\n",
    "#     resultsPath = f'results/plots/cifar10/train_eps_{train_eps_nn}_iter_{attack_iters}/{attackStr}/'\n",
    "#     acc_file = resultsPath + f'acc_maxSamples_{maxSamples}.png'\n",
    "#     adv_acc_file = resultsPath + f'adv_acc_maxSamples_{maxSamples}.png'\n",
    "#     loss_file = resultsPath + f'loss_maxSamples_{maxSamples}.png'\n",
    "#     # wl_train_acc_file = resultsPath + f'wl_train_acc_maxSamples_{maxSamples}.png'\n",
    "#     ensemble.plot_accuracies(acc_file)\n",
    "#     ensemble.plot_loss(loss_file)\n",
    "#     ensemble.plot_adversarial_accuracies(adv_acc_file)\n",
    "    # ensemble.plot_wl_acc(wl_train_acc_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
